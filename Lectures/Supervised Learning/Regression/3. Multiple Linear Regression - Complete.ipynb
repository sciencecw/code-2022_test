{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1769a6",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "Most variables of interest probably depend upon more than just a single feature. That is why statisticians invented multiple linear regression.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce the multiple linear regression model,\n",
    "- Show how to fit the model using the <i>normal equation</i>,\n",
    "- Fit a sample model with `numpy` and\n",
    "- Fit the same model with `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f515b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277995ad",
   "metadata": {},
   "source": [
    "## The multiple linear regression model\n",
    "\n",
    "Suppose there is a quantitative variable you want to predict/model called $y$ and a set of $m$ features $X_1, X_2, \\dots X_m$, then the multiple linear regression model regressing $y$ on $X_1, \\dots, X_m$ is:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_m X_m + \\epsilon = X \\beta + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\beta_0, \\dots, \\beta_m \\in \\mathbb{R}$ are constants, $\\beta$ is an $(m+1) \\times 1$ vector of the $\\beta_i$s in numerical order, \n",
    "\n",
    "$$\n",
    "X = \\left(1 | X_1 | X_2 | \\dots | X_m \\right),\n",
    "$$\n",
    "\n",
    "and $\\epsilon \\sim N(0,\\sigma)$ is an error term independent of $X$.\n",
    "\n",
    "### Fitting the model\n",
    "\n",
    "Suppose that we have $n$ observations $(X^{(i)}, y^{(i)})$. In order to fit a multiple linear regression model regressing $y$ on $X$ using this data we return to the mean square error.\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^n \\left(y^{(i)} - \\hat{y}^{(i)}\\right)^2 = \\frac{1}{n} \\sum_{i=1}^n \\left(y^{(i)} - X^{(i)} \\hat{\\beta}\\right)^2,\n",
    "$$\n",
    "\n",
    "we can rewrite this using some linear algebra as:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\left(y - X\\hat{\\beta} \\right)^T\\left(y-X\\hat{\\beta} \\right) = \\frac{1}{n}\\left( y^T y - \\hat{\\beta}^T X^T y - y^T X \\hat{\\beta} + \\hat{\\beta}^T X^T X \\hat{\\beta}\\right). \n",
    "$$\n",
    "\n",
    "When you take the derivative with respect to $\\hat{\\beta}$ and set it equal to $0$ gives the following:\n",
    "\n",
    "$$\n",
    "X^T X \\hat{\\beta} - X^T y = 0, \\text{ and so } \\hat{\\beta} = (X^T X)^{-1}X^T y.\n",
    "$$\n",
    "\n",
    "This is the <i>ordinary least squares</i> estimate of the coefficient vector $\\beta$. Note that this formula is also sometimes called the <i>normal equation</i>.\n",
    "\n",
    "### Back to baseball\n",
    "\n",
    "We will demonstrate how to fit this model in `numpy` and `sklearn` with the baseball example we looked at in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcf800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note this works on Mac and Linux,\n",
    "## you may need to change the slash directions if\n",
    "## you are running a Windows machine\n",
    "baseball = pd.read_csv(\"../../../Data/baseball.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9251fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf2aeae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the train test split here\n",
    "## Note a slight difference, we have to use .copy()\n",
    "## for pandas dataframes\n",
    "bb_train, bb_test = train_test_split(baseball.copy(),\n",
    "                                        shuffle = True,\n",
    "                                        random_state = 440,\n",
    "                                        test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09500f6b",
   "metadata": {},
   "source": [
    "The multiple linear regression model that we will fit is:\n",
    "\n",
    "$$\n",
    "\\texttt{W} = \\beta_0 + \\beta_1 \\texttt{R} + \\beta_2 \\texttt{RA} + \\epsilon.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e5ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remember X needs a column of 1s\n",
    "X_train = np.ones((len(bb_train), 3))\n",
    "\n",
    "X_train[:,1:] = bb_train[['R', 'RA']].values\n",
    "y_train = bb_train.W.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60377673",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the normal equation\n",
    "## note we'll use the linalg subpackage of numpy a lot\n",
    "## here is a link to the documentation\n",
    "## https://numpy.org/doc/stable/reference/routines.linalg.html\n",
    "beta_hat = np.linalg.inv(X_train.transpose().dot(X_train)).dot(X_train.transpose()).dot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d6ffc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0_hat = 84.08213739723357\n",
      "beta_1_hat = 0.09718334537049489\n",
      "beta_2_hat = -0.10132296424142645\n"
     ]
    }
   ],
   "source": [
    "# looking at beta_hat\n",
    "print(\"beta_0_hat =\", beta_hat[0])\n",
    "print(\"beta_1_hat =\", beta_hat[1])\n",
    "print(\"beta_2_hat =\", beta_hat[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1218294",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the predictions \n",
    "y_pred_numpy = beta_hat[0] + beta_hat[1] * X_train[:,1] + beta_hat[2] * X_train[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b527e4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training mse is 16.945434114275745\n"
     ]
    }
   ],
   "source": [
    "## calculate the mse\n",
    "print(\"the training mse is\", np.sum(np.power(y_train-y_pred_numpy, 2))/len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8823a31",
   "metadata": {},
   "source": [
    "### Using `sklearn`\n",
    "\n",
    "We will end this notebook by showing how simple it is to use `sklearn` to fit this model. In fact you already know how to do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4fac5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the LinearRegression object\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15145ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(fit_intercept=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make the model object\n",
    "## notice we have to us fit_intercept = False\n",
    "## because X_train has a column of 1s\n",
    "reg = LinearRegression(copy_X=True, fit_intercept=False)\n",
    "\n",
    "## Fit the model object\n",
    "## note I do NOT have to use reshape here\n",
    "## because X_train is a 2D np.array\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72be0d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([84.0821374 ,  0.09718335, -0.10132296])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## look at coef\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33ac638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a prediction\n",
    "y_pred_sklearn = reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ab213dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mse is 16.945434114275745\n"
     ]
    }
   ],
   "source": [
    "## calculate the mse\n",
    "print(\"the mse is\", np.sum(np.power(y_train-y_pred_sklearn, 2))/len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478ee19",
   "metadata": {},
   "source": [
    "That's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7d1ea9",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29a3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
