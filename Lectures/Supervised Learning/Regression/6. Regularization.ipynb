{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd7c765",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Regularization involves adding a penalty term to our loss function. It turns out that this penalty term can help combat overfitting.\n",
    "\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce the general idea behind regularization,\n",
    "- Establish regularization as a constrained optimization problem,\n",
    "- Discuss ridge and lasso regression as particular regularization algorithms and\n",
    "- Show how lasso is nice for feature selection.\n",
    "\n",
    "##### Quick Note\n",
    "\n",
    "This notebook is a little math heavy, I will do my best to provide both mathematical insight for those that want it and give a broad overview for those that do not want to delve too much into the math specifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d62ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(440)\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60124ec",
   "metadata": {},
   "source": [
    "## Coefficient explosions\n",
    "\n",
    "Let's return to our example from the `Bias-Variance Tradeoff` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8849b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate data\n",
    "x = np.linspace(-3,3,100)\n",
    "y = x*(x-1) + 1.2*np.random.randn(100)\n",
    "\n",
    "\n",
    "## plot the data alongside the true relationship\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(x,y, label=\"Observed Data\")\n",
    "\n",
    "plt.plot(x,x*(x-1),'k', label=\"True Relationship\")\n",
    "\n",
    "plt.xlabel(\"x\",fontsize=16)\n",
    "plt.ylabel(\"y\",fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd562b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the functions/objects we'll need\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make an array of zeros that will hold some data for me\n",
    "n = 26\n",
    "coef_holder = np.zeros((n,n))\n",
    "\n",
    "## Now we'll fit the data with polynomials degree 1 through n\n",
    "for i in range(1,n+1):\n",
    "    ## Make a pipe\n",
    "    pipe = Pipeline([('poly',PolynomialFeatures(i,include_bias = False)),\n",
    "                    ('reg',LinearRegression())])\n",
    "    \n",
    "    ## fit the data\n",
    "    pipe.fit(x.reshape(-1,1),y)\n",
    "    \n",
    "    ## store the coefficient estimates\n",
    "    coef_holder[i-1,:i] = np.round(pipe['reg'].coef_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b2d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the coefficient estimates as a dataframe\n",
    "pd.DataFrame(coef_holder, \n",
    "             columns = [\"x^\" + str(i) for i in range(1,n+1)],\n",
    "            index = [str(i) + \"_deg_poly\" for i in range(1,n+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_norms = []\n",
    "\n",
    "for i in range(n):\n",
    "    beta_norms.append(np.linalg.norm(coef_holder[i,:]))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.plot(range(1,n+1), beta_norms)\n",
    "plt.ylabel(\"'Size' of $\\hat{beta}$\", fontsize=18)\n",
    "plt.xlabel(\"Degree of polynomial\", fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d754519",
   "metadata": {},
   "source": [
    "Looking at the dataframe we have just produced we can notice that a number of our coefficients get larger in magnitude as the model gets more complex. \n",
    "\n",
    "This observation leads to the main idea behind regularization.\n",
    "\n",
    "## The idea behind regularization\n",
    "\n",
    "Suppose the non-intercept coefficients from the regression are denoted by $\\beta$, i.e. $\\beta=\\left(\\beta_1,\\beta_2,\\dots,\\beta_m\\right)^T$. Recall that in Ordinary Least Squares regression our goal is to estimate $\\beta$ so that\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}(y - X\\beta - \\beta_0)^T(y - X\\beta - \\beta_0)\n",
    "$$\n",
    "\n",
    "is minimized on the training data. <i>Note here that I havve separated the intercept $\\beta_0$ from the remaining coefficients, so $X$ is not assumed to have a column of $1$s in this notebook.</i>\n",
    "\n",
    "The main idea behind regularization is to still minimize the MSE, BUT while also ensuring that $\\beta$ doesn't get too large. \n",
    "\n",
    "#### Norms\n",
    "\n",
    "We measure how large our vector, $\\beta$, is with a <i>vector norm</i>. If you are unfamiliar with what a norm is do not worry we will make this more concrete when we talk about lasso and ridge regression, for now think of it as a measure of how \"long\" the $\\beta$ vector is. We denote a norm like $||\\bullet||$, so we would use the notation $||\\beta||$ to denote a vector norm of $\\beta$.\n",
    "\n",
    "### Constrained optimization\n",
    "\n",
    "In regularization we still minimize the MSE, but we constrain ourselves so that we only consider $\\beta$ with $||\\beta||\\leq c$ for some constant $c$. We can think of this as finding the smallest MSE while we are on a $\\beta$ budget.\n",
    "\n",
    "#### An equivalent problem\n",
    "\n",
    "It turns out this is equivalent to minimizing the following: \n",
    "\n",
    "$$\n",
    "||y-X\\beta - \\beta_0||^2_2 + \\alpha||\\beta||,\n",
    "$$\n",
    "\n",
    "for some constant $\\alpha$ and where $||a||_2^2 = a_1^2 + a_2^2 + \\dots + a_n^2, a\\in\\mathbb{R}^n$. Note that minimizing $||y-X\\beta - \\beta_0||^2_2$ is equivalent to minimizing the MSE. \n",
    "\n",
    "To see a mathematical derivation of this equivalence look at reference 3 below.\n",
    "\n",
    "Another way we can think of $\\alpha||\\beta||$ is as a penalty term, which will not allow $\\beta$ to grow too large as we minimize $||y-X\\beta-\\beta_0||^2_2$. The ammount we \"penalize\" for a large $\\beta$ depends on the value of $\\alpha$. \n",
    "\n",
    "#### Our first hyperparameter\n",
    "\n",
    "$\\alpha$ is the first instance in our notes of a <i>hyperparameter</i>, but it will not be the last. A hyperparameter is a parameter we set before fitting the model. While normal parameters, like $\\beta$, are estimated during the training step.\n",
    "\n",
    "For $\\alpha=0$ we recover the OLS estimate for $\\beta$, for $\\alpha=\\infty$ we get $\\beta=0$, values of $\\alpha$ between those two extremes will give different coefficient estimates. The value of $\\alpha$ that gives the best model for your data depends on the problem and can be found through cross-validation model comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba779f77",
   "metadata": {},
   "source": [
    "## Specific regularization models\n",
    "\n",
    "<i>Ridge regression</i> and <i>lasso</i> are two forms of regularization where we make specific choices for the norm.\n",
    "\n",
    "### Ridge regression\n",
    "\n",
    "In ridge regression we take $||\\bullet||$ to be the square of the Euclidean norm, $||\\bullet||_2^2$, i.e. for a vector, $a\\in\\mathbb{R}^n$, we have:\n",
    "\n",
    "$$\n",
    "||a||^2_2 = a_1^2 + a_2^2 + \\dots + a_n^2.\n",
    "$$\n",
    "\n",
    "### Lasso regression\n",
    "\n",
    "In lasso regression we take $||\\bullet||$ to be the $l_1$-norm, i.e. for $a\\in \\mathbb{R}^n$ we have:\n",
    "\n",
    "$$\n",
    "||a||_1 = |a_1| + |a_2| + \\dots + |a_n|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3709b",
   "metadata": {},
   "source": [
    "### Implementing in `sklearn`\n",
    "\n",
    "We can implement both of these models in `sklearn` with `Ridge` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html</a> for ridge regression and `Lasso` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html</a> for lasso regression.\n",
    "\n",
    "<i>Note: ridge and lasso regression are examples of algorithms/models where scaling the data is a step that should be taken prior to fitting the model. This is because vastly different scales can impact the scales of the components of $\\beta$. This can make it so that there is not enough room in the $\\beta$-budget to afford the actual values of the individual coefficients.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the models here\n",
    "## Ridge and Lasso regression are stored in linear_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code will allow us to demonstrate the effect of \n",
    "## increasing alpha\n",
    "\n",
    "## set values for alpha\n",
    "alpha = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "## The degree of the polynomial we will fit\n",
    "n=10\n",
    "\n",
    "#$ These will hold our coefficient estimates\n",
    "ridge_coefs = np.empty((len(alpha),n))\n",
    "lasso_coefs = np.empty((len(alpha),n))\n",
    "\n",
    "## for each alpha value\n",
    "for i in range(len(alpha)):\n",
    "    ## set up the ridge pipeline\n",
    "    ## first scale\n",
    "    ## then make polynomial features\n",
    "    ## then fit the ridge regression model\n",
    "    ## max_iter=5000000\n",
    "    ridge_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False))\n",
    "                              ])\n",
    "    \n",
    "    ## set up the lasso pipeline\n",
    "    ## same steps as with ridge\n",
    "    lasso_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False))\n",
    "                          ])\n",
    "    \n",
    "    \n",
    "    ## fit the ridge\n",
    "    ridge_pipe.fit(x.reshape(-1,1), y)\n",
    "    \n",
    "    ## fit the lasso\n",
    "    lasso_pipe.fit(x.reshape(-1,1), y)\n",
    "\n",
    "    \n",
    "    # record the coefficients\n",
    "    ridge_coefs[i,:] = ridge_pipe['ridge'].coef_\n",
    "    lasso_coefs[i,:] = lasso_pipe['lasso'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc52e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge Coefficients\")\n",
    "\n",
    "pd.DataFrame(np.round(ridge_coefs,8),\n",
    "            columns = [\"x^\" + str(i) for i in range(1,n+1)],\n",
    "            index = [\"alpha=\" + str(a) for a in alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lasso Coefficients\")\n",
    "\n",
    "pd.DataFrame(np.round(lasso_coefs,8),\n",
    "            columns = [\"x^\" + str(i) for i in range(1,n+1)],\n",
    "            index = [\"alpha=\" + str(a) for a in alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d2c02",
   "metadata": {},
   "source": [
    "### Lasso for feature selection\n",
    "\n",
    "Comparing these two tables we might notice that the coefficients for lasso shrink to 0 quite quickly, while the ridge coefficients tend to hang around and never quite get to $0$. This is one of the benefits of lasso regression, feature selection. The coefficients that tend to stay above $0$ as we increase `alpha` in a lasso regression are typically the \"most important\" features for minimizing the training MSE. \n",
    "\n",
    "#### Why does that happen?\n",
    "\n",
    "To see a geometric explanation for why this happens let's return to our constrained optimization formulation, and assume both $X$ and $y$ have mean $0$ for simplicity.\n",
    "\n",
    "##### Ridge Regression\n",
    "$$\n",
    "\\text{Minimize } || y - X\\beta||_2^2 \\text{ constrained to } ||\\beta||_2^2 \\leq c.\n",
    "$$\n",
    "\n",
    "If we have two features the constraint is $\\beta_1^2 + \\beta_2^2 \\leq (\\sqrt{c})^2$, which you may recall is the formula for a filled in circle centered at the origin with radius $\\sqrt{c}$ in $\\mathbb{R}^2$.\n",
    "\n",
    "##### Lasso\n",
    "$$\n",
    "\\text{Minimize } || y - X\\beta||_2^2 \\text{ constrained to } ||\\beta||_1 \\leq c.\n",
    "$$\n",
    "\n",
    "If we have two features the constraint is $|\\beta_1| + |\\beta_2| \\leq c$, which gives a filled in square with vertices at $(c,0),(0,c),(-c,0),$ and $(0,-c)$.\n",
    "\n",
    "Let's look at a picture in the case of two features.\n",
    "<img src=\"lasso_ridge_eosl.png\" width=\"55%\"></img>\n",
    "Photo Credit to <a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\">Elements of Statistical Learning</a>.\n",
    "\n",
    "In this photo $\\hat{\\beta}$ is the OLS estimate for $\\beta$ at the minimum value of $|| y - X\\beta||_2^2$, the red ellipses are selected level curves for $|| y - X\\beta||_2^2$, and the blue square and circle are the contraint regions for the lasso and ridge respectively. We can think of lasso and ridge as finding the smallest level curve that still intersects with the constraint region. If the OLS estimate is not contained within the constraint region this will occur somewhere on the boundary. This image demonstrates that the level curve corresponding to the minimal value of $|| y - X\\beta||_2^2$ often intercepts the lasso constraint on an axis of the $\\beta$-space, which is not the case for ridge regression.\n",
    "\n",
    "As a reminder for practical purposes decreasing the value of $\\alpha$ for the `sklearn` `Lasso` and `Ridge` objects increases the size of the constraint region. Increasing the value of $\\alpha$ will shrink the constraint region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ec113",
   "metadata": {},
   "source": [
    "## Which one to use?\n",
    "\n",
    "Which algorithm is the better choice? Well that depends on the problem. Both are good at addressing overfitting concerns, but each has a couple unique pros and cons.\n",
    "\n",
    "##### Lasso\n",
    "\n",
    "<b>Pros</b>\n",
    "\n",
    "- Works well when you have a large number of features that do not have any effect on the target and\n",
    "- Feature selection is a plus, this can allow for a sparser model which is good for computational reasons.\n",
    "\n",
    "<b>Cons</b>\n",
    "\n",
    "- Can have trouble with highly correlated features (colinearity), it typically chooses one variable among those that are correlated, which may be random.\n",
    "\n",
    "\n",
    "##### Ridge\n",
    "\n",
    "<b>Pros</b>\n",
    "\n",
    "- Works well when the target depends on all or most of the features and\n",
    "- Can handle colinearity better than lasso.\n",
    "\n",
    "<b>Cons</b>\n",
    "\n",
    "- Because ridge typically keeps most of the predictors in the model, this can be a computationally costly model type for data sets with a large number of predictors.\n",
    "\n",
    "\n",
    "##### Elastic Net\n",
    "\n",
    "Sometimes the best model will be something in between ridge and lasso. This technique is known as <i>elastic net</i> and will be demonstrated in a `Practice Problems` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d9056",
   "metadata": {},
   "source": [
    "## Notebook specific references\n",
    "\n",
    "To help teach this lesson I consulted some additional source I found through a Google search. Here are links to those references for you to take a deeper dive into ridge and lasso regression.\n",
    "\n",
    "1. <a href=\"https://www.statlearning.com/\">https://www.statlearning.com/</a>\n",
    "2. <a href=\"https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/\">https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/</a>\n",
    "3. <a href=\"https://suzyahyah.github.io/optimization/2018/07/20/Constrained-unconstrained-form-Ridge.html\">https://suzyahyah.github.io/optimization/2018/07/20/Constrained-unconstrained-form-Ridge.html</a>\n",
    "4. <a href=\"https://statweb.stanford.edu/~owen/courses/305a/Rudyregularization.pdf\">https://statweb.stanford.edu/~owen/courses/305a/Rudyregularization.pdf</a>\n",
    "5. <a href=\"http://web.mit.edu/zoya/www/linearRegression.pdf\">http://web.mit.edu/zoya/www/linearRegression.pdf</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020ee60",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b99efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
