{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93054cbb",
   "metadata": {},
   "source": [
    "# A First Predictive Modeling Project\n",
    "\n",
    "In this notebook we will use the simple linear regression model to demonstrate the process of predictive model selection in a vaguely realistic scenario.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In particular we will:\n",
    "- Review some of the common steps in a predictive modeling project,\n",
    "- Work with a baseball data set,\n",
    "- Introduct the concept of a baseline model and\n",
    "- Practice implementing cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95baec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353dc9f6",
   "metadata": {},
   "source": [
    "## Example: A baseball regression problem\n",
    "\n",
    "Let's imagine we work for a major league baseball team in the off-season. During the off-season teams are looking to see what players they can bring in to improve upon their number of wins in the coming season.\n",
    "\n",
    "A reasonable question is whether stocking up on good defensive players (limiting the number of runs your team allows) or stocking up on good offensive players (increasing the number of runs your teams scores) is better at predicting the number of wins you will have in a given season. We could use existing projections for each player to estimate our number of runs scores and runs allowed after acquiring said player to predict our expected wins.\n",
    "\n",
    "In this problem we will do just that (kind of). First let's load the data.\n",
    "\n",
    "#### Step 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd28035",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note this works on Mac and Linux,\n",
    "## you may need to change the slash directions if\n",
    "## you are running a Windows machine\n",
    "baseball = pd.read_csv(\"../../../Data/baseball.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball.sample(5, random_state=234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c1682",
   "metadata": {},
   "source": [
    "This data set contains the wins (`W`), losses (`L`), runs scored (`R`) and runs allowed (`RA`) for each team (`teamID`) over the 2001-2018 seasons (`yearID`).\n",
    "\n",
    "#### Step 2. Train test split\n",
    "\n",
    "Let's recall what we learned in `Data Splits for Predictive Modeling`, and make a train test split with $80\\%$ of the data in the training set and $20\\%$ of the data in the test set.\n",
    "\n",
    "You try coding this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the train test split here\n",
    "## Note a slight difference, we have to use .copy()\n",
    "## for pandas dataframes\n",
    "bb_train, bb_test = train_test_split(baseball.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd38b19e",
   "metadata": {},
   "source": [
    "#### Step 3. Exploratory data analysis\n",
    "\n",
    "Prior to picking potential models we will do a quick data exploration by plotting `W` against `R` and `RA` respectively. Note that we chose these plots because these are the only variables that a team may be able to change by signing players to contracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fad84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 2, figsize=(16,8), sharey=True)\n",
    "\n",
    "ax[0].scatter(bb_train.R, \n",
    "              bb_train.W)\n",
    "ax[0].set_ylabel(\"Wins\", fontsize=18)\n",
    "ax[0].set_xlabel(\"Runs Scored\", fontsize=18)\n",
    "\n",
    "\n",
    "ax[1].scatter(bb_train.RA,\n",
    "              bb_train.W)\n",
    "ax[1].set_xlabel(\"Runs Allowed\", fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c882f",
   "metadata": {},
   "source": [
    "Both of these look to be linear relationships, which is an important check for our choice in simple linear regression. If it did not look as though there was a linear relationship, we would not use simple linear regression. \n",
    "\n",
    "In real modeling projects there is more data exploration, but for this notebook we will continue on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c610be",
   "metadata": {},
   "source": [
    "#### Step 4. Choosing some candidate models\n",
    "\n",
    "From the exploration above there appear to be two models we should try, namely:\n",
    "\n",
    "$$\n",
    "\\text{Model 1: } \\ \\ \\texttt{W} = \\beta_0 + \\beta_1 \\texttt{R} + \\epsilon,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Model 2: } \\ \\ \\texttt{W} = \\beta_0 + \\beta_1 \\texttt{RA} + \\epsilon.\n",
    "$$\n",
    "\n",
    "##### Identifying a baseline model\n",
    "\n",
    "In predictive modeling problems it is also smart to have a <i>baseline model</i>. A baseline model is a simple model that exists for comparison purposes. These are important because they allow us to put our model results into context. For example, we may end up with an MSE of $100$. Is that good? Is that bad? In the abstract it is impossible to tell. It is only when we have a reasonable baseline model that we are able to put into context how good our performance is. Again for example, if our baseline model's MSE was $1000$ our model with MSE$=100$ has done quite well, but if our baseline model's MSE was $10$ our model has underperformed the baseline.\n",
    "\n",
    "Moreover, merely outperforming the baseline is not always an indication of the best model. We also need to consider things like training time, model complexity and interpretability. While those will not be considerations in this notebook, they will become more important as we progress through these notebooks and in your own data science projects.\n",
    "\n",
    "For this problem a standard first baseline is to predict the average value of the output, $\\overline{\\texttt{W}}$, for any value of the input. This brings us to three models:\n",
    "\n",
    "$$\n",
    "\\text{Model 0: } \\ \\ \\texttt{W} = E\\left(\\texttt{W}\\right) + \\epsilon.\n",
    "$$\n",
    "\n",
    "Now that we have our models, let's implement cross-validation. The code below will have missing pieces, I encourage you to try and fill them out on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93997193",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import KFold\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6138f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a KFold object with k=5\n",
    "kfold = KFold()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2f8eb",
   "metadata": {},
   "source": [
    "We will look for the model with the lowest mean square error. We could calculate this by hand, but we will use `sklearn`'s `mean_squared_error` instead, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0bb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a033c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08739c7",
   "metadata": {},
   "source": [
    "Now we are ready to perform cross-valdiation. Again there will be some empty spots that you can fill in on your own if you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28460545",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make an array of zeros that will hold our mses\n",
    "mses = np.zeros((3, 5))\n",
    "\n",
    "## This keeps track of what split we are on\n",
    "i = 0\n",
    "## fill in what is missing in the for loop declaration\n",
    "for :\n",
    "    ## now we get the training splits and the holdout split\n",
    "    ### Training\n",
    "    bb_t_t = \n",
    "    \n",
    "    ### Holdout set\n",
    "    bb_ho = \n",
    "    \n",
    "    \n",
    "    ### This is Model 0 ###\n",
    "    ## take the mean W from the training set\n",
    "    ## we need predictions for the entire holdout set\n",
    "    pred0 = bb_t_t.W.mean() * np.ones(len(bb_ho))\n",
    "    \n",
    "    ### This is Model 1 ###\n",
    "    ## W = beta_0 + beta_1 * R + epsilon\n",
    "    ## Define the LinearRegression object\n",
    "    model1 = \n",
    "    \n",
    "    ## fit model1 on the training data, bb_t_t\n",
    "    ## don't forget you may need to reshape the data\n",
    "    model1\n",
    "    \n",
    "    ## get the prediction on holdout set\n",
    "    pred1 = \n",
    "    \n",
    "    ### This is Model 2 ###\n",
    "    ## W = beta_0 + beta_1 * RA + epsilon\n",
    "    ## Define the LinearRegression object\n",
    "    model2 = LinearRegression(copy_X=True)\n",
    "    \n",
    "    ## fit model1 on the training data, bb_t_t\n",
    "    ## don't forget you may need to reshape the data\n",
    "    model2.fit(bb_t_t.RA.values.reshape(-1,1), bb_t_t.W.values)\n",
    "    \n",
    "    ## get the prediction on holdout set\n",
    "    pred2 = model2.predict(bb_ho.RA.values.reshape(-1,1))\n",
    "    \n",
    "    \n",
    "    ### Recording the MSES ###\n",
    "    ## mean_squared_error takes in the true values, then the predicted values\n",
    "    ## model 0\n",
    "    mses[0,i] = mean_squared_error(bb_ho.W.values, pred0)\n",
    "    \n",
    "    ## model 1\n",
    "    mses[1,i] = \n",
    "    \n",
    "    ## model 2\n",
    "    mses[2,i] = mean_squared_error(bb_ho.W.values, pred2)\n",
    "    \n",
    "    ## increase i by 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad359908",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This figure will compare the performance\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.scatter(np.zeros(5), mses[0,:], s=30, c='k', label=\"Single Split\")\n",
    "plt.scatter(np.ones(5), mses[1,:], s=30, c='k')\n",
    "plt.scatter(2*np.ones(5), mses[2,:], s=30, c='k')\n",
    "\n",
    "plt.scatter([0,1,2], np.mean(mses, axis=1), s=60, c='r', label=\"Mean\")\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xticks([0,1,2],[\"Baseline\", \"Model 1\", \"Model 2\"], fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"Model\", fontsize=16)\n",
    "plt.ylabel(\"MSE\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3d782",
   "metadata": {},
   "source": [
    "#### Step 5. Additional modeling\n",
    "\n",
    "From this it appears that Model 2 is our best choice, it has the lowest average cross-validation MSE.\n",
    "\n",
    "For this particular problem we will end here, but in practice you will probably want to tweak and try additional models. After intial modeling you may want to reassess your comparison model. For example, in this problem additional models would be compared to Model 2, since it performed best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3759a8",
   "metadata": {},
   "source": [
    "#### Step 6. Test set sanity check\n",
    "\n",
    "Once you are done tinkering and have settled on a final model (or a final set of a few models) you can perform your final check with the test set. The purposes of this check are twofold:\n",
    "1. We will be able to see if we had any coding errors in our work up to this point, performance that greatly departs from what we would expect may indicate a coding error in our earlier work and\n",
    "2. It allows us to assess overfitting (more on this soon). When our test performance is significantly worse than our training performance it suggests that our model has overfit on the training data and will not generalize well.\n",
    "\n",
    "If you notice weird behavior on the test set it may be worth reviewing your previous work and checking out different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48030df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a final model object\n",
    "model = LinearRegression(copy_X=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit that on the entire training set\n",
    "model.fit(bb_train.RA.values.reshape(-1,1), bb_train.W.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the training set performance\n",
    "print(\"Training set MSE:\", \n",
    "      np.round(mean_squared_error(bb_train.W.values, model.predict(bb_train.RA.values.reshape(-1,1))),2))\n",
    "\n",
    "\n",
    "## print the test set performance\n",
    "print(\"Test set MSE:\", \n",
    "      np.round(mean_squared_error(bb_test.W.values, model.predict(bb_test.RA.values.reshape(-1,1))),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf98ff3",
   "metadata": {},
   "source": [
    "These are comparable performances, so I think we are in the clear for overfitting and coding errors.\n",
    "\n",
    "Also <i>note</i> that it is not common for the test set to have better performance than the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91144169",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73954573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
