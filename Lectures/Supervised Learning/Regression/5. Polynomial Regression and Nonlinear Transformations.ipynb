{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c51f1b",
   "metadata": {},
   "source": [
    "# Polynomial Regression and Nonlinear Transformations\n",
    "\n",
    "We continue to build our regression repetoire with polynomial and nonlinear transformations.\n",
    "\n",
    "## What will we accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce polynomial regression,\n",
    "- Review interaction terms between continuous features and\n",
    "- Discuss adding in nonlinear transformations of your features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c72726",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add5071",
   "metadata": {},
   "source": [
    "## Adding in polynomial transformations\n",
    "\n",
    "We will demonstrate polynomial regression by working through a sample data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967dac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../../Data/poly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0ff35",
   "metadata": {},
   "source": [
    "<i>Note: we will not be doing train test splits, cross-validation or a validation set for this data set because we will not be comparing predictive models. The point of this notebook is to illustrate polynomial regression and nonlinear transformations, not model selection.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a739937",
   "metadata": {},
   "source": [
    "Let's make a plot to explore any potential relationships between `x1`, `x2` and `y`.\n",
    "\n",
    "To do this we will use `pandas` `scatter_matrix` function, <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html\">https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1689a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b036aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(df, figsize=(14,14), alpha=.9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e500d",
   "metadata": {},
   "source": [
    "Looking at the `y` row we can see that while there seems to be a linear relationship between `y` and `x2`, there is clearly not one between `y` and `x1`. Recalling our old algebra courses, we recognize that there does appear to be something like a quadratic ($y \\propto x_1^2$) or quartic ($y \\propto x_1^4$) relationship. While these features are not present in the original data set, we can easily create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcef655",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make x1_sq here\n",
    "df['x1_sq'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6930625",
   "metadata": {},
   "source": [
    "We can re-examine our scatter matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(df[['x1','x2','x1_sq','y']], figsize=(14,14), alpha=.9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b275240",
   "metadata": {},
   "source": [
    "Much more of a linear relationship. Let's fit the following model:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_2 + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the model\n",
    "reg = LinearRegression(copy_X=True)\n",
    "reg.fit(df[['x1','x1_sq','x2']].values, df['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf271dc",
   "metadata": {},
   "source": [
    "Now we have not done this yet, but one trick when building a linear regression model is to look at the <i>residual plot</i>, where we plot our errors ($y - \\hat{y}$) against our $y$ values. Recall that if our model closely approximates $y$ then we should have:\n",
    "$$\n",
    "y - \\hat{y} \\approx \\epsilon.\n",
    "$$\n",
    "\n",
    "So if our model closely approximates $y$ we should expect to see a blob of points that fall uniformly around the horizontal axis. If we see points that clearly depart from such a pattern we know that our model is missing some input that could help explain or predict `y`.\n",
    "\n",
    "<i>We will touch more on residual plots in a coming notebook</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.scatter(df.y, df.y - reg.predict(df[['x1','x1_sq','x2']].values))\n",
    "\n",
    "plt.xlabel(\"$y$\", fontsize=18)\n",
    "plt.ylabel(\"$y - \\hat{y}$\", fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f78cd2",
   "metadata": {},
   "source": [
    "This definitely does not look like a uniform band around the horizontal axis, which suggests we are missing an input into our model. In this setting, we do not have any other variables in our data set that we have left out of the modeling process. We could include additional powers of `x1`, but let's first try adding an <i>interaction</i> term between `x1` and `x2`. Remember that an interaction term between two variables just means that we multiply those variables. So the new model we will fit is:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_2 + \\beta_4 x_1 x_2 + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58185e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First make x1x2\n",
    "df['x1x2'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a2497",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression(copy_X=True)\n",
    "reg.fit(df[['x1','x1_sq','x2','x1x2']], df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a9d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.scatter(df.y, df.y - reg.predict(df[['x1','x1_sq','x2','x1x2']]))\n",
    "\n",
    "plt.xlabel(\"$y$\", fontsize=18)\n",
    "plt.ylabel(\"$y - \\hat{y} $\", fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b24b4c",
   "metadata": {},
   "source": [
    "This is a much better plot.\n",
    "\n",
    "Now I know this seems like a mystical process, and a common question I get is when do I know when to include an interaction term? Unfortunately there is not a silver bullet to finding the perfect model. Sometimes there is no algorithmic replacement to data exploration and playing around with different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3217ea9f",
   "metadata": {},
   "source": [
    "### Respecting the hierarchy when modeling\n",
    "\n",
    "Let's look at the coefficients on this  model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a505e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0042c50",
   "metadata": {},
   "source": [
    "If you notice the coefficient on $x_1$ is close to $0$ in the interaction term model. It may be tempting to remove this feature from the model especially if the true relationship was: \n",
    "\n",
    "$$ \n",
    "y = 2 + x_1^2 - 10 x_2 + x_1 x_2. \n",
    "$$ \n",
    "\n",
    "However, there is no way for you to know ahead of time what the true relationship is between the target and the features, if there was there'd be no need for regression.\n",
    "\n",
    "To further illustrate this point, imagine the true relationship was such that: \n",
    "\n",
    "$$ \n",
    "y \\propto x_1^2, \n",
    "$$ \n",
    "\n",
    "if we do not include $x_1$ in our model we are limiting ourselves to parabolas of the form \n",
    "\n",
    "$$ \n",
    "\\beta_0 + \\beta_1 x_1^2, \n",
    "$$ \n",
    "\n",
    "which leaves out a number of possible parabolas.\n",
    "\n",
    "It is important to remember that anytime you make a model that includes a polynomial transformation you need to include all of the lesser powers as well. So with $x_1^2$ as the highest power you'd need to include $x_1$, with $x_1^3$ as the highest power you'd need to include $x_1^2$ and $x_1$, and so on for $x_1^n$.\n",
    "\n",
    "This also holds for interaction terms. If you include $x_1 x_2$ you need to include both $x_1$ and $x_2$ as predictors as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c473a",
   "metadata": {},
   "source": [
    "## Nonlinear transformations\n",
    "\n",
    "In addition to polynomial transformations, we could also include other transformations of our input data like $\\sqrt{\\bullet}$, $\\log{(\\bullet)}$, $\\sin{(\\bullet)}$, $e^\\bullet$ or more.\n",
    "\n",
    "You will work on an example problem using a nonlinear transformation in our problem session notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4caa8e",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e5b4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
