{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c33e15",
   "metadata": {},
   "source": [
    "# Regression Version of Classification Algorithms\n",
    "\n",
    "Most of the classification algorithms we touched on have an analogous algorithm for regression problems.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Learn about $k$ nearest neighbors regression,\n",
    "- Introduce decision tree regression and\n",
    "- Discuss support vector regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e54529",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09d4ca",
   "metadata": {},
   "source": [
    "For the theoretical setup for all of these models we will suppose that we have $n$ observations of $m$ features stored in a matrix, $X$, with $n$ corresponding outputs stored in a vector $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58775a41",
   "metadata": {},
   "source": [
    "## $k$ nearest neighbors regression\n",
    "\n",
    "For this regression algorithm predictions are generated like so:\n",
    "$$\n",
    "f(X^*) = \\frac{1}{k} \\sum_{i\\in \\mathcal{N}^*} y^{(i)},\n",
    "$$\n",
    "where $\\mathcal{N}^*$ denotes the set of indices of $X^*$'s $k$ closest neighbors in the dataspace.\n",
    "\n",
    "So in summary you find the $k$-nearest neighbors of any point for which you would like a prediction, and then you find the arithmetic mean of their target values.\n",
    "\n",
    "### In `sklearn`\n",
    "\n",
    "$k$ nearest neighbors regression can be performed with `sklearn`'s `KNeighborsRegressor` model object, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html</a>. \n",
    "\n",
    "Let's see how to implement this in `sklearn` with a return to our baseball data set. We will use this to plot the predictions on top of the training data for a $k$-nearest neighbors regression using $k=1$ and $k=10$. These will both be compared to our simple linear regression model regressing wins on run differential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c1aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to import the data\n",
    "# it is stored in the baseball_run_diff.csv file\n",
    "ball = pd.read_csv(\"../../../Data/baseball_run_diff.csv\")\n",
    "\n",
    "ball_train, ball_test = train_test_split(ball.copy(),\n",
    "                                            shuffle=True,\n",
    "                                            random_state=403,\n",
    "                                            test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ae425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a figure\n",
    "plt.figure(figsize = (8,8))\n",
    "\n",
    "# plt.scatter plots RD on the x and W on the y\n",
    "plt.scatter(ball_train.RD, ball_train.W)\n",
    "\n",
    "# Always good practice to label well when\n",
    "# presenting a figure to others\n",
    "# place an xlabel\n",
    "plt.xlabel(\"Run Differential\", fontsize =16)\n",
    "\n",
    "# place a ylabel\n",
    "plt.ylabel(\"Wins\", fontsize = 16)\n",
    "\n",
    "# type this to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd9ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import knnr\n",
    "\n",
    "\n",
    "## import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994277b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make model objects\n",
    "knr_1 = \n",
    "knr_10 = \n",
    "slr = LinearRegression(copy_X=True)\n",
    "\n",
    "\n",
    "## Fit the models\n",
    "knr_1 = \n",
    "knr_10 = \n",
    "\n",
    "slr.fit(ball_train.RD.values.reshape(-1,1),\n",
    "             ball_train.W.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a figure\n",
    "# this makes a figure that is 10 units by 10 units\n",
    "fig,ax = plt.subplots(1, 2, figsize = (16,8), sharex=True, sharey=True)\n",
    "\n",
    "# scatter plots RD on the x and W on the y\n",
    "ax[0].scatter(ball_train.RD, \n",
    "              ball_train.W,\n",
    "              alpha = .3,\n",
    "              label=\"Training Data\")\n",
    "ax[1].scatter(ball_train.RD, \n",
    "              ball_train.W,\n",
    "              alpha = .3,\n",
    "              label=\"Training Data\")\n",
    "\n",
    "ax[0].plot(np.linspace(-350,310,100),\n",
    "           knr_1.predict(np.linspace(-350,310,100).reshape(-1,1)),\n",
    "           'k--',\n",
    "           label=\"KNR\")\n",
    "\n",
    "ax[0].plot(np.linspace(-350,310,100),\n",
    "           slr.predict(np.linspace(-350,310,100).reshape(-1,1)),\n",
    "           'r-.',\n",
    "           label=\"SLR\")\n",
    "\n",
    "ax[1].plot(np.linspace(-350,310,100),\n",
    "           knr_10.predict(np.linspace(-350,310,100).reshape(-1,1)),\n",
    "           'k--',\n",
    "           label=\"KNR\")\n",
    "\n",
    "ax[1].plot(np.linspace(-350,310,100),\n",
    "           slr.predict(np.linspace(-350,310,100).reshape(-1,1)),\n",
    "           'r-.',\n",
    "           label=\"SLR\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Always good practice to label well when\n",
    "# presenting a figure to others\n",
    "# place an xlabel\n",
    "ax[0].set_xlabel(\"Run Differential\", fontsize =16)\n",
    "ax[1].set_xlabel(\"Run Differential\", fontsize =16)\n",
    "\n",
    "\n",
    "# place a ylabel\n",
    "ax[1].set_ylabel(\"Wins\", fontsize = 16)\n",
    "\n",
    "## title\n",
    "ax[0].set_title(\"$k=1$\", fontsize=18)\n",
    "ax[1].set_title(\"$k=10$\", fontsize=18)\n",
    "\n",
    "## add legend\n",
    "ax[0].legend(fontsize=14)\n",
    "\n",
    "# type this to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89296c17",
   "metadata": {},
   "source": [
    "As is the case with $k$-nearest neighbors classifiers, you can find the optimal $k$ using a cross-validation approach.\n",
    "\n",
    "You may be wondering why we would use $k$-nearest neighbors instead of simple linear regression.\n",
    "\n",
    "Simple linear regression is known as a <i>parametric</i> technique (because we estimate a parameter, $\\beta$), whereas $k$-nearest neighbors is a <i>nonparametric</i> technique (because we do not estimate any parameters).\n",
    "\n",
    "Nonparametric techniques are sometimes useful when we cannot confirm the statistical assumptions of the parametric technique. Sometimes parametric techniques may not be appropriate. For instance, consider the case where there is clearly not a linear relationship between the features and target. Instead of guessing what powers or nonlinear transformations to use in the case of linear regression, you can use a nonparametric regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7cfd8a",
   "metadata": {},
   "source": [
    "## Tree-based regression\n",
    "\n",
    "Let's branch out by discussing tree-based regression.\n",
    "\n",
    "Recall that with decision tree classifiers we use the CART algorithm, in which we search through a random subset of the features and use a binary search to obtain the feature-cutpoint pairing that reduces the impurity measure the most.\n",
    "\n",
    "Tree-based regression does the same thing, but instead of Gini Impurity or Entropy the search is for the feature-cutpoint pairing that provides the greatest reduction in the MSE.\n",
    "\n",
    "Once the tree is constructed all predictions are provided as follows.\n",
    "\n",
    "Suppose we want to predict on a datapoint $X^*$. We first run $X^*$ through the decision tree. The prediction is then determined by averaging the target value over all the training points that ended up in the same terminal node.\n",
    "\n",
    "### In `sklearn`\n",
    "\n",
    "A decision tree regression can be implemented with `sklearn`'s  `DecisionTreeRegressor`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor\">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor</a>. \n",
    "\n",
    "Again we demonstrate with our baseball data set. Note that the regressor has many of the same hyperparameter inputs as the decision tree classifier. We will build a model with a `max_depth` of $1$ and a `max_depth` of $5$ then plot both on top of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b50342",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f2dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the model objects\n",
    "tree_1 = \n",
    "tree_5 = \n",
    "\n",
    "## fit the objects\n",
    "tree_1\n",
    "tree_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f46a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a figure\n",
    "# this makes a figure that is 10 units by 10 units\n",
    "fig,ax = plt.subplots(1, 2, figsize = (16,8), sharex=True, sharey=True)\n",
    "\n",
    "# scatter plots RD on the x and W on the y\n",
    "ax[0].scatter(ball_train.RD, \n",
    "              ball_train.W,\n",
    "              alpha = .3,\n",
    "              label=\"Training Data\")\n",
    "ax[1].scatter(ball_train.RD, \n",
    "              ball_train.W,\n",
    "              alpha = .3,\n",
    "              label=\"Training Data\")\n",
    "\n",
    "ax[0].plot(np.linspace(-350,310,100),\n",
    "           tree_1.predict(np.linspace(-350,310,100).reshape(-1,1)),\n",
    "           'k--',\n",
    "           label=\"Decision Tree\")\n",
    "\n",
    "ax[0].plot(np.linspace(-350,310,100),\n",
    "           slr.predict(np.linspace(-350,310,100).reshape(-1,1)),\n",
    "           'r-.',\n",
    "           label=\"SLR\")\n",
    "\n",
    "ax[1].plot(np.linspace(-350,310,100),\n",
    "           tree_5.predict(np.linspace(-350,310,100).reshape(-1,1)),\n",
    "           'k--',\n",
    "           label=\"Decision Tree\")\n",
    "\n",
    "ax[1].plot(np.linspace(-350,310,100),\n",
    "           slr.predict(np.linspace(-350,310,100).reshape(-1,1)),\n",
    "           'r-.',\n",
    "           label=\"SLR\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Always good practice to label well when\n",
    "# presenting a figure to others\n",
    "# place an xlabel\n",
    "ax[0].set_xlabel(\"Run Differential\", fontsize =16)\n",
    "ax[1].set_xlabel(\"Run Differential\", fontsize =16)\n",
    "\n",
    "\n",
    "# place a ylabel\n",
    "ax[1].set_ylabel(\"Wins\", fontsize = 16)\n",
    "\n",
    "## title\n",
    "ax[0].set_title(\"max depth $= 1$\", fontsize=18)\n",
    "ax[1].set_title(\"max depth $= 5$\", fontsize=18)\n",
    "\n",
    "## add legend\n",
    "ax[0].legend(fontsize=14)\n",
    "\n",
    "# type this to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a22a6",
   "metadata": {},
   "source": [
    "## Support vector regression\n",
    "\n",
    "We end with the support vector machine version of regression.\n",
    "\n",
    "The idea behind support vector regression draws upon the concept of a margin that we discussed in the the `Support Vector Machine` notebook. \n",
    "\n",
    "We assume that all observations $(X^{(i)},y^{(i)})$ are such that $y^{(i)} \\in \\left( f(X^{(i)}) - \\epsilon, f(X^{(i)}) + \\epsilon \\right)$ for some function $f$ and some value $\\epsilon$ that isn't too large (otherwise this assumption is pointless). In the linear formulation of the algorithm you assume that $f(X) = Xw$, i.e. the functional form is that of a hyperplane. The specific constrained optimization problem you want to solve in this set up is:\n",
    "\n",
    "$$\n",
    "\\text{minimize } \\frac{1}{2}||w||^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{constrained to } |X^{(i)}w - y^{(i)}| \\leq \\epsilon \\text{ for all training observations}.\n",
    "$$\n",
    "\n",
    "This set up is analogous to the maximal margin classifier. For the soft-margin version we add in slack variables, $\\xi_i,\\xi_i^*$, like so:\n",
    "\n",
    "$$\n",
    "\\text{minimize } \\frac{1}{2}||w||^2 + C \\sum_{i}^n \\left( \\xi_i + \\xi_i^* \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{constrained to } \\left\\lbrace \\begin{array}{l}y^{(i)} - X^{(i)}w \\leq \\epsilon + \\xi_i \\\\\n",
    "X^{(i)}w - y^{(i)} \\leq \\epsilon + \\xi_i^* \\\\\n",
    "\\xi_i, \\xi_i^* \\geq 0\\end{array}\\right. \\text{ for all training observations},\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\xi_i = \\left\\lbrace \\begin{array}{l l} 0 & \\text{if } y_i - X^{(i)}w - \\epsilon \\leq 0 \\\\\n",
    "y^{(i)} - X^{(i)}w - \\epsilon & \\text{else} \\end{array} \\right.,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\xi_i^* = \\left\\lbrace \\begin{array}{l l} 0 & \\text{if } X^{(i)}w - y^{(i)}  - \\epsilon  \\leq 0 \\\\\n",
    "X^{(i)}w - y^{(i)}  - \\epsilon  & \\text{else} \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "To help explain let's examine this picture from <a href=\"https://cs.adelaide.edu.au/~chhshen/teaching/ML_SVR.pdf\">https://cs.adelaide.edu.au/~chhshen/teaching/ML_SVR.pdf</a>\n",
    "<img src=\"SVRpic.png\" width = \"50%\"></img>\n",
    "\n",
    "In this soft margin approach we're only penalizing the cost function by those observations that exceed the $\\epsilon$-margin we've set.\n",
    "\n",
    "### In `sklearn`\n",
    "\n",
    "This model can be implemented in `sklearn` with `LinearSVR`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR\">https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR</a>. More generally you can use `SVR`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html</a>, which again uses the kernel trick to lift the data to a higher dimensional space, but we will stick with the linear version for this demonstration.\n",
    "\n",
    "Let's use this to regress wins on run differential and compare the result to simple linear regression for different values of $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import LinearSVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will use this function to compare different values of epsilon\n",
    "\n",
    "## Use this function to play around with epsilon below\n",
    "## svr should be your model object, epsilon your desired epsilon value\n",
    "def plot_svr(svr, epsilon):\n",
    "    ## fitting the model\n",
    "    svr.fit(ball_train.RD.values.reshape(-1,1),\n",
    "               ball_train.W.values)\n",
    "    \n",
    "    # first make a figure\n",
    "    # this makes a figure that is 10 units by 10 units\n",
    "    plt.figure(figsize = (10,10))\n",
    "\n",
    "    # plt.scatter plots RD on the x and W on the y\n",
    "    plt.scatter(ball_train.RD.values.reshape(-1,1), \n",
    "                ball_train.W.values, \n",
    "                alpha=.3)\n",
    "\n",
    "    ## plot your two prediction lines here\n",
    "    plt.plot(np.linspace(-350,310,100),\n",
    "                svr.predict(np.linspace(-350,310,100).reshape(-1,1)),\n",
    "                'k-',\n",
    "                label = \"SVR\")\n",
    "    plt.plot(np.linspace(-350,310,100),\n",
    "                svr.predict(np.linspace(-350,310,100).reshape(-1,1)) - epsilon,\n",
    "                'k--',\n",
    "                label = \"Epsilon Bound\")\n",
    "    plt.plot(np.linspace(-350,310,100),\n",
    "                svr.predict(np.linspace(-350,310,100).reshape(-1,1)) + epsilon,\n",
    "                'k--')\n",
    "    \n",
    "    plt.plot(np.linspace(-350,310,100),\n",
    "                slr.predict(np.linspace(-350,310,100).reshape(-1,1)),\n",
    "                'r-.',\n",
    "                label=\"SLR\")\n",
    "    # Always good practice to label well when\n",
    "    # presenting a figure to others\n",
    "    # place an xlabel\n",
    "    plt.xlabel(\"Run Differential\", fontsize =16)\n",
    "\n",
    "    # place a ylabel\n",
    "    plt.ylabel(\"Wins\", fontsize = 16)\n",
    "    \n",
    "    plt.title(\"$\\epsilon = $\" + str(epsilon), fontsize=20)\n",
    "\n",
    "    plt.legend(fontsize=16)\n",
    "\n",
    "    # type this to show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2acc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## epsilon = 0\n",
    "epsilon = 0\n",
    "\n",
    "plot_svr(LinearSVR(C=1, epsilon=epsilon, max_iter=100000), epsilon)\n",
    "\n",
    "## epsilon = 1\n",
    "epsilon = 1\n",
    "\n",
    "plot_svr(LinearSVR(C=1, epsilon=epsilon, max_iter=100000), epsilon)\n",
    "\n",
    "## epsilon = 10\n",
    "epsilon = 10\n",
    "\n",
    "plot_svr(LinearSVR(C=1, epsilon=epsilon, max_iter=100000), epsilon)\n",
    "\n",
    "## epsilon = 100\n",
    "epsilon = 100\n",
    "\n",
    "plot_svr(LinearSVR(C=1, epsilon=epsilon, max_iter=100000), epsilon)\n",
    "\n",
    "## epsilon = 1000\n",
    "epsilon = 1000\n",
    "\n",
    "plot_svr(LinearSVR(C=1, epsilon=epsilon, max_iter=100000), epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62890ec2",
   "metadata": {},
   "source": [
    "As we increase $\\epsilon$ we have a wider band in which to fit our data points. Since the only terms that contribute to the cost function are those outside of the band the algorithm will find a $w$ so that $w$ is as close to $0$ while fitting all of the points within the $\\epsilon$ band. So if $\\epsilon$ is large enough we will get a horizontal line from support vector regression.\n",
    "\n",
    "Extra Support Vector Sources:\n",
    "- <a href=\"https://cs.adelaide.edu.au/~chhshen/teaching/ML_SVR.pdf\">https://cs.adelaide.edu.au/~chhshen/teaching/ML_SVR.pdf</a>\n",
    "- <a href=\"https://alex.smola.org/papers/2003/SmoSch03b.pdf\">https://alex.smola.org/papers/2003/SmoSch03b.pdf</a>\n",
    "- <a href=\"https://stats.stackexchange.com/questions/82044/how-does-support-vector-regression-work-intuitively\">https://stats.stackexchange.com/questions/82044/how-does-support-vector-regression-work-intuitively</a>\n",
    "- <a href=\"https://stats.stackexchange.com/questions/198199/how-different-is-support-vector-regression-compared-to-svm\">https://stats.stackexchange.com/questions/198199/how-different-is-support-vector-regression-compared-to-svm</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ccf8d",
   "metadata": {},
   "source": [
    "<i>Note that while we focused on a regression problem with a single predictor in this notebook, all of these techniques can handle multiple predictors as well</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d2acb",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b4ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
