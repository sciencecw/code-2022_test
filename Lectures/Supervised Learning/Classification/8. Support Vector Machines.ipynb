{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972fe0b6",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "A popular type of classification algorithm developed by the computer science community is the support vector machine. We now learn about this techinque.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss linear separability,\n",
    "- Define the maximal margin classifier:\n",
    "    - Provide the optimization problem that leads to its estimate and\n",
    "    - Demonstrate implementaion in `sklearn`,\n",
    "- Define the soft margin classifier:\n",
    "    - Provide the optimization problem that leads to its estimate and\n",
    "    - Demonstrate implementaion in `sklearn`,\n",
    "- Mention the limitations of linear support vector machines,\n",
    "- Give the intuition behind general support vector machines and\n",
    "- Introduce the concept of a kernel function:\n",
    "    - Discuss popular kernel functions for support vector machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e290ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a dark background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b8ebf",
   "metadata": {},
   "source": [
    "## Linear support vector machines\n",
    "\n",
    "The first class of support vector machine (SVM) we will discuss is the <i>linear SVM</i>. These SVMs are designed with for data sets that are linearly separable or nearly linearly separable. A data set for a binary classification problem is said to be <i>linearly separable</i> if there is a hyperplane that separates the two classes. \n",
    "\n",
    "<i>If you are unfamiliar with the term hyperplane here is how to think about it, take a high dimensional space, any subspace that is one dimension lower is a hyperplane. So in $\\mathbb{R}^1$ a hyperplane is a point, in $\\mathbb{R}^2$ a hyperplane is a line, in $\\mathbb{R}^3$ a hyperplane is a $2$-D plane, and in $\\mathbb{R}^n$ it is an $n-1$ subspace.</i>\n",
    "\n",
    "There are two types of linear SVMs.\n",
    "\n",
    "### Maximal margin classifiers\n",
    "\n",
    "We will start with what are sometimes referred to as <i>maximal margin classifiers</i>, for reasons we will understand soon.\n",
    "\n",
    "Let's see some motivation first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd808004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the random data\n",
    "np.random.seed(440)\n",
    "n_rows = 100\n",
    "diff = .1\n",
    "X = np.random.random((n_rows,2))\n",
    "X_prime = X[(X[:,1] - X[:,0]) <= -diff,:]\n",
    "X_2prime = X[(X[:,1] - X[:,0]) >= diff,:]\n",
    "\n",
    "del X\n",
    "X = np.append(X_prime,X_2prime,axis = 0)\n",
    "\n",
    "y = np.empty(np.shape(X)[0])\n",
    "y[(X[:,1] - X[:,0]) <= -diff] = 1\n",
    "y[((X[:,1] - X[:,0]) >= diff)] = -1\n",
    "\n",
    "X[1,:] = [.8,.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the data\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(X[y == -1,0],\n",
    "            X[y == -1,1],\n",
    "            c = \"blue\",\n",
    "            s = 60,\n",
    "            label=\"-1\")\n",
    "plt.scatter(X[y == 1,0],\n",
    "            X[y == 1,1],\n",
    "            c = \"orange\",\n",
    "            marker = 'v',\n",
    "            s = 60,\n",
    "            label=\"1\")\n",
    "\n",
    "plt.legend(fontsize = 16)\n",
    "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e0f44",
   "metadata": {},
   "source": [
    "These classes appear to be linearly separable. The question becomes what is the best line with which to separate these points? That is what the maximal margin classifier finds! Below are three examples of separating lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "plt.scatter(X[y == -1,0],\n",
    "            X[y == -1,1],\n",
    "            c = \"blue\",\n",
    "            s = 60,\n",
    "            label=\"-1\")\n",
    "plt.scatter(X[y == 1,0],\n",
    "            X[y == 1,1],\n",
    "            c = \"orange\",\n",
    "            marker = 'v',\n",
    "            s = 60,\n",
    "            label=\"1\")\n",
    "\n",
    "plt.plot(np.linspace(0,1,100),\n",
    "         np.linspace(0,1,100),\n",
    "         'k',\n",
    "         label=\"x_2-x_1 = 0\")\n",
    "plt.plot(np.linspace(0,1,100),\n",
    "         1.5*np.linspace(0,1,100)-.25,\n",
    "         'b--',\n",
    "         label=\"x_2-1.5x_1 = -0.25\")\n",
    "plt.plot(np.linspace(0,1,100),\n",
    "         (1/1.5)*np.linspace(0,1,100)+.19,\n",
    "         'r-.',\n",
    "         label = \"x_2-(1/1.5)x_1 = 0.19\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 16)\n",
    "\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05580ee0",
   "metadata": {},
   "source": [
    "All three of these lines separate the two classes, but which one do you think will best generalize to future data points?\n",
    "\n",
    "The red dot-dash and blue dotted lines get a bit to close to the training data at certain points. If we use one of those two lines, it is likely that new observations (which we would want to classify) will deviate to the wrong side of the decision boundary because of random noise. So what to do?\n",
    "\n",
    "One approach is to draw a hyperplane that maximizes the total distance from all points to the hyperplane. Another way to think about it is to draw a hyperplane, find the minimum distance from the points to the hyperlane (known as the <i>margin</i>), then make that as large as possible (<i>maximize</i> it). Hence the name <i>maximal margin classifier</i>.\n",
    "\n",
    "Let's formally define this classification algorithm, and then show how to implement it in python.\n",
    "\n",
    "Suppose we have a binary variable, $y \\in \\left\\lbrace -1,1 \\right\\rbrace$, and a set of $m$ features stored in the columns a feature matrix $X$. Then we can find the maximal margin classifier, if it exists, by solving the constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\text{find } \\beta \\text{ and maximal } M, \\text{ subject to}\n",
    "$$\n",
    "\n",
    "$$\n",
    "||\\beta||_2^2 = 1, \\text{ and } \n",
    "$$\n",
    "\n",
    "$$\n",
    "y^{(i)}\\left(X^{(i)} \\beta\\right) \\geq M  \\ \\ \\ \\forall i = 1, \\dots, n,\n",
    "$$\n",
    "\n",
    "where $\\beta=\\left(\\beta_0, \\beta_1, \\dots, \\beta_m \\right)^T$ is a coefficient vector and $X$ has been extended to include a column of ones. It is possible to solve this optimization problem, but we will not touch on the details here. I include a reference for those looking to see how at the end of this notebook.\n",
    "\n",
    "$ y^{(i)}\\left(X^{(i)} \\beta\\right) \\geq M $ may look weird, but $X \\beta = 0$ is a formula that defines a hyperplane. \n",
    "\n",
    "For example:\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 = 0 \\text{ is the formula for a line in } 2\\text{-D},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 = 0 \\text{ is the formula for a plane in } 3\\text{-D}.\n",
    "$$\n",
    "\n",
    "So $ y^{(i)}\\left(X^{(i)} \\beta\\right) \\geq M $ just establishes that we want all of our points to fall outside a margin of $M$ on either side of the hyperplane $X_i \\beta=0$.\n",
    "\n",
    "<i>Note: we are using $y\\in \\left\\lbrace -1, 1 \\right\\rbrace$ rather than $y\\in\\left\\lbrace 0, 1 \\right\\rbrace$ to conform with the standard formulation of the algorithm.</i>\n",
    "\n",
    "#### `LinearSVC`\n",
    "\n",
    "Let's now look at how to fit this model with `sklearn`. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\">https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb21da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make model\n",
    "## For now please ignore C=1000 we will touch on it next,\n",
    "## max_iter=100000 just increases the number of iterations for the fitting algorithm\n",
    "\n",
    "\n",
    "## fit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will allow me to plot the separating line\n",
    "## along with the margins\n",
    "\n",
    "# get a grid of x1 values\n",
    "x1x1 = np.linspace(0, 1, 100)\n",
    "\n",
    "# get a grid of x2 values\n",
    "x2x2 = np.linspace(0, 1, 100)\n",
    "\n",
    "# arrange them in a 2D grid\n",
    "X1X1, X2X2 = np.meshgrid(x1x1, x2x2)\n",
    "\n",
    "# make a cleaner array\n",
    "x1x2 = np.vstack([X1X1.ravel(), X2X2.ravel()]).T\n",
    "\n",
    "# get the value of the decision function for this grid\n",
    "Z = max_margin.decision_function(x1x2).reshape(X1X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00477284",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a figure\n",
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "## plot the training data\n",
    "plt.scatter(X[y == -1,0],\n",
    "            X[y == -1,1],\n",
    "            c = \"blue\",\n",
    "            s = 60,\n",
    "            label=\"Training -1\")\n",
    "plt.scatter(X[y == 1,0],\n",
    "            X[y == 1,1],\n",
    "            c = \"orange\",\n",
    "            marker = 'v',\n",
    "            s = 60,\n",
    "            label=\"Training 1\")\n",
    "\n",
    "\n",
    "## plot the separating line, and the margins\n",
    "plt.contour(X1X1, \n",
    "            X2X2, \n",
    "            Z, \n",
    "            colors='k', \n",
    "            levels=[-1, 0, 1], \n",
    "            alpha=.8,\n",
    "            linestyles=['--', '-', '--'])\n",
    "\n",
    "\n",
    "plt.legend(fontsize=16, loc=2)\n",
    "\n",
    "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 16)\n",
    "\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e286b9d",
   "metadata": {},
   "source": [
    "The solid line above is the separating line, and the dotted lines represent the line that is $M$ from the separating line at all points.\n",
    "\n",
    "You can notice that some of the points touch the margin lines, such observations are known as the <i>support vectors</i> because they \"support\" the separating line in the sense that if we move these points slightly, the line will move as well.\n",
    "\n",
    "### Support vector classifiers (or soft margin classifiers)\n",
    "\n",
    "Okay so we now have an algorithm that can separate groups that are separable by hyperplanes. But, there are two possible issues:\n",
    "- Your data may not be able to be separated by a hyperplane\n",
    "- A maximum margin classifier may be too sensitive to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the random data\n",
    "np.random.seed(440)\n",
    "n_rows = 100\n",
    "diff = .1\n",
    "X = np.random.random((n_rows,2))\n",
    "X_prime = X[(X[:,1] - X[:,0]) <= -diff,:]\n",
    "X_2prime = X[(X[:,1] - X[:,0]) >= diff,:]\n",
    "X_3prime = [[.4,.9],[.6,.45],[.7,.9],[.3,.19],[.1,.4]]\n",
    "\n",
    "del X\n",
    "X = np.append(X_prime,np.append(X_2prime,X_3prime,axis = 0),axis=0)\n",
    "\n",
    "y = np.empty(np.shape(X)[0])\n",
    "y[(X[:,1] - X[:,0]) <= -diff] = 1\n",
    "y[((X[:,1] - X[:,0]) >= diff)] = -1\n",
    "y[-5] = 1\n",
    "y[-4] = -1\n",
    "y[-3] = 1\n",
    "y[-2] = -1\n",
    "y[-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(X[y == -1,0],\n",
    "            X[y == -1,1],\n",
    "            c = \"blue\",\n",
    "            s = 60,\n",
    "            label=\"-1\")\n",
    "plt.scatter(X[y == 1,0],\n",
    "            X[y == 1,1],\n",
    "            c = \"orange\",\n",
    "            s = 60,\n",
    "            marker = \"v\",\n",
    "            label=\"1\")\n",
    "\n",
    "plt.legend(fontsize = 16)\n",
    "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c721be6a",
   "metadata": {},
   "source": [
    "Is this new version of the data set linearly separable?\n",
    "\n",
    "No.\n",
    "\n",
    "But, they are <i>almost</i> linearly separable. \n",
    "\n",
    "If we are willing to not have a perfect separation we can still use the maximal margin classifier as our guide for a new algorithm.\n",
    "\n",
    "#### Soften up the Margin Before Classifying\n",
    "\n",
    "As we said before the maximal margin classifier is a <i>hard margin classifier</i>, meaning that instances of class $-1$ are not allowed to cross the decision boundary over into the area occupied by class $1$. But, what if we relaxed that rule.\n",
    "\n",
    "Mathematically we can relax the rule with this new constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\text{find } \\beta \\text{ and maximal } M, \\text{ subject to}\n",
    "$$\n",
    "\n",
    "$$\n",
    "||\\beta||_2^2 = 1, \\text{ and } \n",
    "$$\n",
    "\n",
    "$$\n",
    "y^{(i)}\\left(X^{(i)} \\beta\\right) \\geq M  (1-\\epsilon^{(i)}) \\ \\ \\ \\forall i = 1, \\dots, n,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\epsilon^{(i)} \\geq 0, \\ \\sum_{i=1}^n \\epsilon^{(i)} \\leq \\frac{1}{C},\n",
    "$$\n",
    "\n",
    "where $C\\in \\mathbb{R}_{>0}$ is a hyperparameter that you tune. The $\\epsilon^{(i)}$ are referred to as slack variables because they control how much observation $i$ can violate the margin. If \n",
    "\n",
    "$$\n",
    "\\begin{array}{l l}\n",
    "\\epsilon^{(i)} = 0, & \\text{ then observation } i \\text{ is on the correct side of the margin.} \\\\\n",
    "0 < \\epsilon^{(i)} < 1, & \\text{ then observation } i \\text{ is on the wrong side of the margin, but the correct side of the hyperplane.} \\\\\n",
    "\\epsilon^{(i)} > 1, & \\text{ then observation } i \\text{ is on the incorrect side of the hyperplane.} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The value of $C$ we choose can be thought of as our \"budget\" for slack variables. Larger values of $C$ lead to a smaller budget and so we approach the earlier maximum margin classifier, smaller values of $C$ allow for more slack. Let's demonstrate this with `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c993cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for C in [10,1,.1,.01,.001]:\n",
    "    svc = LinearSVC(C=C, max_iter=100000)\n",
    "    svc.fit(X, y)\n",
    "    \n",
    "    # get a grid of x1 values\n",
    "    x1x1 = np.linspace(-2, 2, 100)\n",
    "\n",
    "    # get a grid of x2 values\n",
    "    x2x2 = np.linspace(-2, 2, 100)\n",
    "\n",
    "    # arrange them in a 2D grid\n",
    "    X1X1, X2X2 = np.meshgrid(x1x1, x2x2)\n",
    "\n",
    "    # make a cleaner array\n",
    "    x1x2 = np.vstack([X1X1.ravel(), X2X2.ravel()]).T\n",
    "\n",
    "    # get the value of the decision function for this grid\n",
    "    Z = svc.decision_function(x1x2).reshape(X1X1.shape)\n",
    "    \n",
    "    ## Make a figure\n",
    "    plt.figure(figsize = (12,12))\n",
    "    \n",
    "    ## plot the training data\n",
    "    plt.scatter(X[y == -1,0],\n",
    "                X[y == -1,1],\n",
    "                c = \"blue\",\n",
    "                s = 60,\n",
    "                label=\"Training -1\")\n",
    "    plt.scatter(X[y == 1,0],\n",
    "                X[y == 1,1],\n",
    "                c = \"orange\",\n",
    "                marker = 'v',\n",
    "                s = 60,\n",
    "                label=\"Training 1\")\n",
    "\n",
    "\n",
    "    ## plot the separating line, and the margins\n",
    "    plt.contour(X1X1, \n",
    "                X2X2, \n",
    "                Z, \n",
    "                colors='k', \n",
    "                levels=[-1, 0, 1], \n",
    "                alpha=.8,\n",
    "                linestyles=['--', '-', '--'])\n",
    "\n",
    "\n",
    "    plt.legend(fontsize=16, loc=2)\n",
    "\n",
    "    plt.xlabel(\"$x_1$\",fontsize = 16)\n",
    "    plt.ylabel(\"$x_2$\",fontsize = 16)\n",
    "    \n",
    "    plt.title(\"C = \" + str(C), fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c43533",
   "metadata": {},
   "source": [
    "$C$ is a hyperparameter and the value of $C$ that works best for your problem can be determined via hyperparameter tuning with cross-validation.\n",
    "\n",
    "For the support vector classifier we consider any point that touches or is on the wrong side of the margin a <i>support vector</i>, that is because these are the observations for which $\\epsilon^{(i)} > 0$. Again these are called support vectors because they \"support\" the decision boundary in a sense. If we were to move one of these support vectors the boundary would change. On the other hand if we moved a non-support vector, the boundary would remain the same (assuming we don't move it too far!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8516f04b",
   "metadata": {},
   "source": [
    "## General support vector machines\n",
    "\n",
    "### Nonlinear decision boundaries\n",
    "\n",
    "Another issue for the linear SVMs we have discussed is the situation where our data are only separably by a nonlinear decision boundary.\n",
    "\n",
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc2cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the random data\n",
    "np.random.seed(440)\n",
    "\n",
    "X = np.zeros((300, 2))\n",
    "X[:100,0] = np.random.randn(100)\n",
    "X[:100,1] = np.random.randn(100)\n",
    "X[100:200,0] = np.random.randn(100) + 4.5\n",
    "X[100:200,1] = np.random.randn(100) + 4.5\n",
    "X[200:,0] = np.random.randn(100) - 4.5\n",
    "X[200:,1] = np.random.randn(100) - 4.5\n",
    "\n",
    "y = np.zeros(300) - 1\n",
    "y[:100] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cc20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "\n",
    "plt.scatter(X[y==1,0], \n",
    "            X[y==1,1], \n",
    "            c='orange',\n",
    "            marker = 'v',\n",
    "            label=\"1\")\n",
    "plt.scatter(X[y==-1,0], \n",
    "            X[y==-1,1], \n",
    "            c='blue', \n",
    "            label=\"-1\")\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"2-D Example\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33915a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(440)\n",
    "\n",
    "X = 2*np.random.random(50) - 1\n",
    "y = np.ones(len(X))\n",
    "y[(X > .3) | (X < -.3)] = -1\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plt.scatter(X[y==-1],\n",
    "            np.ones(sum(y==-1)),\n",
    "            c=\"blue\",\n",
    "            label=\"-1\")\n",
    "plt.scatter(X[y==1],\n",
    "            np.ones(sum(y==1)),\n",
    "            c=\"orange\",\n",
    "            marker = 'v',\n",
    "            label=\"1\")\n",
    "plt.yticks([])\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(\"$x_1$\",fontsize=16)\n",
    "\n",
    "plt.title(\"1-D Example\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c4d95",
   "metadata": {},
   "source": [
    "Both of these examples cannot be separated by any linear decision boundary. So what can we do?\n",
    "\n",
    "### Lifting to higher dimensions\n",
    "\n",
    "While these examples cannot be linearly separated (or near linearly separated) in their current dataspace, we can separate them if \"lift them\" into a higher dimensional space in the correct way. Let's see an example with the $1$-D data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c0e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a feature x_2 to X, that is just x_1^2\n",
    "## make sure it is an np array with the first column as x_1\n",
    "## and the second column as x_2\n",
    "X_new = np.zeros((len(X),2))\n",
    "X_new[:,0] = X.copy()\n",
    "X_new[:,1] = X**2\n",
    "\n",
    "## Now plot it using the below code\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plt.scatter(X_new[y==-1,0],\n",
    "            X_new[y==-1,1],\n",
    "            c=\"blue\",\n",
    "            label=\"-1\")\n",
    "plt.scatter(X_new[y==1,0],\n",
    "            X_new[y==1,1],\n",
    "            c=\"orange\",\n",
    "            marker='v',\n",
    "            label=\"1\")\n",
    "\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(\"$x_1$\",fontsize=16)\n",
    "plt.ylabel(\"$x_2 = x_1^2$\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1091cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit a LinearSVC here\n",
    "svc = \n",
    "\n",
    "\n",
    "svc.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb571b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a grid of x1 values\n",
    "x1x1 = np.linspace(-1.1, 1.1, 100)\n",
    "\n",
    "# get a grid of x2 values\n",
    "x2x2 = np.linspace(-.1, 1.1, 100)\n",
    "\n",
    "# arrange them in a 2D grid\n",
    "X1X1, X2X2 = np.meshgrid(x1x1, x2x2)\n",
    "\n",
    "# make a cleaner array\n",
    "x1x2 = np.vstack([X1X1.ravel(), X2X2.ravel()]).T\n",
    "\n",
    "# get the value of the decision function for this grid\n",
    "Z = svc.decision_function(x1x2).reshape(X1X1.shape)\n",
    "\n",
    "\n",
    "## Now plot it using the below code\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plt.scatter(X_new[y==-1,0],\n",
    "            X_new[y==-1,1],\n",
    "            c=\"blue\",\n",
    "            label=\"-1\")\n",
    "plt.scatter(X_new[y==1,0],\n",
    "            X_new[y==1,1],\n",
    "            c=\"orange\",\n",
    "            marker = 'v',\n",
    "            label=\"1\")\n",
    "\n",
    "plt.contour(X1X1, \n",
    "            X2X2, \n",
    "            Z, \n",
    "            colors='k', \n",
    "            levels=[0], \n",
    "            alpha=.8, \n",
    "            linestyles=['-'])\n",
    "\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(\"$x_1$\",fontsize=16)\n",
    "plt.ylabel(\"$x_2 = x_1^2$\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6968da93",
   "metadata": {},
   "source": [
    "This trick is the essence of the more general support vector machine. We take data in a lower dimension, somehow embed it into a higher dimension where it is hopefully close to linearly separable.\n",
    "\n",
    "However, one issue that arises is how to choose such an embedding. Should we arbitrarily add powers and interactions of features? What about some nonlinear transformations? In this process we could easily end up with a magnitude of potential features, which in turn would cause our computational cost to skyrocket. Luckily support vector machines have a nice way to handle such a problem.\n",
    "\n",
    "### Kernel functions\n",
    "\n",
    "While we have not explicitly discussed how the solution to the support vector classifier is computed, because it is  a bit long and technical, it turns out that it solely involves inner products between the training observations. For SVMs the inner product between two vectors $a,b\\in \\mathbb{R}^m$ is computed as:\n",
    "\n",
    "$$\n",
    "\\left\\langle a,b \\right\\rangle = \\sum_{i=1}^m a_i b_i.\n",
    "$$\n",
    "\n",
    "So if we have some function, $\\phi$ that takes our observations into a higher dimensionall space in order to separate them, then we will need to  compute $\\left\\langle \\phi(X^*), \\phi(X^\\#) \\right\\rangle$ for pairs of observations $X^*$ and $X^\\#$ (in actuality our computer will do this for us). $\\phi$ can ultimately take our lower dimensional data into very very high dimensional space, even spaces which are considered infinite dimensional. This would make it nearly impossible (or actually impossible) to compute the inner products needed to estimate the separating boundary. But, what if we did not need to actually compute $\\left\\langle \\phi(X^*), \\phi(X^\\#) \\right\\rangle$ in the higher dimensional space? Let's look at an example:\n",
    "\n",
    "Let's say our data has two features $x_1$ and $x_2$ and our $\\phi$ is like so:\n",
    "$$\n",
    "\\phi\\left(\\left(.\\begin{array}{c} x_1 \\\\ x_2 \\end{array} \\right)\\right) = \\left(\\begin{array}{c} x_1^2 \\\\ \\sqrt{2} x_1 x_2 \\\\ x_2^2 \\end{array} \\right).\n",
    "$$\n",
    "For two vectors $a$ and $b$ the inner product between $\\phi(a)$ and $\\phi(b)$ is:\n",
    "$$\n",
    "\\left\\langle \\phi(a) , \\phi(b) \\right\\rangle= a_1^2 b_1^2 + 2 a_1b_1a_2b_2 + a_2^2b_2^2 = (a_1 b_1 + a_2 b_2)^2 = \\left\\langle  a,b\\right\\rangle^2\n",
    "$$\n",
    "\n",
    "So even though we knew what the map $\\phi$ was to the higher space, we didn't need to know what it was to compute the dot product $\\left\\langle \\phi(a) , \\phi(b) \\right\\rangle$!\n",
    "\n",
    "In this context we say a map $\\phi$ has a <i>kernel function</i>, $K$, if $\\left\\langle \\phi(a) , \\phi(b) \\right\\rangle = K(a,b)$ where $K$ is only a function of $a$ and $b$ in the original feature space. So for the example mapping we looked at the kernel function is $K(a,b) = (\\left\\langle  a,b\\right\\rangle)^2$.\n",
    "\n",
    "Common kernel functions for support vector machines are:\n",
    "- Linear, $K(a,b) = \\left\\langle  a,b\\right\\rangle$, this gives the linear classifiers we looked at earlier,\n",
    "- Polynomial, $K(a,b) = (\\gamma \\left\\langle  a,b\\right\\rangle + r)^d$, where $\\gamma$ is a hyperparameter to be tuned,\n",
    "- Gaussian Radial Kernel (Gaussian RBF), $K(a,b) = \\exp\\left( -\\gamma ||a-b||^2 \\right)$, where $||\\bullet ||$ is the Euclidean norm,\n",
    "- Sigmoid, $K(a,b) = \\tanh(\\gamma \\left\\langle  a,b\\right\\rangle + r)$\n",
    "\n",
    "\n",
    "Let's now demonstrate how to implement this in `sklearn` using our two examples from above.\n",
    "\n",
    "We will be using `SVC` from `sklearn.svm`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3824cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now plot it using the below code\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "\n",
    "plt.scatter(X[y==-1],\n",
    "            np.ones(sum(y==-1)),\n",
    "            c=\"blue\",\n",
    "            label=\"-1\")\n",
    "plt.scatter(X[y==1],\n",
    "            np.ones(sum(y==1)),\n",
    "            c=\"orange\",\n",
    "            marker = 'v',\n",
    "            label=\"1\")\n",
    "plt.yticks([])\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(\"$x_1$\",fontsize=16)\n",
    "\n",
    "plt.title(\"1-D Example\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f052fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a model\n",
    "## Set kernel = 'poly', degree = 2, C = 1000\n",
    "svc = \n",
    "\n",
    "## fit a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22aa726",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plt.scatter(X[y==-1],\n",
    "            np.ones(sum(y==-1)),\n",
    "            c=\"blue\",\n",
    "            label=\"-1\")\n",
    "plt.scatter(X[y==1],\n",
    "            np.ones(sum(y==1)),\n",
    "            c=\"orange\",\n",
    "            marker = 'v',\n",
    "            label=\"1\")\n",
    "plt.yticks([])\n",
    "\n",
    "decs = svc.decision_function(np.linspace(-1,1,1000).reshape(-1,1)).round(1)\n",
    "plt.scatter(np.linspace(-1,1,1000)[decs==0], \n",
    "            np.ones(1000)[decs==0], \n",
    "            marker = 'x', \n",
    "            s=200, c='k', \n",
    "            label=\"Decision Boundary\")\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(\"$x_1$\",fontsize=16)\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"1-D Example\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edde8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the random data\n",
    "np.random.seed(440)\n",
    "\n",
    "X = np.zeros((300, 2))\n",
    "X[:100,0] = np.random.randn(100)\n",
    "X[:100,1] = np.random.randn(100)\n",
    "X[100:200,0] = np.random.randn(100) + 4.5\n",
    "X[100:200,1] = np.random.randn(100) + 4.5\n",
    "X[200:,0] = np.random.randn(100) - 4.5\n",
    "X[200:,1] = np.random.randn(100) - 4.5\n",
    "\n",
    "y = np.zeros(300)-1\n",
    "y[:100] = 1\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "\n",
    "plt.scatter(X[y==1,0], \n",
    "            X[y==1,1], \n",
    "            c='orange', \n",
    "            marker='v',\n",
    "            label=\"1\")\n",
    "plt.scatter(X[y==-1,0], \n",
    "            X[y==-1,1], \n",
    "            c='blue', \n",
    "            label=\"-1\")\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"2-D Example\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fea6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make two new svc models\n",
    "\n",
    "## one with poly, degree=2, C = 10\n",
    "svc_poly = \n",
    "\n",
    "## one with rbf kernel, C = 10\n",
    "svc_rbf = \n",
    "\n",
    "\n",
    "## fit a model\n",
    "svc_poly.fit(X, y)\n",
    "svc_rbf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(20,10))\n",
    "\n",
    "\n",
    "ax[0].scatter(X[y==1,0], \n",
    "              X[y==1,1], \n",
    "              c='orange', \n",
    "              marker='v',\n",
    "              label=\"1\")\n",
    "ax[0].scatter(X[y==-1,0], \n",
    "              X[y==-1,1], \n",
    "              c='blue', \n",
    "              label=\"-1\")\n",
    "\n",
    "ax[1].scatter(X[y==1,0], \n",
    "              X[y==1,1], \n",
    "              c='orange', \n",
    "              marker='v',\n",
    "              label=\"1\")\n",
    "ax[1].scatter(X[y==-1,0], \n",
    "              X[y==-1,1], \n",
    "              c='blue', \n",
    "              label=\"-1\")\n",
    "\n",
    "\n",
    "# get a grid of x1 values\n",
    "x1x1 = np.linspace(-8, 8, 100)\n",
    "\n",
    "# get a grid of x2 values\n",
    "x2x2 = np.linspace(-8, 8, 100)\n",
    "\n",
    "# arrange them in a 2D grid\n",
    "X1X1, X2X2 = np.meshgrid(x1x1, x2x2)\n",
    "\n",
    "# make a cleaner array\n",
    "x1x2 = np.vstack([X1X1.ravel(), X2X2.ravel()]).T\n",
    "\n",
    "# get the value of the decision function for this grid\n",
    "Z_poly = svc_poly.decision_function(x1x2).reshape(X1X1.shape)\n",
    "Z_rbf = svc_rbf.decision_function(x1x2).reshape(X1X1.shape)\n",
    "\n",
    "\n",
    "ax[0].contour(X1X1, X2X2, Z_poly,\n",
    "              colors='k', levels=[-1,0,1], \n",
    "              alpha=.8, linestyles=['--','-','--'])\n",
    "ax[1].contour(X1X1, X2X2, Z_rbf,\n",
    "              colors='k', levels=[-1,0,1], \n",
    "              alpha=.8, linestyles=['--','-','--'])\n",
    "\n",
    "ax[0].set_title(\"Polynomial Kernel\", fontsize=20)\n",
    "ax[1].set_title(\"RBF Kernel\", fontsize=20)\n",
    "\n",
    "ax[0].set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax[1].set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax[0].set_ylabel(\"$x_2$\", fontsize=18)\n",
    "ax[1].set_ylabel(\"$x_2$\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77738705",
   "metadata": {},
   "source": [
    "### References for mathematical details\n",
    "\n",
    "- Chapter 4 of Elements of Statistical Learning for the maximum margin classifier,\n",
    "- Chapter 12 of Elements of Statistical Learning for the support vector classifier and\n",
    "- Chapter 12 of Elements of Statistical Learning for the general support vector machine.\n",
    "\n",
    "<a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\">https://web.stanford.edu/~hastie/ElemStatLearn/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fb5c7",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02acf263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
