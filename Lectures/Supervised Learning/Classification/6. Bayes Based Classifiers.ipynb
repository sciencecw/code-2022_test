{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd17e886",
   "metadata": {},
   "source": [
    "# Bayes' Based Classifiers\n",
    "\n",
    "We will now introduce a series of classification algorithms based on Bayes' rule from probability theory.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Review Bayes' rule,\n",
    "- Introduce a classification model framework for the three models we consider,\n",
    "- Reintroduce the iris data set and\n",
    "- Demonstrate:\n",
    "    - Linear discriminant analysis,\n",
    "    - Quadratic discriminant analysis and\n",
    "    - Na&#xEF;ve Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019c7d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bfa46c",
   "metadata": {},
   "source": [
    "## Review of Bayes' rule\n",
    "\n",
    "Assume that we have some probability space, $\\Omega$.\n",
    "\n",
    "### Conditional probability\n",
    "\n",
    "Remember that for events $A$ and $B$ with $P(B)\\neq0$ we define the probability of $A$ conditional on $B$ as:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A\\cap B)}{P(B)}.\n",
    "$$\n",
    "\n",
    "This definition can be visualized with this graphic:\n",
    "\n",
    "<img src=\"cond_prob.png\" width=\"40%\"></img>\n",
    "\n",
    "### Law of total probability\n",
    "\n",
    "If $B_1, \\ B_2, \\dots, B_n$ are disjoint events such that $\\cup_{i=1}^n B_i= \\Omega$, then it holds that:\n",
    "\n",
    "$$\n",
    "P(A) = \\sum_{i=1}^n P(A \\cap B_i),\n",
    "$$\n",
    "\n",
    "for any event $A$.\n",
    "\n",
    "The law of total probability can be visualized with this graphic:\n",
    "\n",
    "<img src=\"tot_prob.png\" width=\"40%\"></img>\n",
    "\n",
    "### Bayes' rule\n",
    "\n",
    "For events $A$ and $B$ with $P(B)\\neq0$, then Bayes' rule (or the Bayesâ€“Price theorem) is\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) P(A)}{P(B)}.\n",
    "$$\n",
    "\n",
    "This is sometimes taken a step further using the law of total probability for example:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) P(A)}{P(B \\cap A) + P(B \\cap A^c)} = \\frac{P(B|A) P(A)}{P(B|A)P(A) + P(B|A^c)P(A^c)},\n",
    "$$\n",
    "\n",
    "where $A^c = \\Omega - A$.\n",
    "\n",
    "For a nice visualization of conditional probability and Baye's rule check out this blog post, <a href=\"https://oscarbonilla.com/2009/05/visualizing-bayes-theorem/\">https://oscarbonilla.com/2009/05/visualizing-bayes-theorem/</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20096c4f",
   "metadata": {},
   "source": [
    "## Using Bayes' rule for classification\n",
    "\n",
    "Suppose we have a set of $m$ features collected in a matrix $X$ and an output variable $y$ that can take on any of $\\mathcal{C}$ possible categories.\n",
    "\n",
    "In the case of logistic regression, $\\mathcal{C} = 2$, and we modeled:\n",
    "\n",
    "$$\n",
    "P(y=1|X=X^*),\n",
    "$$\n",
    "\n",
    "in order to make predictions.\n",
    "\n",
    "However, we can use Bayes' rule to rewrite this expression like so:\n",
    "\n",
    "$$\n",
    "P(y=c|X=X^*) = \\frac{\\pi_c f_c(X^*)}{\\sum_{l=1}^\\mathcal{C} \\pi_l f_l(X^*)},\n",
    "$$\n",
    "\n",
    "where $\\pi_c$ denotes the <i>prior</i> probability that a random observation comes from the $c^\\text{th}$ class and $f_c(X) \\equiv P(X|y=c)$ (<i>note this form assumes that $X$ is a qualitative variable, but is easily rewritten if $X$ is continuous</i>).\n",
    "\n",
    "When fitting this model we estimate the $\\pi_l$s by simply taking the fraction of the sample set belonging to each of the $\\mathcal{C}$ classes. However, for the $f_l(X)$s, we have to make some assumptions.\n",
    "\n",
    "\n",
    "Different assumptions on the $f_l(X)$s result in different algorithms. We will present three models that follow from three different assumptions.\n",
    "\n",
    "### A return to the `iris` data set\n",
    "\n",
    "In this notebook we will illustrate the algorithms with our trusty `iris` data set, which provides four measurements (sepal length, sepal width, petal length and petal width) for $150$ irises of three distinct types ($50$ for each type).\n",
    "\n",
    "We load this data set now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e065dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "X = iris['data']\n",
    "X = X.rename(columns={'sepal length (cm)':'sepal_length',\n",
    "                         'sepal width (cm)':'sepal_width',\n",
    "                         'petal length (cm)':'petal_length',\n",
    "                         'petal width (cm)':'petal_width'})\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed567d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.copy(), y,\n",
    "                                                       shuffle=True,\n",
    "                                                       random_state=413,\n",
    "                                                       test_size=.2,\n",
    "                                                       stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                s=60,\n",
    "                label='$y=0$')\n",
    "plt.scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                s=60,\n",
    "                marker = 'v',\n",
    "                label='$y=1$')\n",
    "plt.scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                s=60,\n",
    "                marker = 'x',\n",
    "                label='$y=2$')\n",
    "\n",
    "plt.xlabel(\"Petal Width (cm)\", fontsize=18)\n",
    "plt.ylabel(\"Petal Length (cm)\", fontsize=18)\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b7a1e",
   "metadata": {},
   "source": [
    "## Linear discriminant analysis (LDA)\n",
    "\n",
    "The first model we discuss is know as <i>linear discriminant analysis</i> or LDA. Note that LDA is an ambiguous acronym in data science/machine learning because it can also stand for latent Dirichlet allocation, but in these notes we will use it to refer to linear discriminant analysis.\n",
    "\n",
    "### Model assumption\n",
    "\n",
    "In LDA we will assume that $X|y=c$ is Gaussian. What that means is dependent upon the number of features, $m$.\n",
    "\n",
    "#### A single feature, $m=1$\n",
    "\n",
    "For a single feature we assume that \n",
    "\n",
    "$$\n",
    "f_c(X) = \\frac{1}{\\sqrt{2\\pi}\\sigma_c} \\exp \\left( -\\frac{1}{2\\sigma_c^2} (X - \\mu_c)^2 \\right),\n",
    "$$\n",
    "\n",
    "which is the probability density function of a normal random variable with mean $\\mu_c$ and standard deviation $\\sigma_c$. In linear discriminant analysis we assume that $\\sigma_1 = \\sigma_2 = \\dots = \\sigma_\\mathcal{C} = \\sigma$.\n",
    "\n",
    "This assumption leads to the following estimate of $P(y=c|X)$\n",
    "\n",
    "$$\n",
    "P(y=c|X) = \\frac{\\pi_c \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(X-\\mu_c)^2\\right)}{\\sum_{l=1}^\\mathcal{C} \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(X-\\mu_l)^2\\right)}.\n",
    "$$\n",
    "\n",
    "In this setting we would estimate $\\mu_c$ and $\\sigma$ with the following formulae:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_c = \\frac{1}{n_c} \\sum_{i:y_i = c} X_i,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n-\\mathcal{C}} \\sum_{c=1}^\\mathcal{C} \\sum_{i:y_i = c}(X_i - \\hat{\\mu}_c)^2.\n",
    "$$\n",
    "\n",
    "#### Making classifications\n",
    "\n",
    "We typically make classifications in this setting by choosing the class, $c$, for which $P(y=c|X)$ is largest, through some algebra and $\\log$ manipulations you can show that this is equivalent to choosing the class, $c$, for which the <i>discriminant function</i> is largest where:\n",
    "\n",
    "$$\n",
    "\\delta_c = X \\frac{\\mu_c}{\\sigma^2} - \\frac{\\mu_c^2}{2\\sigma^2} + \\log\\left(\\pi_c\\right)\n",
    "$$\n",
    "\n",
    "is the discriminant function for class $c$. <i>We estimate this function with the $\\hat{\\mu}_c$s and $\\hat{\\sigma}$.</i>\n",
    "\n",
    "Let's apply this to our iris data set. We will use petal length as our single feature for this LDA model.\n",
    "\n",
    "LDA is implemented in `sklearn` with `LinearDiscriminantAnalysis`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\">https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1588a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import linear discriminant analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1305ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the model object\n",
    "LDA = \n",
    "\n",
    "## Fit the object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstrate predict_proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86559a",
   "metadata": {},
   "source": [
    "#### Demonstrating what LDA is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining the discriminant function\n",
    "def delta_c(x, mu_hat, sigma_hat_sq, pi_c):\n",
    "    return x*(mu_hat/sigma_hat_sq) - mu_hat**2/(2*sigma_hat_sq) + np.log(pi_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we estimate the means of the three normal distributions\n",
    "mu_0_hat = np.mean(X_train.loc[y_train==0].petal_length)\n",
    "mu_1_hat = np.mean(X_train.loc[y_train==1].petal_length)\n",
    "mu_2_hat = np.mean(X_train.loc[y_train==2].petal_length)\n",
    "\n",
    "## Then the common variance\n",
    "sigma_hat_sq = np.sum(np.power(X_train.loc[y_train==0].petal_length - mu_0_hat,2))\n",
    "sigma_hat_sq = sigma_hat_sq + np.sum(np.power(X_train.loc[y_train==1].petal_length - mu_1_hat,2))\n",
    "sigma_hat_sq = sigma_hat_sq + np.sum(np.power(X_train.loc[y_train==2].petal_length - mu_2_hat,2))\n",
    "sigma_hat_sq = sigma_hat_sq/(len(y_train)-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c4bb8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot the sample distributions\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.hist(X_train.loc[y_train==0].petal_length,alpha=.6, \n",
    "         label=\"y=0\",\n",
    "         edgecolor=\"black\")\n",
    "plt.hist(X_train.loc[y_train==1].petal_length,alpha=.6, \n",
    "         label=\"y=1\",\n",
    "         hatch = \"\\\\\",\n",
    "         edgecolor=\"black\")\n",
    "plt.hist(X_train.loc[y_train==2].petal_length,alpha=.6, \n",
    "         label=\"y=2\",\n",
    "         hatch='//',\n",
    "         edgecolor=\"black\")\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.title(\"The Actual Sample Distributions\", fontsize=20)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlim(0,7)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "## Plot the fit normal distributions\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.hist(np.sqrt(sigma_hat_sq)*np.random.randn(1000)+mu_0_hat,alpha=.6, \n",
    "         label=\"y=0\",\n",
    "         edgecolor=\"black\")\n",
    "plt.hist(np.sqrt(sigma_hat_sq)*np.random.randn(1000)+mu_1_hat,alpha=.6, \n",
    "         label=\"y=1\",\n",
    "         hatch='\\\\',\n",
    "         edgecolor=\"black\")\n",
    "plt.hist(np.sqrt(sigma_hat_sq)*np.random.randn(1000)+mu_2_hat,alpha=.6, \n",
    "         label=\"y=2\",\n",
    "         hatch='//',\n",
    "         edgecolor=\"black\")\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.title(\"The Fit Normal Distributions\", fontsize=20)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlim(0,7)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "## Plot the discriminant lines\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(np.linspace(X_train.petal_width.min()-1,\n",
    "                        X_train.petal_length.max()+1,100),\n",
    "        delta_c(np.linspace(X_train.petal_width.min()-1,\n",
    "                        X_train.petal_length.max()+1,100), mu_0_hat, sigma_hat_sq, 1/3),\n",
    "            label=\"delta_0_hat\")\n",
    "\n",
    "plt.plot(np.linspace(X_train.petal_length.min()-1,\n",
    "                        X_train.petal_length.max()+1,100),\n",
    "        delta_c(np.linspace(X_train.petal_length.min()-1,\n",
    "                        X_train.petal_length.max()+1,100), mu_1_hat, sigma_hat_sq, 1/3),\n",
    "            '--',\n",
    "            label=\"delta_1_hat\")\n",
    "\n",
    "\n",
    "plt.plot(np.linspace(X_train.petal_length.min()-1,\n",
    "                        X_train.petal_length.max()+1,100),\n",
    "        delta_c(np.linspace(X_train.petal_length.min()-1,\n",
    "                        X_train.petal_length.max()+1,100), mu_2_hat, sigma_hat_sq, 1/3),\n",
    "            '-.',\n",
    "            label=\"delta_2_hat\")\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.title(\"The Estimated Descriminant Lines\", fontsize=20)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlim(0,7)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Plot the model predictions\n",
    "x = np.linspace(0.01, X_train.petal_length.max()+1,100)\n",
    "y = LDA.predict(x.reshape(-1,1))\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(x[y==0], y[y==0], c='blue')\n",
    "plt.scatter(x[y==1], y[y==1], c='orange')\n",
    "plt.scatter(x[y==2], y[y==2], c='green')\n",
    "\n",
    "plt.xlabel(\"Petal Length (cm)\", fontsize=18)\n",
    "plt.ylabel(\"Predicted Iris Class\", fontsize=18)\n",
    "plt.title(\"Model Predictions\", fontsize=20)\n",
    "plt.yticks([0,1,2], fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlim(0,7)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa965c1",
   "metadata": {},
   "source": [
    "### Multiple features, $m>1$\n",
    "\n",
    "When we have multiple features we'd like to use to predict $y$ we assume the $f_c(X)$ is a probability density function for a multivariate normal distribution with a class specific mean vector and a common covariance matrix. In other words, each column of $X$ is assumed to be a normal distribution whose mean is dependant upon the class you are looking at, and the columns of $X$ also have some correlation with one another which is identical across the possible classes. This is denoted as $X|y=c\\sim N(\\mu_c, \\Sigma)$, where $\\mu_c = E(X|y=c)$ and $\\text{cov}(X) = \\Sigma$.\n",
    "\n",
    "A bivariate normal is shown in this image from Wikipedia:\n",
    "\n",
    "<img src=\"Multivariate_Gaussian.png\" width=\"60%\"></img>\n",
    "\n",
    "<i>Source: <a href=\"https://en.wikipedia.org/wiki/Multivariate_normal_distribution\">https://en.wikipedia.org/wiki/Multivariate_normal_distribution</a></i>\n",
    "\n",
    "In this case $f_c(X)$ is given as follows:\n",
    "\n",
    "$$\n",
    "f_c(X) = \\frac{1}{(2\\pi)^{m/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(X-\\mu_c)^T \\Sigma^{-1} (X-\\mu_c) \\right),\n",
    "$$\n",
    "\n",
    "which will result in a class specific discriminant function of:\n",
    "\n",
    "$$\n",
    "\\delta_c(X) = X^T \\Sigma^{-1} \\mu_c - \\frac{1}{2}\\mu_c^T\\Sigma^{-1} \\mu_c + \\log(\\pi_c).\n",
    "$$\n",
    "\n",
    "The LDA classifier will select the class, $c$, with highest estimated $\\delta_c(X)$.\n",
    "\n",
    "Estimation of the $\\mu_k$ and $\\Sigma$ are similar to the single feature case.\n",
    "\n",
    "Implementation in `sklearn` is identical to the single feature case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8885d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a new model object\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "## Fit that model\n",
    "LDA.fit(X_train[['petal_width', 'petal_length']].values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making a grid\n",
    "p_width_min, p_width_max = X_train.petal_width.min()-.1, X_train.petal_width.max()+.1\n",
    "p_length_min, p_length_max = X_train.petal_length.min()-.1, X_train.petal_length.max()+.1\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.arange(p_width_min, p_width_max, .01),\n",
    "                          np.arange(p_length_min, p_length_max, .01))\n",
    "\n",
    "X_pred = np.zeros((len(xx1.reshape(-1,1)), 2))\n",
    "X_pred[:,0] = xx1.flatten()\n",
    "X_pred[:,1] = xx2.flatten()\n",
    "\n",
    "preds = LDA.predict(X_pred)\n",
    "\n",
    "\n",
    "\n",
    "## plotting the decision boundary with the training points\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(X_pred[preds==0,0],\n",
    "            X_pred[preds==0,1],\n",
    "            alpha=.01)\n",
    "plt.scatter(X_pred[preds==1,0],\n",
    "            X_pred[preds==1,1],\n",
    "            alpha=.01)\n",
    "plt.scatter(X_pred[preds==2,0],\n",
    "            X_pred[preds==2,1],\n",
    "            alpha=.01,\n",
    "            color='lightgreen')\n",
    "\n",
    "plt.scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                label='Training 0',\n",
    "                c = 'blue',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "plt.scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                label='Training 1',\n",
    "                c = 'darkorange',\n",
    "                marker='v',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "plt.scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                label='Training 2',\n",
    "                c = 'darkgreen',\n",
    "                marker = 'x',\n",
    "                s=100)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Petal Width (cm)\", fontsize=18)\n",
    "plt.ylabel(\"Petal Length (cm)\", fontsize=18)\n",
    "plt.legend(fontsize=16, loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e5bcc",
   "metadata": {},
   "source": [
    "## Quadratic discriminant analysis (QDA)\n",
    "\n",
    "In <i>quadratic discriminant analysis</i> (QDA) we relax the assumption that the covariance matrix is the same across all classes. We thus have that $X|y=c \\sim N(\\mu_c, \\Sigma_c)$, where $\\Sigma_c$ is the covariance matrix of $X|y=c$.\n",
    "\n",
    "When we perform QDA we assign to $X^*$ the class for which:\n",
    "\n",
    "$$\n",
    "\\delta_c(X^*)  = -\\frac{1}{2} \\left( X^* - \\mu_c \\right)^T \\Sigma_c^{-1}  \\left(X^* - \\mu_c  \\right) - \\frac{1}{2}\\log\\left(|\\Sigma_c| \\right) + \\log(\\pi_c) \n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{2} X^{*T} \\sigma^{-1}_c X^* + X^{*T} \\sigma^{-1}_c \\mu_c - \\frac{1}{2} \\mu_c^T \\sigma_c^{-1} \\mu_c - \\frac{1}{2}\\log\\left(|\\Sigma_c| \\right) + \\log(\\pi_c)\n",
    "$$\n",
    "\n",
    "Let's demonstrate the difference using our `iris` data set and `sklearn`.\n",
    "\n",
    "QDA is implemented in `sklearn` with `QuadraticDiscriminantAnalysis`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html\">https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d757c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing QDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a QDA object\n",
    "QDA = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f641bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c97764",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making a grid\n",
    "p_width_min, p_width_max = X_train.petal_width.min()-.1, X_train.petal_width.max()+.1\n",
    "p_length_min, p_length_max = X_train.petal_length.min()-.1, X_train.petal_length.max()+.1\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.arange(p_width_min, p_width_max, .01),\n",
    "                          np.arange(p_length_min, p_length_max, .01))\n",
    "\n",
    "X_pred = np.zeros((len(xx1.reshape(-1,1)), 2))\n",
    "X_pred[:,0] = xx1.flatten()\n",
    "X_pred[:,1] = xx2.flatten()\n",
    "\n",
    "LDA_preds = LDA.predict(X_pred)\n",
    "QDA_preds = QDA.predict(X_pred)\n",
    "\n",
    "\n",
    "\n",
    "## plotting the decision boundary with the training points\n",
    "fig,ax = plt.subplots(1,2,figsize=(16,8))\n",
    "\n",
    "\n",
    "## LDA first\n",
    "### Decision Boundaries\n",
    "ax[0].scatter(X_pred[LDA_preds==0,0],\n",
    "            X_pred[LDA_preds==0,1],\n",
    "            alpha=.01)\n",
    "ax[0].scatter(X_pred[LDA_preds==1,0],\n",
    "            X_pred[LDA_preds==1,1],\n",
    "            alpha=.01)\n",
    "ax[0].scatter(X_pred[LDA_preds==2,0],\n",
    "            X_pred[LDA_preds==2,1],\n",
    "            c='lightgreen',\n",
    "            alpha=.01)\n",
    "\n",
    "### Training Points\n",
    "ax[0].scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                label='Training 0',\n",
    "                c = 'blue',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "ax[0].scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                label='Training 1',\n",
    "                marker='v',\n",
    "                c = 'darkorange',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "ax[0].scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                label='Training 2',\n",
    "                marker='x',\n",
    "                c = 'darkgreen',\n",
    "                s=100)\n",
    "ax[0].set_title(\"LDA Decision Boundary\", fontsize=20)\n",
    "ax[0].set_xlabel(\"Petal Width (cm)\", fontsize=16)\n",
    "ax[0].set_ylabel(\"Petal Length (cm)\", fontsize=16)\n",
    "\n",
    "\n",
    "\n",
    "## QDA second\n",
    "### Decision Boundaries\n",
    "ax[1].scatter(X_pred[QDA_preds==0,0],\n",
    "            X_pred[QDA_preds==0,1],\n",
    "            alpha=.01)\n",
    "ax[1].scatter(X_pred[QDA_preds==1,0],\n",
    "            X_pred[QDA_preds==1,1],\n",
    "            alpha=.01)\n",
    "ax[1].scatter(X_pred[QDA_preds==2,0],\n",
    "            X_pred[QDA_preds==2,1],\n",
    "            c='lightgreen',\n",
    "            alpha=.01)\n",
    "\n",
    "### Training Points\n",
    "ax[1].scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                label='Training 0',\n",
    "                c = 'blue',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "ax[1].scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                label='Training 1',\n",
    "                c = 'darkorange',\n",
    "                marker='v',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "ax[1].scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                label='Training 2',\n",
    "                c = 'darkgreen',\n",
    "                marker='x',\n",
    "                s=100)\n",
    "\n",
    "ax[1].set_title(\"QDA Decision Boundary\", fontsize=20)\n",
    "ax[1].set_xlabel(\"Petal Width (cm)\", fontsize=16)\n",
    "ax[1].set_ylabel(\"Petal Length (cm)\", fontsize=16)\n",
    "\n",
    "\n",
    "\n",
    "ax[0].legend(fontsize=16, loc=2)\n",
    "ax[1].legend(fontsize=16, loc=2)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d7c9b",
   "metadata": {},
   "source": [
    "As we can see a key difference between LDA and QDA as classifiers is the shape of the decision boundary. \n",
    "\n",
    "- The LDA decision boundary is restricted to linear separation. \n",
    "    - In 2-D this means that our feature plane is split using lines, in 3-D planes, in higher dimensions hyperplanes. \n",
    "- The QDA decision boundary can be nonlinear, which gives it more flexibility.\n",
    "\n",
    "Recalling our bias-variance tradeoff, LDA has more bias, while QDA has more variance. The one that you want depends on your data set. In this example I think the LDA model would better generalize to additional iris observations.\n",
    "\n",
    "#### LDA vs QDA\n",
    "\n",
    "##### When LDA\n",
    "\n",
    "- LDA works better than QDA for smaller data sets, because we don't have to estimate as many parameters\n",
    "    - This is because of the $\\Sigma$ assumption, here we only have to estimate $m(m+1)/2$ covariances, while in QDA you have to estimate $\\mathcal{C} m (m+1)/2$\n",
    "- LDA works better if you think your data can be mostly separated by linear decision boundaries. This can be guessed at by examining the training data\n",
    "\n",
    "##### When QDA\n",
    "\n",
    "- QDA needs a large data set to properly fit\n",
    "    - See above argument\n",
    "- QDA is preferable to LDA if you suspect that the data is better separated by a nonlinear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5712eb",
   "metadata": {},
   "source": [
    "## Na&#xEF;ve Bayes classifier\n",
    "\n",
    "Recall that we are using Bayes' rule to estimate $P(y=c|X)$:\n",
    "\n",
    "$$\n",
    "P(y=c|X=X^*) = \\frac{\\pi_c f_c(X^*)}{\\sum_{l=1}^\\mathcal{C} \\pi_l f_l(X^*)}.\n",
    "$$\n",
    "\n",
    "In order to make this estimate we have to:\n",
    "- estimate $\\mathcal{C}$ $\\pi_c$s:\n",
    "    - this is straightforward\n",
    "- estimate $\\mathcal{C}$ $m$-dimensional density functions, $f_c(X)$\n",
    "    - not so straightforward\n",
    "    \n",
    "In LDA and QDA we make strong assumptions regarding the form of the density functions. This allows us to take a hard estimation problem, and turn it into a much easier estimation problem. However, these are strong assumptions that could be way off.\n",
    "\n",
    "### Assuming independence\n",
    "\n",
    "The na&#xEF;ve Bayes classifier takes a different approach. Instead of assuming a set form for the density we instead assume that within a given class, $c$, each of $m$ features are independent. That allows us to write:\n",
    "\n",
    "$$\n",
    "f_c(X) = f_{c_1}(X_1) \\times f_{c_2}(X_2) \\times \\dots \\times f_{c_m}(X_m),\n",
    "$$\n",
    "\n",
    "where $f_{c_j}(X_j)$ denotes the probability density function for $X_j$ among observations of the $c^\\text{th}$ class.\n",
    "\n",
    "Under this assumption we have:\n",
    "\n",
    "$$\n",
    "P(y=c|X=X^*) = \\frac{\\pi_c f_{c_1}(X_1^*) \\times f_{c_2}(X_2^*) \\times \\dots \\times f_{c_m}(X_m^*)}{\\sum_{l=1}^\\mathcal{C} \\pi_l f_{l_1}(X_1^*) \\times f_{l_2}(X_2^*) \\times \\dots \\times f_{l_m}(X_m^*)}.\n",
    "$$\n",
    "\n",
    "Assuming independence between the feature variables allows us to turn our difficult problem of estimating an $m$-dimensional probability distribution (which involves estimating both $m$ marginal distributions and a joint distribution) into a problem where we just have to estimate $m$ independent probability distributions, which is much more tractable.\n",
    "\n",
    "### Estimating the $f_{c_j}$\n",
    "\n",
    "When it comes to estimating the $f_{c_j}$ we typically assume some kind of distribution and then estimate the parameters for that distribution accordingly.\n",
    "\n",
    "For example:\n",
    "- if $X_j$ is quantitative we typically assume it is a normal distribution\n",
    "    - note this is different than LDA or QDA because those do not assume independence, hence the covariance matrices $\\Sigma$ or $\\Sigma_c$.\n",
    "- if $X_j$ is categorical we could just use a Bernouli distribution (think biased coin toss) estimating the value of $p$ using the proportion of observations where $y=c$ for each possible value of $X_j$.\n",
    "\n",
    "### Is independence a good assumption?\n",
    "\n",
    "Assuming independence is not always a <i>good</i> assumption in the sense that the some of the features may in fact be related to one another. However, even if this assumption does not hold, we can still get decent to good classifiers using na&#xEF;ve Bayes. This can be particularly true if we do not have enough data to reasonably estimate a joint probability distribution. We can think of this assumption as adding bias to our model.\n",
    "\n",
    "Let's show how to implement this algorithm in `sklearn` with the `iris` data set.\n",
    "\n",
    "Na&#xEF;ve Bayes is implemented in `sklearn` with a few different methods, all found in the `naive_bayes` module, <a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes\">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes</a>.\n",
    "\n",
    "Because the two features we will use are quantitative we will use the `GaussianNB` model, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\">https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcbb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f53537",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the model\n",
    "\n",
    "\n",
    "## Fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da1f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making a grid\n",
    "p_width_min, p_width_max = X_train.petal_width.min()-.1, X_train.petal_width.max()+.1\n",
    "p_length_min, p_length_max = X_train.petal_length.min()-.1, X_train.petal_length.max()+.1\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.arange(p_width_min, p_width_max, .01),\n",
    "                          np.arange(p_length_min, p_length_max, .01))\n",
    "\n",
    "X_pred = np.zeros((len(xx1.reshape(-1,1)), 2))\n",
    "X_pred[:,0] = xx1.flatten()\n",
    "X_pred[:,1] = xx2.flatten()\n",
    "\n",
    "LDA_preds = LDA.predict(X_pred)\n",
    "QDA_preds = QDA.predict(X_pred)\n",
    "NB_preds = nb.predict(X_pred)\n",
    "\n",
    "\n",
    "\n",
    "## plotting the decision boundary with the training points\n",
    "\n",
    "## make the subplots\n",
    "fig,ax = plt.subplots(1,3,figsize=(30,10))\n",
    "\n",
    "## Plot the LDA first\n",
    "### Decision Boundaries\n",
    "ax[0].scatter(X_pred[LDA_preds==0,0],\n",
    "            X_pred[LDA_preds==0,1],\n",
    "            alpha=.01)\n",
    "ax[0].scatter(X_pred[LDA_preds==1,0],\n",
    "            X_pred[LDA_preds==1,1],\n",
    "            alpha=.01)\n",
    "ax[0].scatter(X_pred[LDA_preds==2,0],\n",
    "            X_pred[LDA_preds==2,1],\n",
    "            c = 'lightgreen',\n",
    "            alpha=.01)\n",
    "\n",
    "### Training Data\n",
    "ax[0].scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                label='Training 0',\n",
    "                c = 'blue',\n",
    "                edgecolor='black',\n",
    "                s=200)\n",
    "ax[0].scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                label='Training 1',\n",
    "                c = 'darkorange',\n",
    "                marker = 'v',\n",
    "                edgecolor='black',\n",
    "                s=200)\n",
    "ax[0].scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                label='Training 2',\n",
    "                c = 'darkgreen',\n",
    "                marker = 'x',\n",
    "                s=200)\n",
    "ax[0].set_title(\"LDA Decision Boundary\", fontsize=26)\n",
    "ax[0].set_xlabel(\"Petal Width (cm)\", fontsize=20)\n",
    "ax[0].set_ylabel(\"Petal Length (cm)\", fontsize=20)\n",
    "\n",
    "## Plot the QDA next\n",
    "### Decision boundaries\n",
    "ax[1].scatter(X_pred[QDA_preds==0,0],\n",
    "            X_pred[QDA_preds==0,1],\n",
    "            alpha=.01)\n",
    "ax[1].scatter(X_pred[QDA_preds==1,0],\n",
    "            X_pred[QDA_preds==1,1],\n",
    "            alpha=.01)\n",
    "ax[1].scatter(X_pred[QDA_preds==2,0],\n",
    "            X_pred[QDA_preds==2,1],\n",
    "            c = 'lightgreen',\n",
    "            alpha=.01)\n",
    "\n",
    "### The training data\n",
    "ax[1].scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                label='Training 0',\n",
    "                c = 'blue',\n",
    "                edgecolor='black',\n",
    "                s=200)\n",
    "ax[1].scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                label='Training 1',\n",
    "                c = 'darkorange',\n",
    "                marker = 'v',\n",
    "                edgecolor='black',\n",
    "                s=200)\n",
    "ax[1].scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                label='Training 2',\n",
    "                c = 'darkgreen',\n",
    "                marker = 'x',\n",
    "                s=200)\n",
    "\n",
    "ax[1].set_title(\"QDA Decision Boundary\", fontsize=26)\n",
    "ax[1].set_xlabel(\"Petal Width (cm)\", fontsize=20)\n",
    "ax[1].set_ylabel(\"Petal Length (cm)\", fontsize=20)\n",
    "\n",
    "\n",
    "## Plot the naive bayes finally\n",
    "### Decision Boundaries\n",
    "ax[2].scatter(X_pred[NB_preds==0,0],\n",
    "            X_pred[NB_preds==0,1],\n",
    "            alpha=.01)\n",
    "ax[2].scatter(X_pred[NB_preds==1,0],\n",
    "            X_pred[NB_preds==1,1],\n",
    "            alpha=.01)\n",
    "ax[2].scatter(X_pred[NB_preds==2,0],\n",
    "            X_pred[NB_preds==2,1],\n",
    "            c = 'lightgreen',\n",
    "            alpha=.01)\n",
    "\n",
    "### Training data\n",
    "ax[2].scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                label='Training 0',\n",
    "                c = 'blue',\n",
    "                edgecolor='black',\n",
    "                s=200)\n",
    "ax[2].scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                label='Training 1',\n",
    "                c = 'darkorange',\n",
    "                marker = 'v',\n",
    "                edgecolor='black',\n",
    "                s=200)\n",
    "ax[2].scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                label='Training 2',\n",
    "                c = 'darkgreen',\n",
    "                marker = 'x',\n",
    "                s=200)\n",
    "\n",
    "ax[2].set_title(\"NB Decision Boundary\", fontsize=26)\n",
    "ax[2].set_xlabel(\"Petal Width (cm)\", fontsize=20)\n",
    "ax[2].set_ylabel(\"Petal Length (cm)\", fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax[0].legend(fontsize=20, loc=2)\n",
    "ax[1].legend(fontsize=20, loc=2)\n",
    "ax[2].legend(fontsize=20, loc=2)\n",
    "\n",
    "plt.subplots_adjust(wspace=.1)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51424673",
   "metadata": {},
   "source": [
    "As we can see the na&#xEF;ve Bayes classifier still allows for non-linear boundaries, and by introducing some bias we may even produce a model that generalized better than say the QDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c6d7d",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the ErdÅ‘s Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec4a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
