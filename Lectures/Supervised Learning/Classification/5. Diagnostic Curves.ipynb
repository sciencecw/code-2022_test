{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4dce531",
   "metadata": {},
   "source": [
    "# Diagnostic Curves\n",
    "\n",
    "Predicting probabilities instead of just classes allows us to assess model performance using a variety of diagnostic curves.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss how different probability cutoffs produce different classification metrics,\n",
    "- Parlay that into definitions of various diagnostic curves like:\n",
    "    - The precision-recall curve,\n",
    "    - The receiver operating characteristic (ROC) Curve\n",
    "    - A gains chart and\n",
    "    - A lift chart,\n",
    "- Demonstrate how to plot the various curves using `sklearn` and `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c58495",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7f8a6",
   "metadata": {},
   "source": [
    "In this notebook we will work with the synthetic logistic regression problem we saw in the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601356ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the randomly generated data\n",
    "data = np.loadtxt(\"../../../Data/random_binary.csv\",delimiter = \",\")\n",
    "X = data[:,0]\n",
    "y = data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                       shuffle=True,\n",
    "                                                       random_state=435,\n",
    "                                                       test_size=.2,\n",
    "                                                       stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e0610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make model object\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "## fit the model\n",
    "log_reg.fit(X_train.reshape(-1,1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figure \n",
    "plt.figure(figsize = (10,8))\n",
    "\n",
    "# With classifications we have a new method\n",
    "# predict_proba which returns the probability\n",
    "# that an observation is a certain class.\n",
    "plt.plot(np.linspace(0,1,1000),\n",
    "            log_reg.predict_proba(np.linspace(0,1,1000).reshape(-1,1))[:,1],\n",
    "            'r--',linewidth=2.5,label = \"Model Fit\")\n",
    "plt.scatter(X_train,y_train,label = 'Training Data',alpha=.7)\n",
    "plt.legend(fontsize = 14,loc = 4)\n",
    "plt.xlabel(\"Feature\",fontsize = 16)\n",
    "plt.ylabel(\"p(X)\",fontsize=16) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289c7a47",
   "metadata": {},
   "source": [
    "Choosing different probability cutoffs gives different confusion matrices and thus different performance metrics. Here's an example with a cutoff of $0.4$ and $0.6$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc9564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b98158",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix for a cutoff of 0.4\")\n",
    "print(confusion_matrix(y_train, \n",
    "                       np.int64(log_reg.predict_proba(X_train.reshape(-1,1))[:,1] >= 0.4)))\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Confusion Matrix for a cutoff of 0.6\")\n",
    "print(confusion_matrix(y_train, \n",
    "                       np.int64(log_reg.predict_proba(X_train.reshape(-1,1))[:,1] >= 0.6)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ff666",
   "metadata": {},
   "source": [
    "As we may have suspected, increasing our cutoff increased our number of true and false negatives while decreasing our number of false and true positives. \n",
    "\n",
    "We could do this process for any cutoff value in $[0,1]$ which implies that all of our previously introduced performance metrics can be thought of as functions of the probability cutoff. This allows us to make series of diagnostic curves for any algorithm that we are considering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9ab32",
   "metadata": {},
   "source": [
    "## Precision-Recall curve\n",
    "\n",
    "The first curve we will consider is the <i>precision-recall</i> curve. As the name suggests this curve plots the different precision scores against the different recall scores your algorithm receives as you alter the probability cutoff. \n",
    "\n",
    "If we remember, precision estimates the probability that an observation is actually of class $1$ when your algorithm says it is of class $1$, i.e. $P(y=1 | \\hat{y} = 1)$. Recall estimates the probability that your algorithm classifies something as $1$ when it is actually a $1$, or $P(\\hat{y}=1|y=1)$. A nice way to think about this is an analogy about web search results. When you perform a search for a specific topic precision measures what fraction of your results are relevant to your topic, while recall measure what fraction of the total possible relevant results were returned.\n",
    "\n",
    "### Generating in python\n",
    "\n",
    "We can generate a precision-recall curve in python using `precision_score` and `recall_score` in a `for` loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81911d7",
   "metadata": {},
   "source": [
    "##### Exercise try to fill in the missing code in the next two chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267bb60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d2c9d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cutoffs = np.arange(0.001,.975,.001)\n",
    "\n",
    "prec_scores = []\n",
    "rec_scores = []\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    pred = \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4bbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(rec_scores, prec_scores)\n",
    "\n",
    "plt.xlabel(\"Recall\", fontsize=16)\n",
    "plt.ylabel(\"Precision\", fontsize=16)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf08156",
   "metadata": {},
   "source": [
    "We can note that an algorithm that perfectly predicts the probability of the observations would produce a precision-recall curve that traces the upper right-hand corner of the unit square. This is not likely to happen in practice.\n",
    "\n",
    "### Trade-off between precision and recall\n",
    "\n",
    "While precision-recall curves may not look this nice in practice, the general trend that precision tends to decrease as recall increases is a mainstay in such plots. Returning to our web seach example, when the web search is less stringent on what results it considers relevant (i.e. a low probability cutoff) we will capture a greater portion of all of the relevant documents, but the fraction of our results that are actually relevant to us is lower (high recall with low precision). Conversely, when we make our web search more stringent (higher probability cutoff) we can be sure that the returned results are more likely to be relevant to our query, but we will have captured fewer of the overall relevant results (low recall with high precision).\n",
    "\n",
    "\n",
    "We can also see the reason for the trade-off by reminding ourselves of the  precision and recall formulae:\n",
    "\n",
    "$$\n",
    "\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} \\text{ and}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}.\n",
    "$$\n",
    "\n",
    "At a cutoff of $0$ every instance is classified as a $1$ and so $\\text{recall} = 1$ and precision is equal to whatever percentage of the observations are of class $1$. Note that the denominator of recall is not impacted by the cutoff. Since the number of true positives decreases as the cutoff increases then the recall must also decrease (to $0$). Although the denominator of precision does depend upon the cutoff, the number of false positives will go to $0$ as we increase the cutoff and so the precision will tend to approach $1$ as we increase the cutoff.\n",
    "\n",
    "With that in mind the precision-recall curve provides us a way to examine this trade-off and choose a cutoff that is appropriate for the needs of our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090e682",
   "metadata": {},
   "source": [
    "## The receiver operating characteristic (ROC) curve\n",
    "\n",
    "The <i>receiver operating characteristic</i> (ROC) curve arose in World War II as a way to aid operators of radar receivers for detecting enemy objects in battlefields. It plots the true positive rates (TPRs) against the false positive rates (FPRs) for various cutoff values. The curve for a specific algorithm is often compared to the theoretical ROC curve for a random guess algorithm which is the line $y=x$.\n",
    "\n",
    "As a reminder the TPR is the same as the recall, so it measures the probability that an observation is classified as a $1$ when if it is actually a $1$, i.e. $P(\\hat{y} = 1 | y =1)$. The FPR estimates the probability that an observation is classified as $1$ when it is actually a $0$, i.e. $P(\\hat{y} = 1 | y = 0)$.\n",
    "\n",
    "A nice analogy for these metrics comes from oncology. Sometimes people will have surgery to remove a tumor (a collection of cancerous cells). The goal of such surgeries is to maximize the amount of cancerous cells and minimize the amount of normal cells that are removed. If we treat cancerous cells as \"class 1\" observations and normal cells as \"class 0\"  then we can think of this as trying to maximimize TPR while minimizing FPR.\n",
    "\n",
    "Below we demonstrate two methods of constructing this curve, one with a `for` loop and one using an `sklearn` function.\n",
    "\n",
    "#### `for` loop\n",
    "\n",
    "##### Exercise: Try to fill in the missing code in the next code chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for loop approach\n",
    "cutoffs = np.arange(0,1,.001)\n",
    "\n",
    "tprs = []\n",
    "fprs = []\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    pred = 1*(log_reg.predict_proba(X_train.reshape(-1,1))[:,1] >= cutoff)\n",
    "    \n",
    "    conf_mat = \n",
    "    \n",
    "    tn = \n",
    "    fp = \n",
    "    fn = \n",
    "    tp = \n",
    "    \n",
    "    tprs.append()\n",
    "    fprs.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2351d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(fprs, tprs, label=\"ROC Curve\")\n",
    "plt.plot([0,1], [0,1], 'k--', label=\"Random Guess Curve\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=16)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=16)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c23cc",
   "metadata": {},
   "source": [
    "#### With `sklearn`\n",
    "\n",
    "`sklearn` has an `roc_curve` metric function that will generate curve points for you, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html</a>. However, with the function you have less control over the chosen cutoff values.\n",
    "\n",
    "This returns three arrays, the false positive rates, the true positive rates and the cutoffs that are associated with those scores. Note that the cutoffs have an additional value in the `0` position of the array which is $1$ plus the largest cutoff value used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924841c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6560e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First input the true values\n",
    "## Then input the predicted probability for class 1\n",
    "## the fprs, tprs and cutoffs are returned in that order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90dd8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(fprs, tprs, label=\"ROC Curve\")\n",
    "plt.plot([0,1], [0,1], 'k--', label=\"Random Guess Curve\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=16)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f94e6",
   "metadata": {},
   "source": [
    "A theoretical perfect classifier would have an ROC curve that hugs the upper left-hand corner of the unit square.\n",
    "\n",
    "Remember that the TPR is the same as the sensitivity of the algorithm and the FPR is one minus the specifity. Thus the ROC curve is equivalent to plotting the sensitivity against the specifity.\n",
    "\n",
    "In general as we increase the TPR we also increase the FPR. Returning to our cancer surgery example, if we get too ambitious targeting cancer cells, then we are more likely to accidentally remove a normal cell. Conversely, if we are too cautious we will miss too much of the tumor and the surgery will have been a failure. The ROC curve allows us to find the perfect balance for our particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781fb8ef",
   "metadata": {},
   "source": [
    "## Gains and lift charts\n",
    "\n",
    "The final plots we will introduce are called <i>gain</i> and <i>lift</i> charts.\n",
    "\n",
    "The point of these curves is to give an estimate of the true positive rate of your algorithm if you classify the $v^\\text{th}$ upper percentile of predicted probabilities and classify them as $1$.\n",
    "\n",
    "A nice motivation for such a plot comes from marketing. Suppose that you only have the funds in your advertising budget to market to $v \\%$ of your potential customers. You would want to allocate those ads in such a way that you maximize the number of people who see your ads and then become customers. In this setting we can take class $1$ to represent a person who would become a customer after seeing an ad and class $0$ as someone who would not become a customer after seeing an ad. By only marketing to those customers that fall in the top $v^\\text{th}$ percentile of predicted probabilities, we will only market to those people our algorithm thinks are most likely to become a customer after seeing an ad.\n",
    "\n",
    "### Gains chart\n",
    "\n",
    "The gains chart plots the TPR as a function of the percent of observations we have classified as $1$. Similar to the ROC curve, it is typical to plot a \"baseline\" curve for comparison. Again the baseline is taken to be the line $y=x$, which represents us randomly selecting $v\\%$ of the observations to be of class $1$.\n",
    "\n",
    "### Lift chart\n",
    "\n",
    "The lift chart is the ratio of the gains curve for the algorithm to the baseline curve. It is called a lift chart because this ratio is supposed to represent the \"lift\" the algorithm gives you over random guessing.\n",
    "\n",
    "### Plotting in python\n",
    "\n",
    "We can plot this in python using `pandas`'s `quantile` function, <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.quantile.html\">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.quantile.html</a>.\n",
    "\n",
    "<i>This can also be accomplished with `numpy`'s `quantile` function, <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.quantile.html\">https://numpy.org/doc/stable/reference/generated/numpy.quantile.html</a></i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb017129",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'actual':y_train,\n",
    "                           'prob':log_reg.predict_proba(X_train.reshape(-1,1))[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the p cutoffs using .quantile\n",
    "## put in a number from 0 to 1 and recieve the \n",
    "## quantile for that number\n",
    "ps = [pred_df.prob.quantile(i) for i in np.arange(1, 0, -0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the first five upper probability quantiles\n",
    "ps[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d794233",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we get the tprs for the corresponding quantiles\n",
    "tprs = []\n",
    "lifts = []\n",
    "\n",
    "\n",
    "for i,p in enumerate(ps):\n",
    "    pred = 1*(pred_df.prob.values >= p)\n",
    "    \n",
    "    ## using the fact the tpr is recall\n",
    "    tprs.append(recall_score(pred_df.actual.values, pred))\n",
    "    \n",
    "## as well as the lift_scores\n",
    "lifts = [tprs[i]/np.arange(.01,1.01,.01)[i] for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700962af",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(20,8))\n",
    "\n",
    "ax[0].plot(np.arange(0.01,1.01,0.01), tprs, label=\"Logistic Regression\")\n",
    "ax[0].plot([0,1], [0,1], 'k--', label=\"Baseline\")\n",
    "\n",
    "ax[0].set_xlabel(\"Percent of Observations\", fontsize=16)\n",
    "ax[0].set_ylabel(\"True Positive Rate\", fontsize=16)\n",
    "ax[0].set_xticks([0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1])\n",
    "ax[0].set_xticklabels([0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1], fontsize=14)\n",
    "ax[0].set_yticks([0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1])\n",
    "ax[0].set_yticklabels([0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1], fontsize=14)\n",
    "ax[0].set_title(\"Gains Plot\", fontsize=18)\n",
    "ax[0].legend(fontsize=14)\n",
    "\n",
    "ax[1].plot(np.arange(0.01,1.01,0.01), lifts)\n",
    "ax[1].plot([0,1],[1,1],'k--')\n",
    "ax[1].set_ylim((0,2.25))\n",
    "\n",
    "ax[1].set_xlabel(\"Percent of Observations\", fontsize=16)\n",
    "ax[1].set_ylabel(\"Lift\", fontsize=16)\n",
    "ax[1].set_xticks([0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1])\n",
    "ax[1].set_xticklabels([0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1], fontsize=14)\n",
    "ax[1].set_yticks([0,.25,.5,.75,1,1.25,1.5,1.75,2,2.25], fontsize=14)\n",
    "ax[1].set_yticklabels([0,.25,.5,.75,1,1.25,1.5,1.75,2,2.25], fontsize=14)\n",
    "ax[1].set_title(\"Lift Plot\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71b0c24",
   "metadata": {},
   "source": [
    "If our model is any good it should generally lie above the Baseline curve. In particular the \"best\" model (i.e. the one that ranks all actual $1$s higher than all actual $0$s) should be the line from $(0,0)$ to $(r,1)$ where $r$ is the proportion of observations that are actually of class $1$.\n",
    "\n",
    "The gains plot can be interpreted by drawing a vertical line at a given percent of observations. For example, by selecting the $20\\%$ of observations most likely to be $1$, according to the model, we capture $40\\%$ of the actual $1$s in the sample. \n",
    "\n",
    "Returning to our marketing motivation, this can be useful for setting expectations for what is obtainable with stakeholders. For example, if you are told that there is a budget for advertising to $20\\%$ of all possible customers, you can reply that with this method of selecting ad targets we think we can get $40\\%$ of the likely customers.\n",
    "\n",
    "We should note that these plots say nothing about the false positive rate. In marketing you may not care too much about accidentally sending a few ads to people that will not become customers, but this is not the case in other applications. You may think to supplement the gains plot with a similar plot tracking false positive rate as a function of the percent of observations classified as $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c896f",
   "metadata": {},
   "source": [
    "## Which curves or metrics?\n",
    "\n",
    "When combined with our various individual metrics, a natural question is what curve or metric should you use for evaluating a model or method? There is no one-size-fits-all approach. What matters for one problem may not be important for another.\n",
    "\n",
    "It is important to think about what is most important for your particular problem and select the metric(s) and curve(s) accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d5090",
   "metadata": {},
   "source": [
    "## A note on training sets\n",
    "\n",
    "In this notebook we presented these curves using our training set. This was done for coding convenience. \n",
    "\n",
    "We should remember that in practice we are not usually interested performance metrics on a single training set. We are most interested in trying to estimate what the performance of the model will be on unknown data under the assumption that the unknown data follows the same distribution as our sample. Just like we can use a validation set or cross-validation to provide estimates of a single metric we can do a similar process for these diagnostic curves. That is, we can plot the curves using a validation set or plot a series of curves that result from cross-validation and examine their distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2035b",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203f660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9becf6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
