{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980f8d0b",
   "metadata": {},
   "source": [
    "# The Confusion Matrix\n",
    "\n",
    "In our $k$NN notebook we introduced accuracy, but accuracy is not always the best metric. Let's introduce some additional metrics for classification problems now.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Mention some deficiencies with accuracy as a metric,\n",
    "- Introduce the confusion matrix,\n",
    "- Derive some new performance metrics and discuss when they are appropriate,\n",
    "- Define:\n",
    "    - Precision,\n",
    "    - Recall,\n",
    "    - Specificity,\n",
    "    - Sensitivity and\n",
    "    - Various other rate based metrics and\n",
    "- Provide a useful summary table that you can use as a \"cheat sheet\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9692a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the iris data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# for data handling \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"dark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c047f0",
   "metadata": {},
   "source": [
    "## Problems with accuracy\n",
    "\n",
    "In the $k$-nearest neighbors notebook we defined the accuracy metric as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy } = \\ \\frac{\\text{The number of correct predictions}}{\\text{Total number of predictions made}}.\n",
    "$$\n",
    "\n",
    "This can be a misleading metric because it obfuscates which kinds of observations the model got correct. For example, if we had a problem where the distribution of classes was: $10\\% - 1$ and $90\\% - 0$ then a model that predicts every observation to be $0$ would have $90\\%$ accuracy. While we would generally assume that $90\\%$ indicates good performance, in this situation we have failed to identify any observation that was of class $1$. This would be terrible if, for instance, class $1$ represented the diagnosis of a treatable or curable disease.\n",
    "\n",
    "We will thus look to develop additional performance metrics for classification problems that will allow us to think about how our models are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af891ab",
   "metadata": {},
   "source": [
    "## The confusion matrix\n",
    "\n",
    "Additional performance measures can be derived from the confusion matrix, pictured for binary problems below.\n",
    "\n",
    "<img src=\"conf_mat.png\" alt=\"Confusion Matrix Image\" width=\"50%;\">\n",
    "\n",
    "Contained within each box of the confusion matrix are counts of how the algorithm sorted. For instance, in the TP box would be the total number of correct positive (correctly classified as $1$) classifications the algorithm made. The diagonal thus represents data points that are correctly predicted by the algorithm and the off-diagonal represents points that are incorrectly predicted by the algorithm.  \n",
    "\n",
    "For those of you more familiar with frequentist statistics we can think of the false negatives as the classifier making a type II error and the false positives as the classifier making a type I error.\n",
    "\n",
    "A confusion matrix is referred to as a <i>contingency table</i> in some fields.\n",
    "\n",
    "<i>Note that you can extend the confusion matrix to a multiclass problem by just adding rows and columns accordingly. However, we will lose the true positive, true negative nomenclature. We will see this extension in a later notebook.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48170850",
   "metadata": {},
   "source": [
    "### Metrics derived from the confusion matrix\n",
    "\n",
    "It can be difficult to convey classifier performance by just looking at the confusion matrix. Moreover, in your particular problem you may only be interested in a certain kind of performance. As an example, consider the case where you work for a company that builds software to flag hate speech in forum posts. In this situation your primary concern is to accurately flag hate speech when it is posted, while limiting incorrect hate speech flags may be a secondary concern.\n",
    "\n",
    "There has thus been extensive development of metrics derived from the confusion matrix that assess different types of classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba227ae",
   "metadata": {},
   "source": [
    "#### Precision and recall\n",
    "\n",
    "Two popular measures derived from the confusion matrix are the algorithm's <i>precision</i> and <i>recall</i>:\n",
    "\n",
    "$$\n",
    "\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}, \\text{ out of all points predicted to be class } 1, \\text{ what fraction were actually class } 1?\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}, \\text{ out of all the actual data points in class } 1 \\text{, what fraction did the algorithm correctly predict?}\n",
    "$$\n",
    "\n",
    "You can think of precision as how much you should trust the algorithm when it says something is class $1$. \n",
    "\n",
    "Recall estimates the probability that the algorithm correctly detects class $1$ data points.\n",
    "\n",
    "\n",
    "Let's examine the training precision and recall for a virginica classifier using the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76375744",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris['data'],columns = ['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "\n",
    "## Create a virginica variable\n",
    "## this will be our target\n",
    "iris_df['virginica'] = 0 \n",
    "iris_df.loc[iris['target'] == 2,'virginica'] = 1\n",
    "\n",
    "X = iris_df[['sepal_length','sepal_width','petal_length','petal_width']].to_numpy()\n",
    "y = iris_df['virginica'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f6dd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  virginica\n",
       "145           6.7          3.0           5.2          2.3          1\n",
       "146           6.3          2.5           5.0          1.9          1\n",
       "147           6.5          3.0           5.2          2.0          1\n",
       "148           6.2          3.4           5.4          2.3          1\n",
       "149           5.9          3.0           5.1          1.8          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9540a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78d4b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=111,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79148eab",
   "metadata": {},
   "source": [
    "Now we will build a $k$-nearest neighbor classifier using $k=5$. We'll then examine the confusion matrix on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a28ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "559bd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the model object\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "## Fit the model object\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "## get the predictions\n",
    "y_train_pred = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1636da02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0758d",
   "metadata": {},
   "source": [
    "`sklearn` provides a quick way to get the confusion matrix for a classifier, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1763c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we can import the confusion matrix\n",
    "## function from sklearn\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff604515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[77,  3],\n",
       "       [ 2, 38]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## just like mse, actual then prediction\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40554889",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the confusion matrix here\n",
    "\n",
    "TN = confusion_matrix(y_train, y_train_pred)[0,0]\n",
    "FP = confusion_matrix(y_train, y_train_pred)[0,1]\n",
    "FN = confusion_matrix(y_train, y_train_pred)[1,0]\n",
    "TP = confusion_matrix(y_train, y_train_pred)[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6082cf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training recall is 0.95\n",
      "The training precision is 0.9268\n"
     ]
    }
   ],
   "source": [
    "## calculate recall and precision here\n",
    "print(\"The training recall is\", \n",
    "         np.round(TP/(FN + TP), 4))\n",
    "\n",
    "print(\"The training precision is\", \n",
    "         np.round(TP/(FP + TP), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980c6b0",
   "metadata": {},
   "source": [
    "Alternatively we could use `sklearn`'s precision and recall metrics.\n",
    "\n",
    "- `precision_score` docs, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html</a>\n",
    "- `recall_score` docs, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06a153be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training recall is 0.95\n",
      "The training precision is 0.92683\n"
     ]
    }
   ],
   "source": [
    "## import precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "## print the precision and recall here\n",
    "print(\"The training recall is\",\n",
    "         np.round(recall_score(y_train, y_train_pred), 5))\n",
    "\n",
    "print(\"The training precision is\",\n",
    "         np.round(precision_score(y_train, y_train_pred), 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d0910",
   "metadata": {},
   "source": [
    "#### True positive rate, false positive rate, true negative rate and false negative rate\n",
    "\n",
    "We may also be interested in easier to remember metrics. For example things like:\n",
    "- Given that an observation is a true positive:\n",
    "    - what is the probability that we correctly predict it is a positive? This is estimated with the <i>true positive rate</i>. (Note that this is the same as recall).\n",
    "    - what is the probability that we incorrectly predict it is a negative? This is estimated with the <i>false negative rate</i>.\n",
    "- Given that an observation is a true negative:\n",
    "    - what is the probability that we correctly predict it is a negative? This is estimated with the <i>true negative rate</i>.\n",
    "    - what is the probability that we incorrectly predict it is a positive? This is estimated with the <i>false positive rate</i>.\n",
    "    \n",
    "Depending on the application we may be incredibly interested in optimizing one or more of these measures. For example, if we were trying to predict that someone has a serious infectious disease we may be most interested in the false negative rate.\n",
    "\n",
    "The formulae for these are given by:\n",
    "\n",
    "$$\n",
    "\\text{true positive rate } = \\text{ TPR } = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{false negative rate } = \\text{ FNR } = \\frac{\\text{FN}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{true negative rate } = \\text{ TNR } = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{false positive rate } = \\text{ FPR } = \\frac{\\text{FP}}{\\text{TN} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "These we would calculate by hand using `confusion_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57dbaf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training true positive rate is 0.95\n",
      "The training false negative rate is 0.05\n",
      "The training true negative rate is 0.9625\n",
      "The training false positive rate is 0.0375\n"
     ]
    }
   ],
   "source": [
    "## Calculate All four for our virginica classifier\n",
    "\n",
    "## TPR\n",
    "print(\"The training true positive rate is\",\n",
    "         np.round(TP/(TP+FN),4))\n",
    "\n",
    "\n",
    "## FNR\n",
    "print(\"The training false negative rate is\",\n",
    "         np.round(FN/(TP+FN),4))\n",
    "\n",
    "\n",
    "\n",
    "## TNR\n",
    "print(\"The training true negative rate is\",\n",
    "         np.round(TN/(TN+FP),4))\n",
    "\n",
    "\n",
    "\n",
    "## FPR\n",
    "print(\"The training false positive rate is\",\n",
    "         np.round(FP/(FP+TN),4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9f929",
   "metadata": {},
   "source": [
    "#### Sensitivity and specificity\n",
    "\n",
    "These two have a long history of use in public health when assessing the performance of screening and diagnostic tests.\n",
    "\n",
    "- The <i>sensitivity</i> of a classifier is the probability that it correctly identifies a positive observation (note that this is the same as the true positive rate and recall) and\n",
    "- The <i>specificity</i> of a classifier is the probability that it correctly identifies a negative observation (note that this is the same as the true negative rate).\n",
    "\n",
    "The formulae for both are given:\n",
    "\n",
    "$$\n",
    "\\text{Sensitivity } = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}, \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Specificity } = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}.\n",
    "$$\n",
    "\n",
    "While these two are identical to metrics given above, these are common enough names that it is important for you to be formally introduced to them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646cbfe",
   "metadata": {},
   "source": [
    "## Too much to remember\n",
    "\n",
    "That is a lot of metrics to remember. It is okay if you cannot perfectly remember what name goes with what formula (I still have to look up precision and recall). To help you out we have provided a \"cheat sheet\" with a table of metrics derived from the confusion matrix. You can find it here <a href=\"confusion_matrix_cheat_sheet.pdf\">confusion_matrix_cheat_sheet.pdf</a>. You can also find metrics that we did not cover in this notebook here, <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">https://en.wikipedia.org/wiki/Confusion_matrix</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff0c99",
   "metadata": {},
   "source": [
    "## Careful consideration\n",
    "\n",
    "In real world settings it is important to give careful consideration to which performance metric(s) are optimized in model selection. When selecting try to translate what the metric translates into when considering the real world problem you are considering.\n",
    "\n",
    "For example, public health often focus on sensitivity and specificity because they can be translated into real world health impacts.\n",
    "\n",
    "In the case of deadly disease for which we have successful regimens we may choose to go for tests that have high sensitivity. While we may opt for high specificity if the disease or condition in question does not tend to cause severe outcomes in the individual and the test or treatment is highly invasive or expensive.\n",
    "\n",
    "A careful consideration of metrics can also contribute to your understanding of what the classifier is capable of, which should help you frame your findings in terms that stakeholders can better understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0b6df",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a91fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
