{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b21a86ad",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Just when we thought we were done with regression, it pulls us back in!\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Learn the logistic regression algorithm,\n",
    "- Show how you can interpret logistic regression output,\n",
    "- Talk about classification cutoffs and\n",
    "- Think about predicting probabilities instead of hard classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b0def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b010a",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "\n",
    "We will be using logistic regression for binary classification, classification problems with only two classes typically coded as $0$ or $1$. Normally the class denoted as $1$ is something we want to identify, for example someone that has a disease or someone that qualifies for a loan. \n",
    "\n",
    "<i>Note that logistic regression can be adapted to multi-class classification, but we will focus on that in another notebook.</i>\n",
    "\n",
    "### From binary to continuous\n",
    "\n",
    "<i>Logistic regression is a form of statistical regression algorithm.</i> It turns out that this may be somewhat of a controversial statement... <a href=\"https://twitter.com/TenanATC/status/1386332061087277057?s=20\">https://twitter.com/TenanATC/status/1386332061087277057?s=20</a>, but it is important to remember that it is in fact a statistical regression technique used to solve classification problems in supervised learning.\n",
    "\n",
    "Remember that regression algorithms are usually used to predict continuous outcomes, but binary classification is in no way continuous. This is where logisitic regression is a little clever, instead of modeling the output class, it models the probability that a particular data point is an instance of class $1$.\n",
    "\n",
    "Let's dive in with some random data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d546cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the randomly generated data\n",
    "data = np.loadtxt(\"../../../Data/random_binary.csv\",delimiter = \",\")\n",
    "X = data[:,0]\n",
    "y = data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f31931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf84e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                       shuffle=True,\n",
    "                                                       random_state=435,\n",
    "                                                       test_size=.2,\n",
    "                                                       stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fcaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "\n",
    "plt.scatter(X_train,y_train)\n",
    "plt.ylim((-.1,1.1))\n",
    "plt.xlabel(\"Feature\",fontsize = 16)\n",
    "plt.ylabel(\"Class\",fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d70cc2",
   "metadata": {},
   "source": [
    "While the vertical axis of the above plot says \"Class\" we could just as easily label it the \"Probability the instance is $1$\". In this case since we know the class of each data point in the training set, the probability can only be $0$ or $1$. Now suppose you have a new data point for which you only have the vector of predictors, $X$. We are interested in the probability that this data point has class $y=1$, call this probability $P(y=1|X) = p(X)$. $p(X)$ can take on all values in $[0,1]$. Attempting to estimate $p(X)$ takes us back to the realm of regression problems.\n",
    "\n",
    "The way we model the probability in logistic regression is with a sigmoidal curve, the general form looks like this:\n",
    "$$\n",
    "f(x) = \\frac{1}{1+e^{-x}}.\n",
    "$$\n",
    "A graph of the curve this function produces is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "\n",
    "plt.plot(np.arange(-x,x,.01),1/(1+np.exp(-np.arange(-x,x,.01))))\n",
    "\n",
    "\n",
    "plt.xlabel(\"x\",fontsize = 16)\n",
    "plt.ylabel(\"f(x)\",fontsize = 16)\n",
    "\n",
    "plt.title(\"A Sigmoidal Curve\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c46a16",
   "metadata": {},
   "source": [
    "Notice that this function stays between $0$ and $1$. Also like our phony data it transitions from class $0$, to class $1$ in a continuous manner. This is the function type we would like to use as our model.\n",
    "\n",
    "The model that is used in logistic regression is:\n",
    "$$\n",
    "p(X) = \\frac{1}{1 + e^{-X\\beta}},\n",
    "$$\n",
    "where $\\beta = \\left(\\beta_0,\\beta_1,\\dots,\\beta_m\\right)^T$ is a column vector of coefficients, $X$ has been extended to include a column of ones.\n",
    "\n",
    "The model is fit using the statistical method of maximum likelihood estimators (for a derivation of the loss function see the practice problems on logistic regression).\n",
    "\n",
    "Let's see how to use `sklearn` to fit a logistic regression model to our phony data, then see how we can use it for classification.\n",
    "\n",
    "Logistic regression can be implemented in `sklearn` with `LogisticRegression`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c624483",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make model object\n",
    "log_reg = \n",
    "\n",
    "## fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstrate predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e17719",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstrate predict_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figure \n",
    "plt.figure(figsize = (10,8))\n",
    "\n",
    "# With classifications we have a new method\n",
    "# predict_proba which returns the probability\n",
    "# that an observation is a certain class.\n",
    "plt.plot(np.linspace(0,1,1000),\n",
    "            log_reg.predict_proba(np.linspace(0,1,1000).reshape(-1,1))[:,1],\n",
    "            'r--',linewidth=2.5,label = \"Model Fit\")\n",
    "plt.scatter(X_train,y_train,label = 'Training Data',alpha=.7)\n",
    "plt.legend(fontsize = 14,loc = 4)\n",
    "plt.xlabel(\"Feature\",fontsize = 16)\n",
    "plt.ylabel(\"p(X)\",fontsize=16) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d27219",
   "metadata": {},
   "source": [
    "### From probabilities to classifications\n",
    "\n",
    "The standard approach to generate classifications is to choose a probability cutoff, for instance if $p(X) \\geq .5$ we classify the instance as $1$, otherwise we say it is a $0$. This is an example of a <i>decision boundary</i>, any point to the left of the boundary gets classified as a $0$, on the right a $1$. Decision boundaries are a big part of many classification algorithms, so this won't be the last time we see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a693c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the cutoff\n",
    "cutoff = \n",
    "\n",
    "## store the predicted probabilities\n",
    "y_prob = \n",
    "\n",
    "## assign the value based on the cutoff\n",
    "y_train_pred = \n",
    "\n",
    "## print the accuracy\n",
    "## input the accuracy after \"is\",\n",
    "print(\"The training accuracy for a cutoff of\",cutoff,\n",
    "      \"is\", np.sum(y_train_pred == y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now plot how the accuracy changes with the cutoff\n",
    "cutoffs = np.arange(0,1.01,.01)\n",
    "accs = []\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    y_train_pred = 1*(y_prob >= cutoff)\n",
    "    accs.append(np.sum(y_train_pred == y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.scatter(cutoffs,accs)\n",
    "\n",
    "plt.xlabel(\"Cutoff\",fontsize=16)\n",
    "plt.ylabel(\"Training Accuracy\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486fa57a",
   "metadata": {},
   "source": [
    "While we produced this plot using the training data, in practice we would use a validation set or cross-validation to select a probability cutoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8fc19",
   "metadata": {},
   "source": [
    "## Interpreting logistic regression\n",
    "\n",
    "One nice thing about this algorithm is that we can interpret the results. This is always a nice feature of an algorithm.\n",
    "\n",
    "Reconsider the statistical model that we fit:\n",
    "$$\n",
    "p(X) = \\frac{1}{1 + e^{- X \\beta}}.\n",
    "$$\n",
    "Rearranging this equation we find the following:\n",
    "$$\n",
    "\\log\\left(\\frac{p(X)}{1-p(X)}\\right) =  X \\beta.\n",
    "$$\n",
    "The expression $p(X)/(1-p(X))$ is known as the odds of the event $y=1$. So the statistical model for logistic regression is really just a linear model for the $\\log$ odds of being class $1$. This allows us to interpret the coefficients of our model.\n",
    "\n",
    "Look at the model we just fit:\n",
    "$$\n",
    "\\log\\left(\\frac{p(x)}{1-p(x)}\\right) = \\beta_0 + \\beta_1 x, \\text{ or } \\text{Odds}|x = C e^{\\beta_1 x}\n",
    "$$\n",
    "where $x$ is the feature, and $C$ is some constant we do not care about. \n",
    "\n",
    "So if we increase $x$ from say $d$ to $d+1$, a $1$ unit increase, then our odds are $e^{\\beta_1}$ units times larger (or smaller depending on the value of $\\beta_1$), we can see this below:\n",
    "$$\n",
    "\\frac{\\text{Odds}|x = d+1}{\\text{Odds}|x=d} = \\frac{e^{\\beta_1 (d+1)}}{e^{\\beta_1 d}} = e^{\\beta_1}\n",
    "$$\n",
    "\n",
    "Let's look at the coefficient from our phony data logistic regression and interpret it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a762290",
   "metadata": {},
   "outputs": [],
   "source": [
    "## demonstrate log_reg.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7226b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A .1 unit increase in our feature multiplies\" + \n",
    "      \" the odds of being classified as 1 by \" + \n",
    "      str(np.round(np.exp(.1*log_reg.coef_[0][0]),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ce53c",
   "metadata": {},
   "source": [
    "### Algorithm Assumptions\n",
    "\n",
    "While we were explaining the concept of logistic regression, we did not mention any of the assumptions of the algorithm. Let's talk about that here before we move on to real data.\n",
    "\n",
    "- Each sample must be independent from all other samples,\n",
    "- When using multiple predictors, they should not be correlated,\n",
    "- The log odds depend linearly on the predictors and\n",
    "- Logistic regression should have a largish data set to work with.\n",
    "\n",
    "We did not worry about these assumptions in this notebook because the data were randomly generated to fit these assumptions. However, in real world applications you may want check them when deciding whether or not logistic regression is a good model choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832db17d",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
