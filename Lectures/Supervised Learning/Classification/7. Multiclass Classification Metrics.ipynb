{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83aeb84d",
   "metadata": {},
   "source": [
    "# Multiclass Classification Metrics\n",
    "\n",
    "Let's expand our model diagnostic toolbox to account for classification problems with more than one possible outcome.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss how to assess multiclass classifiers,\n",
    "- Expand the confusion matrix to account for more than two classes and\n",
    "- Introduce cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5413a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a dark background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732f48c",
   "metadata": {},
   "source": [
    "## Making multiclass into binary\n",
    "\n",
    "The first way you may evaluate a multiclass model is essentially the same as a binary model.\n",
    "\n",
    "Sometimes there may be a couple of classes that you are most interested in, in which case you can just focus on optimizing the performance of binary classifier metrics on those classes.\n",
    "\n",
    "For example, the Cleveland Heart Disease data set, <a href=\"https://archive.ics.uci.edu/ml/datasets/heart+disease\">https://archive.ics.uci.edu/ml/datasets/heart+disease</a>, provides four possible heart disease outcomes, but perhaps we are most interested in the most serious heart disease classification or the case where someone does not have heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6af8b4",
   "metadata": {},
   "source": [
    "## Multiclass confusion matrix\n",
    "\n",
    "Whether we are interested in a couple of classes or all of the classes we can still gather useful information from the confusion matrix. The confusion matrix we introduced earlier naturally extends to more than two possible classes.\n",
    "\n",
    "<img src=\"conf_mat_multi.png\" width=\"70%\"></img>\n",
    "\n",
    "However, in this setting we lose our ability to interpret things like true positive rate, false positve rate, etc.\n",
    "\n",
    "Let's see this in action in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311550c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "X = iris['data']\n",
    "X = X.rename(columns={'sepal length (cm)':'sepal_length',\n",
    "                         'sepal width (cm)':'sepal_width',\n",
    "                         'petal length (cm)':'petal_length',\n",
    "                         'petal width (cm)':'petal_width'})\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84909302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e011c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.copy(), y,\n",
    "                                                       shuffle=True,\n",
    "                                                       random_state=413,\n",
    "                                                       test_size=.2,\n",
    "                                                       stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                s = 60,\n",
    "                label='$y=0$')\n",
    "plt.scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                marker = 'v',\n",
    "                s = 60,\n",
    "                label='$y=1$')\n",
    "plt.scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                marker = 'x',\n",
    "                s = 60,\n",
    "                label='$y=2$')\n",
    "\n",
    "plt.xlabel(\"Petal Width (cm)\", fontsize=18)\n",
    "plt.ylabel(\"Petal Length (cm)\", fontsize=18)\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make LDA object\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "## Fit the model\n",
    "LDA.fit(X_train[['petal_width', 'petal_length']], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd73844",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b564f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(conf_mat,\n",
    "                 columns = ['Predicted 0', 'Predicted 1', 'Predicted 2'],\n",
    "                 index = ['Actual 0', 'Actual 1', 'Actual 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0015d",
   "metadata": {},
   "source": [
    "## Cross-entropy\n",
    "\n",
    "A metric that expands nicely to multiclass classification is known as cross-entropy, sometimes known as $\\log$-loss for reasons that will soon become clear.\n",
    "\n",
    "Cross-entropy is a measure of how well two probability distributions align. Here we want to compare the \"distribution\" of the sample, where we take \"distribution\" to a set of indicator functions, $y_c = 1_{y=c}$, to the estimated probability distribution from the model.\n",
    "\n",
    "For each observation, $i$, we have we compute:\n",
    "\n",
    "$$\n",
    "-\\sum_{c=1}^\\mathcal{C} y_{c,i} \\log \\left( p_{c,i} \\right),\n",
    "$$\n",
    "\n",
    "where $\\mathcal{C}$ is the total number of possible classes and $p_{c,i}$ is the probability that observation $i$ is of class $c$.\n",
    "\n",
    "The total cross-entropy over the sample is the total sum.\n",
    "\n",
    "Let's calculate this now by hand, then show how to do it in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate the ycs\n",
    "ycs = pd.get_dummies(y_train).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ad097",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7301a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate the pcs\n",
    "pcs = LDA.predict_proba(X_train[['petal_width', 'petal_length']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9bba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## take the sum of each rows product\n",
    "- np.sum(ycs * np.log(pcs), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sum that sum\n",
    "np.sum(- np.sum(ycs * np.log(pcs), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa725f",
   "metadata": {},
   "source": [
    "##### In `sklearn`\n",
    "\n",
    "This can be done with `log_loss` in `sklearn`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "## with sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9325b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement log_loss on the training set\n",
    "## first put in the true values\n",
    "## then the predicted probabilities\n",
    "## labels are the labels for the three classes [0,1,2]\n",
    "## normalize=False uses the log-loss formula we presented above\n",
    "log_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0247a1",
   "metadata": {},
   "source": [
    "#### What is the goal of cross entropy?\n",
    "\n",
    "Let's think about the formula for a second:\n",
    "\n",
    "$$\n",
    "-\\sum_{c=1}^\\mathcal{C} y_{c,i} \\log \\left( p_{c,i} \\right),\n",
    "$$\n",
    "\n",
    "The only term that contributes to this sum is the one corresponding to the class that observation $i$ actually is, say it is $l$. If we know the class of $i$ is $l$, then observation $i$ contributes\n",
    "\n",
    "$$\n",
    "- \\log \\left( p_{l,i} \\right)\n",
    "$$\n",
    "\n",
    "to the total cross-entropy. When $p_{l,i}$ is closer to $1$,  $- \\log \\left( p_{l,i} \\right)$ is closer to 0. Conversely as $p_{l,i} \\rightarrow 0$, $- \\log \\left( p_{l,i} \\right) \\rightarrow \\infty$. Since we want to assign observation $i$ to its actual class, what we want is a value of $p_{l,i}$ that is close to $1$. The closer to $1$ each of the $p_{l,i}$s are, the closer to $0$ our cross-entropy is. Thus a good model is one with low cross-entropy.\n",
    "\n",
    "<i>Cross-entropy, \"punishes\" models that are confidently incorrect because such models would have a very low value for $p_{l,i}$.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a3c6f",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960bb54e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
