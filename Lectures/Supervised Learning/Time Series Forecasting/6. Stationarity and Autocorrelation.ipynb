{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b57fd4",
   "metadata": {},
   "source": [
    "# Stationarity and Autocorrelation\n",
    "\n",
    "Before we learn another forecasting technique we have a brief aside on stationarity and autocorrelation.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Define the concepts of:\n",
    "    - Strict stationarity and\n",
    "    - Weak stationarity,\n",
    "- Learn about autocorrelation,\n",
    "- Plot autocorrelation at different lags and\n",
    "- Learn about differencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63bf8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from seaborn import set_style\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25685086",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "\n",
    "<i>Stationarity</i> is a statistical property of a time series that can be needed for a forecasting method to work.\n",
    "\n",
    "We say that a time series $\\left\\lbrace y_t  \\right\\rbrace$ is <i>strictly stationary</i> if the joint probability distribution of $y_{t_1}, y_{t_2}, \\dots y_{t_n}$ is equal to that of $y_{t_1 + \\tau}, y_{t_2 + \\tau}, \\dots y_{t_n + \\tau}$. for any $t_1, \\dots, t_n, \\tau, n$. This implies that the joint distribution only depends on the intervals between $t_1, \\dots, t_n$ and in particular if $n=1$:\n",
    "\n",
    "$$\n",
    "E(y_t) =  \\mu, \\ \\ \\ \\text{Var}(y_t) = \\sigma^2\n",
    "$$\n",
    "\n",
    "are not functions of $t$. Further if $n=2$ the joint distribution of $y_{t_1}$ and $y_{t_2}$ only depends on $(t_2 - t_1)$, which is called the <i>lag</i> between $t_1$ and $t_2$.\n",
    "\n",
    "Strict stationarity is quite restrictive and we more often define a weaker sense of <i>stationarity</i>.\n",
    "\n",
    "We will say that a time series is <i>stationary</i> if:\n",
    "\n",
    "$$\n",
    "E(y_t) = \\mu, \\ \\text{ and } \\ \\text{Cov}\\left( y_t, y_{t+\\tau} \\right) = \\gamma(\\tau).\n",
    "$$\n",
    "\n",
    "### Examples\n",
    "\n",
    "Some examples of a stationary time series are:\n",
    "- White noise, \n",
    "- The first differences of a random walk, i.e. \n",
    "\n",
    "$$\n",
    "y_{t+1} - y_t \\text{ if } y_{t+1} = y_t + \\epsilon \\text{ for all } t \\text{ and}\n",
    "$$\n",
    "\n",
    "- A moving average process, i.e. \n",
    "\n",
    "$$\n",
    "y_{t} = \\beta_0 \\epsilon_{t} + \\beta_1 \\epsilon_{t-1} + \\beta_2 \\epsilon_{t-2} + \\dots + \\beta_q \\epsilon_{t-q}.\n",
    "$$\n",
    "\n",
    "In the next forecasting approach we will learn it is important for the time series data to be stationary. One way to visually gauge if that is true is by plotting the autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8069abc",
   "metadata": {},
   "source": [
    "## Autocorrelation\n",
    "\n",
    "The <i>autocorrelation</i> of a time series is essentially the correlation of that time series with its future observations placed at different lags. In particular, the autocorrelation of a time series at lag $k$ is given by:\n",
    "\n",
    "$$\n",
    "r_k = \\frac{\\sum_{t=1}^{n-k} \\left(y_t - \\overline{y}\\right) \\left(y_{t+k} - \\overline{y} \\right)}{\\sum_{t=1}^n \\left(y_t - \\overline{y}\\right)^2},\n",
    "$$\n",
    "\n",
    "where $n$ is the last observation of the time series and $\\overline{y} = \\frac{1}{n}\\sum_{t=1}^n y_t$. This is a function of the lag.\n",
    "\n",
    "### Correlogram\n",
    "\n",
    "We commonly plot this as a way to visually probe if our data series clearly violates stationarity. This can be done with `statsmodels` `plot_acf` function, <a href=\"https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_acf.html\">https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_acf.html</a>.\n",
    "\n",
    "#### Example: White noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440457f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make some data\n",
    "## white noise\n",
    "np.random.seed(440)\n",
    "series = np.random.random(100) - .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "\n",
    "## first put in the time series\n",
    "## then set how many lags you want to consider\n",
    "## the default is lags=30\n",
    "## we will turn alpha=None\n",
    "## this turns off the confidence interval lines, which we will not discuss\n",
    "sm.graphics.tsa.plot_acf()\n",
    "\n",
    "plt.title('White Noise', fontsize=18)\n",
    "plt.ylabel(\"Autocorrelation\", fontsize=16)\n",
    "plt.xlabel(\"Lag\", fontsize=16)\n",
    "\n",
    "plt.ylim(-1.1,1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395e1d58",
   "metadata": {},
   "source": [
    "The autocorrelation of every time series will be $1$ at lag $0$. For stationary series, like white noise, we excpect to seem some low autocorrelation for small lags. Autocorrelation should then tend to randomly bubble around getting closer and closer to $0$ as the lag increases.\n",
    "\n",
    "#### Example: Data with a trend\n",
    "\n",
    "If your data exhibits a trend it is <b>not</b> stationary. Here is an example of what the associated correlogram may look like for data with a trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "goog = pd.read_csv(\"../../../Data/google_stock.csv\", parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(14,6))\n",
    "\n",
    "sm.graphics.tsa.plot_acf()\n",
    "\n",
    "plt.title('Trend - Google Stock Data', fontsize=18)\n",
    "plt.ylabel(\"Autocorrelation\", fontsize=16)\n",
    "plt.xlabel(\"Lag\", fontsize=16)\n",
    "\n",
    "plt.ylim(-1.1,1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a796e7c9",
   "metadata": {},
   "source": [
    "If you see a correlogram that has high (in magnitude) autocorrelation values over a long range of lags this is indicative of a time series that is not stationary. Importantly, seing a time series with a correlogram like this does not necessarily imply that the time series in question exhibits a trend, just that it is not stationary.\n",
    "\n",
    "<i>We will see how we may turn this into a stationary series below</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f20de",
   "metadata": {},
   "source": [
    "#### Example: Data with seasonality\n",
    "\n",
    "If your data exhibits seasonality it is <b>not</b> stationary. Here is an example that demonstrates what a seasonal correlogram may look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flu = pd.read_csv(\"../../../Data/us_flu_1928_1948.csv\", parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5006ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(14,6))\n",
    "\n",
    "sm.graphics.tsa.plot_acf()\n",
    "\n",
    "plt.title('Seasonality - Flu Cases', fontsize=18)\n",
    "plt.ylabel(\"Autocorrelation\", fontsize=16)\n",
    "plt.xlabel(\"Lag\", fontsize=16)\n",
    "\n",
    "plt.ylim(-1.1,1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d654af4",
   "metadata": {},
   "source": [
    "Time series with seasonality often produce sinusoidal autocorrelation plots. Similar to our above point, seeing sinusoidal autocorrelation may hint at seasonality but is not always proof that a time series exhibits seasonality. However, it does imply that the time series is probably not stationary.\n",
    "\n",
    "<i>There are also techniques for producing a stationary time series out of a seasonal one, but we leave that for the corresponding `Practice Problems` notebook.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8fba7",
   "metadata": {},
   "source": [
    "### Statistical tests\n",
    "\n",
    "There are formal statistical tests that you can perform to examine if a time series whether a time series is stationary, but we will not cover them. Interested parties may wish to start here:\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Unit_root_test\">https://en.wikipedia.org/wiki/Unit_root_test</a> and\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test\">https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1252d3b8",
   "metadata": {},
   "source": [
    "## Differencing\n",
    "\n",
    "At times we may wish to apply a forecasting method that requires a stationary time series. It is often possible to produce a stationary series from a non-stationary one with <i>differencing</i>. We will demonstrate this process for data without seasonality, the approach for data with seasonality is left for the `Practice Problems` notebook.\n",
    "\n",
    "Differencing refers to subtracting the previous observation from the current. For example, the first differences of a time series $\\left\\lbrace y_t \\right\\rbrace$ are:\n",
    "\n",
    "$$\n",
    "\\nabla y_2 = y_2 - y_1, \\nabla y_3 = y_3 - y_2, \\nabla y_4 = y_4 - y_3, \\dots \\nabla y_t = y_t - y_{t-1}.\n",
    "$$\n",
    "\n",
    "The series $\\left\\lbrace \\nabla y_t \\right\\rbrace$ now begins indexing at $2$ and is $1$ observation smaller than the original series. In some sense we can think of the first differences as analagous to the first derivative of a function.\n",
    "\n",
    "#### In python\n",
    "\n",
    "We can perform differencing quickly with a the `.diff()` function in `pandas`, <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.diff.html\">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.diff.html</a>.\n",
    "\n",
    "For non-seasonal data a single difference is typically sufficient to produce a stationary time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(14,6))\n",
    "\n",
    "## use pandas .diff()\n",
    "sm.graphics.tsa.plot_acf()\n",
    "\n",
    "plt.title('First Differenced Google Stock Data', fontsize=18)\n",
    "plt.ylabel(\"Autocorrelation\", fontsize=16)\n",
    "plt.xlabel(\"Lag\", fontsize=16)\n",
    "\n",
    "plt.ylim(-1.1,1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83515dd",
   "metadata": {},
   "source": [
    "Sometimes you may need to perform differencing more than once. You perform additional differencing to the series that resulted from the previous differencing step. For example:\n",
    "\n",
    "$$\n",
    "\\nabla ^2 y_t = \\nabla \\nabla y_t = \\nabla y_t - \\nabla y_{t-1} =  \\left(y_t - y_{t-1}\\right) - \\left(y_{t-1} - y_{t-2}\\right) = y_t - 2y_{t-1} + y_{t-2}.\n",
    "$$\n",
    "\n",
    "Applying further differencing is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c55d29a",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85de04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
