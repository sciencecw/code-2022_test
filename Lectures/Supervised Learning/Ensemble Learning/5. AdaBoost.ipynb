{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ddf371d",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "\n",
    "AdaBoost is short for adaptive boosting. AdaBoost trains a series of weak learners, with each subsequent classifier paying more attention to the training instances that were misclassified by its predecessors.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss the theoretical foundation of the AdaBoost algorithm,\n",
    "- Demonstrate how to implement AdaBoost in `sklearn` and\n",
    "- Provide references for you to learn more about AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6603493",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3648e697",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "AdaBoost (short for adaptive boosting) is an algorithm that builds a, hopefully, strong learner with an iterative sequence of weak learners.\n",
    "\n",
    "The first weak learner is trained on the training data normally. For weak learner $j$, the weights on each training sample are determined by the performance by weak learner $j-1$. After training $W$ total weak learners a final prediction is made by performing a weighted vote among all $W$ weak learners, with voting weight determined by the learner's accuracy (in the case of classification).\n",
    "\n",
    "Let's explicitly set this up now, then we will demonstrate the process with a toy example. For this example we will be working with a classification problem.\n",
    "\n",
    "Let $y^{(i)}$ denote the class of observation $i$, $\\hat{y}^{(i)}_j$ the prediction on observation $i$ of weak learner $j$ and $w^{(i)}$ the current weight assigned to observation $i$.\n",
    "\n",
    "After the $j^{\\text{th}}$ weak learner is trained, that learner's <i>weighted error rate</i> is calculated:\n",
    "\n",
    "$$\n",
    "r_j = \\frac{\\sum_{i=1}^n w^{(i)} 1_{\\{ \\hat{y}^{(i)}_j \\neq y^{(i)}\\}} }{\\sum_{i=1}^n w^{(i)}} = 1 -  \\text{weighted accuracy}.\n",
    "$$\n",
    "\n",
    "$r_j$ is large precisely when the $j^\\text{th}$ weak learner is bad and $r_j$ is small when the $j^\\text{th}$ weak learner is good. The next step is to compute the weight assigned to the weak learner itself:\n",
    "\n",
    "$$\n",
    "\\alpha_j = \\eta \\log \\left( \\frac{1-r_j}{r_j} \\right),\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the <i>learning rate</i> of the algorithm, a hyperparameter you must set prior to fitting the algorithm. Notice that $\\alpha_j$ is small when $r_j$ is large (a bad weak learner) and large when $r_j$ is small (a good weak learner). Finally we update the training sample weights for weak learner $j+1$:\n",
    "\n",
    "$$\n",
    "w^{(i)} \\leftarrow \\left\\lbrace \\begin{array}{l c}w^{(i)} & \\text{if } \\hat{y}^{(i)}_j = y^{(i)} \\\\\n",
    "                                    w^{(i)} \\exp(\\alpha_j) & \\text{if } \\hat{y}^{(i)}_j \\neq y^{(i)}\\end{array}\\right.,\n",
    "$$\n",
    "\n",
    "which will increase the weight on incorrectly predict samples (assuming $\\alpha_j > 0$).\n",
    "\n",
    "#### A toy example\n",
    "\n",
    "Let's demonstrate what's going on with AdaBoost using a toy example, let's say I want to train an AdaBoost classifier on these data:\n",
    "\n",
    "<img src=\"ensemble1.png\" width = \"60%\"></img>\n",
    "\n",
    "For the first weak learner each observation is given a uniform weight $w_i = \\frac{1}{6}$, $i=1,\\dots,6$. Let's say that this first weak learner gives a decision rule like so:\n",
    "\n",
    "<img src=\"ensemble2.png\" width = \"60%\"></img>\n",
    "\n",
    "AdaBoost would now calculate in order, $r_1$, $\\alpha_1$ and then the new weights, for simplicity let's take $\\eta=1$ as our learning rate.\n",
    "\n",
    "$$\n",
    "r_1 = \\frac{0 + 0 + 1/6 + 0 + 0 + 0}{1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6} = \\frac{1}{6},\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\alpha_1 = \\log\\left(\\frac{1-1/6}{1/6}\\right) =  \\log(5)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "w^{(1)} \\leftarrow \\frac{1}{6},\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(2)} \\leftarrow \\frac{1}{6},\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(3)} \\leftarrow \\frac{1}{6} \\exp\\left( \\log(5) \\right) = \\frac{5}{6},\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(4)} \\leftarrow \\frac{1}{6},\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(5)} \\leftarrow \\frac{1}{6},\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(6)} \\leftarrow \\frac{1}{6}.\n",
    "$$\n",
    "\n",
    "These new weights are then used in training the second weak learner which, due to the increased weight on observation $3$, would produce a new decision boundary:\n",
    "\n",
    "<img src=\"ensemble3.png\" width = \"60%\"></img>\n",
    "\n",
    "We now calculate $r_2$, $\\alpha_2$ and update the weights.\n",
    "\n",
    "$$\n",
    "r_2 = \\frac{0 + 0 + 0 + 1/6 + 0 + 0}{1/6 + 1/6 + 5/6 + 1/6 + 1/6 + 1/6} = \\frac{1}{10},\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\alpha_2 = \\log\\left( \\frac{1-1/10}{1/10} \\right) = \\log(9)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "w^{(1)} \\leftarrow \\frac{1}{6},\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(2)} \\leftarrow \\frac{1}{6},\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(3)} \\leftarrow \\frac{5}{6},\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(4)} \\leftarrow \\frac{1}{6} \\exp\\left( \\log(9) \\right) = \\frac{3}{2},\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(5)} \\leftarrow \\frac{1}{6},\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(6)} \\leftarrow \\frac{1}{6}.\n",
    "$$\n",
    "\n",
    "\n",
    "If we were to stop here predictions would be made using a weighted vote where weak learner 1 gets a vote worth $\\log(5)$ and weak learner 2 get a vote worth $\\log(9)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7ed1a",
   "metadata": {},
   "source": [
    "## In `sklearn`\n",
    "\n",
    "Let's now demonstrate how to implement this algorithm as a classifier in `sklearn` with `AdaBoostClassifier`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html</a>. <i>For an adaptive boosting regression model check out `AdaBoostRegressor`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html</a>.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c83699",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(888)\n",
    "\n",
    "X = np.zeros((25,2))\n",
    "X[:,0] = np.random.random(25)\n",
    "X[:,1] = np.random.random(25)\n",
    "\n",
    "y = np.zeros(25)\n",
    "y[X[:,0]-X[:,1]>=0] = 1\n",
    "\n",
    "## for predictions\n",
    "xx1, xx2 = np.meshgrid(np.arange(-.01, 1.01, .01),\n",
    "                          np.arange(-.01, 1.01, .01))\n",
    "\n",
    "X_pred = np.zeros((len(xx1.reshape(-1,1)), 2))\n",
    "X_pred[:,0] = xx1.flatten()\n",
    "X_pred[:,1] = xx2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.scatter(X[y == 0,0], \n",
    "            X[y == 0,1],\n",
    "            c='blue',\n",
    "            label=\"0\")\n",
    "plt.scatter(X[y == 1,0], \n",
    "            X[y == 1,1],\n",
    "            c='orange',\n",
    "            marker='v',\n",
    "            label=\"1\")\n",
    "\n",
    "\n",
    "plt.plot([0,1],\n",
    "         [0,1],\n",
    "         'k--',\n",
    "         label=\"Actual Decision Boundary\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 16)\n",
    "plt.legend(fontsize='14', loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f6ae8",
   "metadata": {},
   "source": [
    "For our weak learner we will use a decision stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fec175",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import AdaBoost\n",
    "\n",
    "\n",
    "## Import the base classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1460be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(3,2, figsize=(16,30))\n",
    "\n",
    "j = 0\n",
    "for i in range(1,7):\n",
    "    # n_estimators controls how many weak learners we use\n",
    "    # learning_rate is a hyperparameter that controls how\n",
    "    # aggressively we correct incorrect labels\n",
    "    # algorithm is the algorithm that sklearn runs to fit the model\n",
    "    # SAMME.R or SAMME are the options, SAMME.R allows calculation\n",
    "    # of probabilities.\n",
    "    ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                                    n_estimators=i,\n",
    "                                    algorithm = 'SAMME.R',\n",
    "                                    random_state=123)\n",
    "    \n",
    "    # fit the classifier\n",
    "    ada_clf.fit(X, y)\n",
    "    \n",
    "    # make some predictions\n",
    "    preds = ada_clf.predict(X_pred)\n",
    "    \n",
    "    \n",
    "    ## this plots the resulting decision boundary ##\n",
    "    ax[j//2,j%2].scatter(X_pred[preds==1,0],\n",
    "                X_pred[preds==1,1],\n",
    "                alpha=.1,\n",
    "                c='orange',\n",
    "                s=20)\n",
    "    ax[j//2,j%2].scatter(X_pred[preds==0,0],\n",
    "                X_pred[preds==0,1],\n",
    "                alpha=.1,\n",
    "                c='lightblue',\n",
    "                s=20)\n",
    "    \n",
    "    ax[j//2,j%2].scatter(X[y == 1,0],\n",
    "                         X[y == 1,1],\n",
    "                         s=100,\n",
    "                         c = \"darkorange\",\n",
    "                         marker = 'v',\n",
    "                         label=\"Training 1\", \n",
    "                         edgecolor='black')\n",
    "    ax[j//2,j%2].scatter(X[y == 0,0],\n",
    "                         X[y == 0,1],\n",
    "                         s=100,\n",
    "                         c = \"blue\",\n",
    "                         label=\"Training 0\", \n",
    "                         edgecolor='black')\n",
    "    \n",
    "    ax[j//2,j%2].set_title(str(i) + \" Weak Learners\", fontsize=16)\n",
    "\n",
    "    \n",
    "    j = j+1\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cab4e9",
   "metadata": {},
   "source": [
    "As you might notice when we increase the number of weak learners we use in the model we tend to overfit to training data. One way to control for this is to not use too many estimators. Where as always the correct number of estimators depends on the data you are fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b62d8a",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Some additional references on AdaBoost models can be found below:\n",
    "\n",
    "<a href = \"https://link.springer.com/content/pdf/10.1007/BF00058655.pdf\">Bagging Predictors</a> by Leo Breiman\n",
    "\n",
    "<a href = \"http://rob.schapire.net/papers/Schapire99c.pdf\">A Brief Introduction to Boosting</a> by Robert E. Schapire\n",
    "\n",
    "<a href = \"https://mitpress.mit.edu/books/boosting\">Boosting</a> by  Robert E. Schapire and Yoav Freund\n",
    "\n",
    "<a href = \"https://www.csie.ntu.edu.tw/~mhyang/course/u0030/papers/schapire.pdf\">A Boosting Tutorial</a> by Robert E.  Schapire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea600c",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad863b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
