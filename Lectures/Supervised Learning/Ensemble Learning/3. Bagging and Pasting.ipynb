{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fcb5d8",
   "metadata": {},
   "source": [
    "# Bagging and Pasting\n",
    "\n",
    "We review bagging and pasting learners, a generalization of the concept behind random forests.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Review the concept behind bagging and pasting and\n",
    "- Demonstrate how to implement them in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4431d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a051022",
   "metadata": {},
   "source": [
    "## With or without replacement\n",
    "\n",
    "If we recall from our random forest notebook we used bagging to proved each decision tree in a random forest with its own randomly selected data set. This random selection was accomplished by randomly choosing subsets of the training set <b>with</b> replacement.\n",
    "\n",
    "Both bagging and pasting refer to this process of producing a number of randomly selected subsets of the training data which are in turn used to train the same type of model/algorithm. The key difference between bagging and pasting is whether that sampling is done <b>with</b> or <b>without</b> replacement:\n",
    "\n",
    "- <i>Bagging</i>: samples <b>with</b> replacement and\n",
    "- <i>Pasting</i>: samples <b>without</b> replacement.\n",
    "\n",
    "A way to remember which is which is to remember where the origins of the term bagging. Bagging is short for Bootstrap AGGregatING. When we want to use bagging in `sklearn` we set `bootstrap = True`. So if you want sampling with replacement you want `bootstrap = True`, so you want bagging. Conversely, if you want sampling without replacement you want `bootstrap = False`, so you want pasting.\n",
    "\n",
    "Bagging/pasting can be applied to any kind of supervised learning algorithm, but the training time of the algorithm does limit how many models you can realistically use in the ensemble.\n",
    "\n",
    "## In `sklearn`\n",
    "\n",
    "`sklearn` offers the `BaggingClasifier` to build both pasting and bagging models, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html</a>.\n",
    "\n",
    "We will demonstrate this with a base $k$ nearest neighbors classifier and the synthetic data set we have used in the past couple of notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a25f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4930)\n",
    "\n",
    "X = np.zeros((200,2))\n",
    "X[:,0] = np.random.random(200)\n",
    "X[:,1] = np.random.random(200)\n",
    "\n",
    "y = np.zeros(200)\n",
    "y[X[:,0]-X[:,1]>=0] = 1\n",
    "\n",
    "## to show off our decision boundary later\n",
    "xx1, xx2 = np.meshgrid(np.arange(-.01, 1.01, .01),\n",
    "                          np.arange(-.01, 1.01, .01))\n",
    "\n",
    "X_pred = np.zeros((len(xx1.reshape(-1,1)), 2))\n",
    "X_pred[:,0] = xx1.flatten()\n",
    "X_pred[:,1] = xx2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2851058",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.scatter(X[y == 0,0], \n",
    "            X[y == 0,1],\n",
    "            c='blue',\n",
    "            label=\"0\")\n",
    "plt.scatter(X[y == 1,0], \n",
    "            X[y == 1,1],\n",
    "            c='orange',\n",
    "            marker='v',\n",
    "            label=\"1\")\n",
    "plt.plot([0,1],[0,1],'k--',label=\"Actual Decision Boundary\")\n",
    "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 16)\n",
    "plt.legend(fontsize='14', loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the model objects here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's use a knn classifier\n",
    "## base_estimator is the estimator we want to use in our ensemble\n",
    "## if bootstrap = True we use bagging\n",
    "## n_estimators is how many models we fit\n",
    "## max_samples is the number of training points sampled\n",
    "bag = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Let's use a knn classifier\n",
    "## base_estimator is the estimator we want to use in our ensemble\n",
    "## if bootstrap = False it is pasting\n",
    "## n_estimators is how many models we fit\n",
    "## max_samples is the number of training points sampled\n",
    "paste = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## We'll compare it to a single knn\n",
    "knn = KNeighborsClassifier(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(24,8))\n",
    "\n",
    "## Fit knn\n",
    "knn.fit(X,y)\n",
    "y_pred = knn.predict(X)\n",
    "\n",
    "acc = sum(y == y_pred)/len(y_pred)\n",
    "\n",
    "ax[0].set_title(\"KNN Training Accuracy \" + str(np.round(acc,5)), fontsize=16)\n",
    "\n",
    "preds = knn.predict(X_pred)\n",
    "    \n",
    "\n",
    "\n",
    "ax[0].scatter(X_pred[preds==1,0],\n",
    "            X_pred[preds==1,1],\n",
    "            alpha=.2,\n",
    "            c='orange',\n",
    "            s=10)\n",
    "ax[0].scatter(X_pred[preds==0,0],\n",
    "            X_pred[preds==0,1],\n",
    "            alpha=.2,\n",
    "            c='lightblue',\n",
    "            s=10)\n",
    "\n",
    "ax[0].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[0].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            marker = 'v',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[0].plot([0,1],[0,1],'k--')\n",
    "\n",
    "\n",
    "\n",
    "## Fit Bagged Data\n",
    "bag.fit(X,y)\n",
    "y_pred = bag.predict(X)\n",
    "\n",
    "acc = sum(y == y_pred)/len(y_pred)\n",
    "\n",
    "ax[1].set_title(\"Bagging Training Accuracy \" + str(np.round(acc,5)), fontsize=16)\n",
    "\n",
    "preds = bag.predict(X_pred)\n",
    "    \n",
    "\n",
    "\n",
    "ax[1].scatter(X_pred[preds==1,0],\n",
    "            X_pred[preds==1,1],\n",
    "            alpha=.2,\n",
    "            c='orange',\n",
    "            s=10)\n",
    "ax[1].scatter(X_pred[preds==0,0],\n",
    "            X_pred[preds==0,1],\n",
    "            alpha=.2,\n",
    "            c='lightblue',\n",
    "            s=10)\n",
    "\n",
    "ax[1].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            marker = 'v',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[1].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[1].plot([0,1],[0,1],'k--')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Fit Paste Data\n",
    "paste.fit(X,y)\n",
    "y_pred = paste.predict(X)\n",
    "\n",
    "acc = sum(y == y_pred)/len(y_pred)\n",
    "\n",
    "ax[2].set_title(\"Pasting Training Accuracy \" + str(np.round(acc,5)), fontsize=16)\n",
    "\n",
    "preds = paste.predict(X_pred)\n",
    "    \n",
    "\n",
    "\n",
    "ax[2].scatter(X_pred[preds==0,0],\n",
    "            X_pred[preds==0,1],\n",
    "            alpha=.2,\n",
    "            c='lightblue',\n",
    "            s=10)\n",
    "ax[2].scatter(X_pred[preds==1,0],\n",
    "            X_pred[preds==1,1],\n",
    "            alpha=.2,\n",
    "            c='orange',\n",
    "            s=10)\n",
    "\n",
    "ax[2].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            marker = 'v',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[2].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[2].plot([0,1],[0,1],'k--')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d28c05",
   "metadata": {},
   "source": [
    "## Why use a bagging/pasting model?\n",
    "\n",
    "Bagging or pasting introduces bias into the model through the random selection process. This is because the extremes of the data set (which could lead to overfitting in a single model) are not typically selected in the random selection process because they are rare observations. Thinking back to our bias-variance trade-off notebook, this means that bagging or pasting could improve the performance of a single base model by reducing overfitting.\n",
    "\n",
    "Let's see that in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389be386",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[5] = 0\n",
    "y[173] = 0\n",
    "\n",
    "y[69] = 1\n",
    "y[122] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.scatter(X[y == 0,0], \n",
    "            X[y == 0,1],\n",
    "            c='blue',\n",
    "            label=\"0\")\n",
    "plt.scatter(X[y == 1,0], \n",
    "            X[y == 1,1],\n",
    "            c='orange',\n",
    "            marker='v',\n",
    "            label=\"1\")\n",
    "plt.plot([0,1],[0,1],'k--',label=\"Actual Decision Boundary\")\n",
    "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 16)\n",
    "plt.legend(fontsize='14', loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's use a knn classifier\n",
    "## base_estimator is the estimator we want to use in our ensemble\n",
    "## if bootstrap = True we use bagging\n",
    "## n_estimators is how many models we fit\n",
    "## max_samples is the number of training points sampled\n",
    "bag = BaggingClassifier(base_estimator = KNeighborsClassifier(4),\n",
    "                           n_estimators = 100,\n",
    "                           max_samples = 100,\n",
    "                           bootstrap = True,\n",
    "                           random_state = 7556)\n",
    "\n",
    "\n",
    "\n",
    "## Let's use a knn classifier\n",
    "## base_estimator is the estimator we want to use in our ensemble\n",
    "## if bootstrap = False it is pasting\n",
    "## n_estimators is how many models we fit\n",
    "## max_samples is the number of training points sampled\n",
    "paste = BaggingClassifier(base_estimator = KNeighborsClassifier(4),\n",
    "                           n_estimators = 100,\n",
    "                           max_samples = 100,\n",
    "                           bootstrap = False,\n",
    "                           random_state = 892)\n",
    "\n",
    "\n",
    "## We'll compare it to a single knn\n",
    "knn = KNeighborsClassifier(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54373d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(24,8))\n",
    "\n",
    "## Fit knn\n",
    "knn.fit(X,y)\n",
    "y_pred = knn.predict(X)\n",
    "\n",
    "acc = sum(y == y_pred)/len(y_pred)\n",
    "\n",
    "ax[0].set_title(\"KNN Training Accuracy \" + str(np.round(acc,5)), fontsize=16)\n",
    "\n",
    "preds = knn.predict(X_pred)\n",
    "    \n",
    "\n",
    "\n",
    "ax[0].scatter(X_pred[preds==1,0],\n",
    "            X_pred[preds==1,1],\n",
    "            alpha=.2,\n",
    "            c='orange',\n",
    "            s=10)\n",
    "ax[0].scatter(X_pred[preds==0,0],\n",
    "            X_pred[preds==0,1],\n",
    "            alpha=.2,\n",
    "            c='lightblue',\n",
    "            s=10)\n",
    "\n",
    "ax[0].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[0].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            marker = 'v',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[0].plot([0,1],[0,1],'k--')\n",
    "\n",
    "\n",
    "\n",
    "## Fit Bagged Data\n",
    "bag.fit(X,y)\n",
    "y_pred = bag.predict(X)\n",
    "\n",
    "acc = sum(y == y_pred)/len(y_pred)\n",
    "\n",
    "ax[1].set_title(\"Bagging Training Accuracy \" + str(np.round(acc,5)), fontsize=16)\n",
    "\n",
    "preds = bag.predict(X_pred)\n",
    "    \n",
    "\n",
    "\n",
    "ax[1].scatter(X_pred[preds==1,0],\n",
    "            X_pred[preds==1,1],\n",
    "            alpha=.2,\n",
    "            c='orange',\n",
    "            s=10)\n",
    "ax[1].scatter(X_pred[preds==0,0],\n",
    "            X_pred[preds==0,1],\n",
    "            alpha=.2,\n",
    "            c='lightblue',\n",
    "            s=10)\n",
    "\n",
    "ax[1].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            marker = 'v',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[1].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[1].plot([0,1],[0,1],'k--')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Fit Paste Data\n",
    "paste.fit(X,y)\n",
    "y_pred = paste.predict(X)\n",
    "\n",
    "acc = sum(y == y_pred)/len(y_pred)\n",
    "\n",
    "ax[2].set_title(\"Pasting Training Accuracy \" + str(np.round(acc,5)), fontsize=16)\n",
    "\n",
    "preds = paste.predict(X_pred)\n",
    "    \n",
    "\n",
    "\n",
    "ax[2].scatter(X_pred[preds==0,0],\n",
    "            X_pred[preds==0,1],\n",
    "            alpha=.2,\n",
    "            c='lightblue',\n",
    "            s=10)\n",
    "ax[2].scatter(X_pred[preds==1,0],\n",
    "            X_pred[preds==1,1],\n",
    "            alpha=.2,\n",
    "            c='orange',\n",
    "            s=10)\n",
    "\n",
    "ax[2].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            marker = 'v',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[2].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[2].plot([0,1],[0,1],'k--')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd90fc",
   "metadata": {},
   "source": [
    "## Bagging vs. Pasting\n",
    "\n",
    "In general people tend to using bagging as a default and pasting does not get used as much.\n",
    "\n",
    "The main reason is because of sample size. In order to be effective, pasting needs a large data set. With smaller data sets the \"random\" sample tends to be the same across your estimators. If you have a very large data set it may be worth trying pasting as well as bagging and comparing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbccf5",
   "metadata": {},
   "source": [
    "## Bagging/Pasting regressors\n",
    "\n",
    "Bagging and pasting can be implemented with regression models as well, where the prediction for a particular value of $X^*$ is the average of all the bagging base model predictions. In `sklearn` it is implemented with `BaggingRegressor` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f79f7",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "Here is a reference on bagging/pasting.\n",
    "\n",
    "<a href = \"https://link.springer.com/content/pdf/10.1007/BF00058655.pdf\">Bagging Predictors</a> by Leo Breiman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9791be3f",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e4d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
