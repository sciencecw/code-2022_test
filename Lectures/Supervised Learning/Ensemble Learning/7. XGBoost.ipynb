{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef43718",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Let's learn about a very popular package for gradient boosting called `XGBoost`.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce the `XGBoost` package and point to the package installation process,\n",
    "- Discuss what `XGBoost` is and why we use it over `sklearn` and\n",
    "- Show how to implement gradient boosting regression in `XGBoost`:\n",
    "    - Demonstrate `XGBoost` early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da4cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccbbda7",
   "metadata": {},
   "source": [
    "## Gradient boosting reminder\n",
    "\n",
    "In the previous notebook we learned what gradient boosting was and demonstrated how to implement it using `sklearn`'s `GradientBoostingRegressor` model object. Recall that this technique is a boosting approach where we iteratively train weak learners by training on the previous learner's residuals. \n",
    "\n",
    "## What is `XGBoost`?\n",
    "\n",
    "While we implemented this algorithm using `sklearn`, another very popular package for gradient boosting is `XGBoost` which stands for eXtreme Gradient Boosting. This particular package is often utilized in winning data science competitions, which likely led to its increase in popularity.\n",
    "\n",
    "#### Installing `XGBoost`\n",
    "\n",
    "A quick note! You likely do not have `XGBoost` already installed on your computer (at least I did not prior to writing this notebook). If you have used `pip` to install python packages before you can install `XGBoost` using the command here, <a href=\"https://xgboost.readthedocs.io/en/latest/install.html#python\">https://xgboost.readthedocs.io/en/latest/install.html#python</a>. If you use `conda` to install packages this link should help, <a href=\"https://anaconda.org/conda-forge/xgboost\">https://anaconda.org/conda-forge/xgboost</a>.\n",
    "\n",
    "<i>Note: When I installed `XGBoost` on my machine it did not work at first, I had to install another piece of softward onto my MacBook before it would work. Follow the documentation from `XGBoost`, it worked for me.</i>\n",
    "\n",
    "<i>Also Note: If you are running a Mac with an M1 chip the standard installation instructions may not work for you. In that case you should perform a web search to find the relevant instructions.</i>\n",
    "\n",
    "### Why `XGBoost`?\n",
    "\n",
    "Why do so many people like using `XGBoost` over `sklearn`'s `GradientBoostingRegressor` and `GradientBoostingClassifier`? In comparison to `sklearn`'s implementation `XGBoost`'s code for fitting gradient boosting models is much faster and tends to perform better than `sklearn`. It even offers the capability for your model to be trained in parallel, which `sklearn` does not currently offer for gradient boosting.\n",
    "\n",
    "## Implementing gradient boosting regression in `XGBoost`\n",
    "\n",
    "With this motivation in mind, let's learn how to implement the same regression functionality we did with `sklearn` in the previous notebook. We will provide information on how to run gradient boosting classification and expand on the `XGBoost` syntax in the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First make our data set\n",
    "np.random.seed(220)\n",
    "X = np.linspace(-2,2,200)\n",
    "\n",
    "y = X**2 + np.random.randn(200)\n",
    "\n",
    "## Visualize the training data\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X,y)\n",
    "plt.xlabel(\"$X$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589909b5",
   "metadata": {},
   "source": [
    "One way to make a gradient boosting regressor in `XGBoost` is to use `XGBRegressor`, <a href=\"https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\">https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6cdc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import xgboost\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's recreate our learning_rate comparison\n",
    "\n",
    "### Create an XGBRegressor object\n",
    "### learning_rate=.1, max_depth=1, n_estimators=10\n",
    "xgb_reg1 = \n",
    "\n",
    "\n",
    "## fit it\n",
    "xgb_reg1\n",
    "\n",
    "### Create an XGBRegressor object\n",
    "### learning_rate=1, max_depth=1, n_estimators=10\n",
    "xgb_reg2 =\n",
    "\n",
    "\n",
    "## fit it\n",
    "xgb_reg2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(20,8))\n",
    "\n",
    "ax[0].scatter(X,y,label='Training Points')\n",
    "ax[0].plot(X, xgb_reg1.predict(X.reshape(-1,1)), 'k',label=\"Prediction\")\n",
    "ax[0].set_title(\"learning_rate=0.1\", fontsize=18)\n",
    "ax[0].legend(fontsize=14)\n",
    "ax[0].set_xlabel(\"$X$\", fontsize=16)\n",
    "ax[0].set_ylabel(\"$y$\", fontsize=16)\n",
    "\n",
    "ax[1].scatter(X,y,label='Training Points')\n",
    "ax[1].plot(X, xgb_reg2.predict(X.reshape(-1,1)), 'k',label=\"Prediction\")\n",
    "ax[1].set_title(\"learning_rate=1\", fontsize=18)\n",
    "ax[1].legend(fontsize=14)\n",
    "ax[1].set_xlabel(\"$X$\", fontsize=16)\n",
    "ax[1].set_ylabel(\"$y$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221352f",
   "metadata": {},
   "source": [
    "A nice feature of `xgboost`'s model is that it automatically records the performance at each training step on a validation set, provided we give the model the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49cb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here I will generate a validation set because the data are randomly generated\n",
    "## in practice you would need to split the data\n",
    "X_val = np.linspace(-2,2,200)\n",
    "y_val = X_val**2 + np.random.randn(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a51777",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make an XGBRegressor object\n",
    "## n_estimators = 500, max_depth = 1, learning_rate = .1\n",
    "xgb_reg = xgboost.XGBRegressor(n_estimators=500,\n",
    "                          max_depth=1,\n",
    "                          learning_rate=.1)\n",
    "\n",
    "## fit the model, including an eval_set\n",
    "xgb_reg.fit(X.reshape(-1,1), y, eval_set=[(X_val.reshape(-1,1), y_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## demonstrate .evals_result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee29a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the 'rmse'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7323de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(range(1,len(xgb_reg.evals_result()['validation_0']['rmse'])+1), \n",
    "         xgb_reg.evals_result()['validation_0']['rmse'])\n",
    "plt.scatter([range(1,len(xgb_reg.evals_result()['validation_0']['rmse'])+1)[np.argmin(xgb_reg.evals_result()['validation_0']['rmse'])]], \n",
    "            [np.min(xgb_reg.evals_result()['validation_0']['rmse'])], c='k')\n",
    "plt.text(range(1,len(xgb_reg.evals_result()['validation_0']['rmse'])+1)[np.argmin(xgb_reg.evals_result()['validation_0']['rmse'])], \n",
    "         np.min(xgb_reg.evals_result()['validation_0']['rmse'])-.05, \"Min.\", fontsize=14)\n",
    "\n",
    "plt.title(\"Validation Error\", fontsize=20)\n",
    "plt.xlabel(\"Number of Weak Learners\", fontsize=16)\n",
    "plt.ylabel(\"RMSE\", fontsize=16)\n",
    "\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905bdc2",
   "metadata": {},
   "source": [
    "Further, `XGBoost` allows us to implement early stopping without having to write our own code to do so. We just have to include an `early_stopping_rounds` argument during the `fit` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## same xgb_reg as before\n",
    "xgb_reg = xgboost.XGBRegressor(n_estimators = 500,\n",
    "                                  max_depth = 1,\n",
    "                                  learning_rate = .1)\n",
    "\n",
    "\n",
    "## Now show off early_stopping_rounds with eval_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3af35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(range(1,len(xgb_reg.evals_result()['validation_0']['rmse'])+1), \n",
    "         xgb_reg.evals_result()['validation_0']['rmse'])\n",
    "plt.scatter([range(1,len(xgb_reg.evals_result()['validation_0']['rmse'])+1)[np.argmin(xgb_reg.evals_result()['validation_0']['rmse'])]], \n",
    "            [np.min(xgb_reg.evals_result()['validation_0']['rmse'])], c='k')\n",
    "plt.text(range(1,len(xgb_reg.evals_result()['validation_0']['rmse'])+1)[np.argmin(xgb_reg.evals_result()['validation_0']['rmse'])], \n",
    "         np.min(xgb_reg.evals_result()['validation_0']['rmse'])-.05, \"Min.\", fontsize=14)\n",
    "\n",
    "plt.title(\"Validation Error\", fontsize=20)\n",
    "plt.xlabel(\"Number of Weak Learners\", fontsize=16)\n",
    "plt.ylabel(\"RMSE\", fontsize=16)\n",
    "\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg = xgboost.XGBRegressor(n_estimators = 220,\n",
    "                                  max_depth = 1,\n",
    "                                  learning_rate = .1)\n",
    "xgb_reg.fit(X.reshape(-1,1), y)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(X,y,label='Training Points')\n",
    "plt.plot(X, xgb_reg.predict(X.reshape(-1,1)), 'k',label=\"Prediction\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"$X$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95693b59",
   "metadata": {},
   "source": [
    "Here we have scratched the surface of what `XGBoost` can do. To learn more about the package check out the gradient boosting `Practice Problems` as well as the `XGBoost` documentation, <a href=\"https://xgboost.readthedocs.io/en/latest/index.html\">https://xgboost.readthedocs.io/en/latest/index.html</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04670df9",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
