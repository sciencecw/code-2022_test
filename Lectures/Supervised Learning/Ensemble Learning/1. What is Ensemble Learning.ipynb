{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde34526",
   "metadata": {},
   "source": [
    "# What is Ensemble Learning?\n",
    "\n",
    "<i>Ensemble learning</i> is not a new type of supervised learning problem, but rather a different approach to solving supervised learning problems.\n",
    "\n",
    "To this point we have introduced unique algorithms for solving a particular problem with the thought that we would try some handful of these methods and choose the one single method that works best. The idea behind ensemble learning is to use all of the models in the handful (or ensemble) simultaneously to make a model that performs better than any individual. Sometimes this will involve vastly different models (for example combining $k$ nearest neighbors, logistic regression, support vector machines and decision trees) or many slight random perturbations of the same base model (hundreds of decision trees, known as a random forest). Ensemble learning can be applied for regression and classification problems alike.\n",
    "\n",
    "## Wisdom of the crowd\n",
    "\n",
    "Ensemble learning can be thought of as a machine learning implementation of the concept of the <i>wisdom of the crowd</i>. \n",
    "\n",
    "Suppose you want to know the answer to some question. Instead of basing the answer on a single person's response, you survey thousands or millions of people. To get your answer you aggregate all of the individual answers you have collected. In many cases this aggregated answer will be more correct than a single expert's answer. A fun illustration of this idea involves weighing cows, <a href=\"https://www.npr.org/transcripts/430372183\">https://www.npr.org/transcripts/430372183</a>.\n",
    "\n",
    "So the idea with ensemble methods is to build a number of different algorithms then average their predictions into a \"wiser\" prediction. \n",
    "\n",
    "## What we will cover\n",
    "\n",
    "In this section we will introduce some of the most common ensemble learning methods including:\n",
    "- Random forests,\n",
    "- Bagging and pasting,\n",
    "- Boosting which includes:\n",
    "    - Adaptive boosting and\n",
    "    - Gradient boosting and\n",
    "- Voter Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78168ac2",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83553c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
