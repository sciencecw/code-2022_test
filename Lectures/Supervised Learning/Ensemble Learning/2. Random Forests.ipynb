{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ba73f3",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "One tree, two tree, red tree random forest.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Describe the random forest model,\n",
    "- Mention how we introduce random perturbations to a decision tree,\n",
    "- Extend the random forest idea to an extra trees model and\n",
    "- Demonstrate how random forest models can be used for feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb386a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c7aa83",
   "metadata": {},
   "source": [
    "## Forests are many trees\n",
    "\n",
    "The <i>random forest</i> model is made by building many different decision trees. These trees are made \"different\" through a variety of random perturbations (more on this later in the notebook). Random forests are thus an ensemble of decision tree models.\n",
    "\n",
    "We will demonstrate the advantages of this ensemble with the synthetic data set we used in our decision tree classification notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a34895",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4930)\n",
    "\n",
    "X = np.zeros((200,2))\n",
    "X[:,0] = np.random.random(200)\n",
    "X[:,1] = np.random.random(200)\n",
    "\n",
    "y = np.zeros(200)\n",
    "y[X[:,0]-X[:,1]>=0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dabc83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "\n",
    "plt.scatter(X[y == 0,0], \n",
    "            X[y == 0,1],\n",
    "            c='blue',\n",
    "            label=\"0\")\n",
    "plt.scatter(X[y == 1,0], \n",
    "            X[y == 1,1],\n",
    "            c='orange',\n",
    "            marker='v',\n",
    "            label=\"1\")\n",
    "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 16)\n",
    "plt.legend(fontsize='14', loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bccf17",
   "metadata": {},
   "source": [
    "Let's compare the decision boundaries produced on these data with a decision tree of maximum depth 2, and a random forest of trees with maximum depth 2. In `sklearn` we can make a random forest classifier with `RandomForestClassifier`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd694b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import model objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a80991",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make model objects\n",
    "\n",
    "## decision tree\n",
    "## we've seen this before\n",
    "tree = DecisionTreeClassifier(max_depth = 2)\n",
    "\n",
    "## random forest\n",
    "## n_estimators determines the number of tree\n",
    "## each tree has the same hyperparameters as the DecisionTreeClassifier\n",
    "## we set a random_state to make sure that our fits are the same\n",
    "## max_depth comes from the decision tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0daccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the models\n",
    "\n",
    "\n",
    "## rf may take slightly longer because we are fitting many trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b606b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstrating\n",
    "\n",
    "## Making a grid for the decision boundaries\n",
    "xx1, xx2 = np.meshgrid(np.arange(-.01, 1.01, .01),\n",
    "                          np.arange(-.01, 1.01, .01))\n",
    "\n",
    "X_pred = np.zeros((len(xx1.reshape(-1,1)), 2))\n",
    "X_pred[:,0] = xx1.flatten()\n",
    "X_pred[:,1] = xx2.flatten()\n",
    "\n",
    "tree_preds = tree.predict(X_pred)\n",
    "rf_preds = rf.predict(X_pred)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(20,8))\n",
    "\n",
    "ax[0].scatter(X_pred[tree_preds==0,0],\n",
    "            X_pred[tree_preds==0,1],\n",
    "            alpha=.1,\n",
    "            c='lightblue',\n",
    "            s=100)\n",
    "\n",
    "ax[0].scatter(X_pred[tree_preds==1,0],\n",
    "            X_pred[tree_preds==1,1],\n",
    "            alpha=.1,\n",
    "            c='orange',\n",
    "            s=100)\n",
    "\n",
    "ax[0].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[0].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            edgecolor='black',\n",
    "            marker = 'v',\n",
    "            s=100)\n",
    "\n",
    "\n",
    "ax[1].scatter(X_pred[rf_preds==0,0],\n",
    "            X_pred[rf_preds==0,1],\n",
    "            alpha=.1,\n",
    "            c='lightblue',\n",
    "            s=100)\n",
    "ax[1].scatter(X_pred[rf_preds==1,0],\n",
    "            X_pred[rf_preds==1,1],\n",
    "            alpha=.1,\n",
    "            c='orange',\n",
    "            s=100)\n",
    "\n",
    "ax[1].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[1].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            marker = 'v',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "\n",
    "ax[0].set_title(\"Decision Tree\", fontsize=18)\n",
    "ax[1].set_title(\"Random Forest\", fontsize=18)\n",
    "\n",
    "ax[0].set_xlabel(\"$x_1$\", fontsize=16)\n",
    "ax[1].set_xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "ax[0].set_ylabel(\"$x_2$\", fontsize=16)\n",
    "ax[1].set_ylabel(\"$x_2$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6319b793",
   "metadata": {},
   "source": [
    "## How to plant trees\n",
    "\n",
    "So what is `sklearn` doing when we train a random forest? \n",
    "\n",
    "We already know that it is somehow building many decision trees with random perturbations and then averaging the results, but how does it build those trees?\n",
    "\n",
    "There are a number of ways that can be done.\n",
    "\n",
    "### Sampling subsets of the training data\n",
    "\n",
    "\n",
    "One way is to randomly sample training points <b>with replacement</b> from the data set, then train the algorithm on the randomly sampled set. Note that this is the default for `sklearn`'s decision trees, it can be controlled with the hyperparameter `bootstrap` a value of `True` takes a random subset, a value of `False` trains each tree with the entire data set. So if your training set has $n$ points, the algorithm randomly samples $n$ points with replacement (note this is the default it can be changed to be less than the entire dataset using `max_samples`) then trains a decision tree on it. \n",
    "\n",
    "Random Forests train `n_estimators` (an input to the `sklearn` method) independent trees. The default is $100$ trees, but this can be changed.\n",
    "\n",
    "This process of randomly selecting subsets of the data with replacement is more generally known as <i>bagging</i>, which we'll return to in the more general ensemble learning notebook. \n",
    "\n",
    "### Randomly selecting predictors\n",
    "\n",
    "In addition to the ability to randomly sample data, every decision tree is built on a random sample of the features of the data. This means that unlike in a single decision tree where the best cut is chosen from all of the features at each step, we limit ourselves to which features we consider. \n",
    "\n",
    "Also just like in decision trees you can control the maxinum number of features considered in your model with the hyperparameter `max_features`.\n",
    "\n",
    "#####  A note on hyperparameters\n",
    "\n",
    "Compared to all of the algorithms we have examined this far random forests have the most hyperparameters to think about. Depending on the settings you choose for the algorithm, you could wind up with vastly different predictions. It is always important to put thought into why you choose a particular hyperparameter value.\n",
    "\n",
    "### Extra-trees\n",
    "\n",
    "If you were sitting there thinking that you'd like to make this process even more random, you are in luck.\n",
    "\n",
    "An extension of random forests is know as extra-trees. This algorithm is just like a random forest, but in addition to randomly selecting a handful of features to optimize it also randomly selects the cutpoints instead of having the tree search for the optimal one.\n",
    "\n",
    "This algorithm is faster random forests, but does tend to have a little more bias. Typically you will have to build both classifiers and compare measures via cross-validation to decide if an extra-trees classifier is better than a standard random forest.\n",
    "\n",
    "This algorithm can be enacted using `ExtraTreesClassifier` from `sklearn`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6760a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import ExtraTreesClassifier here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99188537",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a model object\n",
    "\n",
    "\n",
    "## fit the model object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1300ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstrating\n",
    "\n",
    "## Making a grid for the decision boundaries\n",
    "et_preds = et.predict(X_pred)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(24,8))\n",
    "\n",
    "ax[0].scatter(X_pred[tree_preds==0,0],\n",
    "            X_pred[tree_preds==0,1],\n",
    "            alpha=.1,\n",
    "            c='lightblue',\n",
    "            s=100)\n",
    "\n",
    "ax[0].scatter(X_pred[tree_preds==1,0],\n",
    "            X_pred[tree_preds==1,1],\n",
    "            alpha=.1,\n",
    "            c='orange',\n",
    "            s=100)\n",
    "\n",
    "ax[0].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[0].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            edgecolor='black',\n",
    "            marker = 'v',\n",
    "            s=100)\n",
    "\n",
    "\n",
    "ax[1].scatter(X_pred[rf_preds==0,0],\n",
    "            X_pred[rf_preds==0,1],\n",
    "            alpha=.1,\n",
    "            c='lightblue',\n",
    "            s=100)\n",
    "ax[1].scatter(X_pred[rf_preds==1,0],\n",
    "            X_pred[rf_preds==1,1],\n",
    "            alpha=.1,\n",
    "            c='orange',\n",
    "            s=100)\n",
    "\n",
    "ax[1].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[1].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            marker = 'v',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "\n",
    "ax[2].scatter(X_pred[et_preds==0,0],\n",
    "            X_pred[et_preds==0,1],\n",
    "            alpha=.1,\n",
    "            c='lightblue',\n",
    "            s=100)\n",
    "ax[2].scatter(X_pred[et_preds==1,0],\n",
    "            X_pred[et_preds==1,1],\n",
    "            alpha=.1,\n",
    "            c='orange',\n",
    "            s=100)\n",
    "\n",
    "ax[2].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "ax[2].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            marker = 'v',\n",
    "            edgecolor='black',\n",
    "            s=100)\n",
    "\n",
    "ax[0].set_title(\"Decision Tree\", fontsize=18)\n",
    "ax[1].set_title(\"Random Forest\", fontsize=18)\n",
    "ax[2].set_title(\"Extra-Trees\", fontsize=18)\n",
    "\n",
    "\n",
    "ax[0].set_xlabel(\"$x_1$\", fontsize=16)\n",
    "ax[1].set_xlabel(\"$x_1$\", fontsize=16)\n",
    "ax[2].set_xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "\n",
    "ax[0].set_ylabel(\"$x_2$\", fontsize=16)\n",
    "ax[1].set_ylabel(\"$x_2$\", fontsize=16)\n",
    "ax[2].set_ylabel(\"$x_2$\", fontsize=16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af0b516",
   "metadata": {},
   "source": [
    "## Random forest for feature importances\n",
    "\n",
    "Random forests can also provide feature importance scores. \n",
    "\n",
    "The `sklearn` algorithm measures importance in the following way. For each feature it looks at every tree and identifies the nodes using that feature to make a cut. It then measures how much those cuts reduced impurity and averages that value over all the trees in the forest. After getting the average impurity reduction for each feature, `sklearn` scales the results so that the sum of all feature importances is equal to $1$.\n",
    "\n",
    "We will demonstrate this on the `iris` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55cd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e8fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "X = iris['data']\n",
    "X = X.rename(columns={'sepal length (cm)':'sepal_length',\n",
    "                         'sepal width (cm)':'sepal_width',\n",
    "                         'petal length (cm)':'petal_length',\n",
    "                         'petal width (cm)':'petal_width'})\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b90ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.copy(),y.copy(),\n",
    "                                                       shuffle=True,\n",
    "                                                       random_state=153,\n",
    "                                                       stratify=y,\n",
    "                                                       test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=500, max_depth=4)\n",
    "\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331c9d3",
   "metadata": {},
   "source": [
    "The `sklearn` scaled impurity reduction can be found with `feature_importances_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ced36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb1a3985",
   "metadata": {},
   "source": [
    "We can make it a little more readable with a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame({'feature':X_train.columns,\n",
    "                            'importance_score': forest.feature_importances_})\n",
    "\n",
    "score_df.sort_values('importance_score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e7328",
   "metadata": {},
   "source": [
    "This is a nice feature of random forests, it allows us to understand what variables are most important, which can help us explain the algorithm. It is also useful as another method for feature selection.\n",
    "\n",
    "##### Extra Trees\n",
    "\n",
    "Extra trees classifiers also has the ability to be used for feature importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76773af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=500, max_depth=4)\n",
    "\n",
    "et.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247b0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_score_df = pd.DataFrame({'feature':X_train.columns,\n",
    "                            'importance_score': et.feature_importances_})\n",
    "\n",
    "et_score_df.sort_values('importance_score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ba8a8",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63aaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
