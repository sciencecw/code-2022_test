{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf40691",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Another notebook touching on boosting, specifically gradient boosting.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Learn the concept behind gradient boosting:\n",
    "    - See why it is called gradient boosting and\n",
    "    - Demonstrate the method with nice plots and\n",
    "- Implement the algorithm in `sklearn`:\n",
    "    - Define early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59fc232",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a5f71",
   "metadata": {},
   "source": [
    "Gradient boosting is another boosting technique in which we iteratively build an ensemble of weak learners to hopefully create a strong learner. The approach for this boosting algorithm is to directly train weak learner $j+1$ to model weak learner $j$'s errors. \n",
    "\n",
    "Let's be more explicit now with an example in regression. Recall that in regression we try to model a quantitative variable, $y$, using $m$ features contained in a matrix, $X$. Gradient boosting for this problem then wooks like so:\n",
    "\n",
    "<i>Step 1</i>\n",
    "- Train a weak learner regression algorithm (say a decision stump regressor) to predict $y$, this is weak learner $1$.\n",
    "- Calculate the residuals, $r_1 = y - h_1(X)$, where $h_1(X) = \\hat{y}$ is the prediction of weak learner $1$.\n",
    "\n",
    "<i>Step $j$</i>\n",
    "- Train a weak learner to predict the residuals at step $j-1$, $r_{j-1}$,\n",
    "    - Set $h_j(X) = \\hat{r}_{j-1}$ denote the $j^\\text{th}$ weak learner's estimate of the residuals.\n",
    "- Calculate the residuals for this weak learner, $r_{j} = r_{j-1} - h_j(X)$.\n",
    "- Stop when $j+1 = J$, a predetermined stopping point.\n",
    "\n",
    "\n",
    "The prediction for $y$ at step $j$ is then found by:\n",
    "$$\n",
    "h(X) = h_1(X) + h_2(X) + \\dots + h_j(X)\n",
    "$$\n",
    "\n",
    "Let's visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate some data\n",
    "np.random.seed(220)\n",
    "X = np.linspace(-2,2,200)\n",
    "\n",
    "y = X**2 + np.random.randn(200)\n",
    "\n",
    "## Visualize the training data\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X,y)\n",
    "plt.xlabel(\"$X$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bfc15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(4,2, figsize=(14,25), sharey=True)\n",
    "\n",
    "### WEAK LEARNER 1 ###\n",
    "## Train tree 1\n",
    "tree1 = DecisionTreeRegressor(max_depth=1)\n",
    "tree1.fit(X.reshape(-1,1),y)\n",
    "h1 = tree1.predict(X.reshape(-1,1))\n",
    "r1 = y-h1\n",
    "\n",
    "## plot h1\n",
    "ax[0,0].scatter(X, y ,c='lightblue', alpha=.8, label='Training Data')\n",
    "ax[0,0].plot(X, h1, 'k', label=\"$h_1(X)$\")\n",
    "ax[0,0].legend(fontsize=12, loc=2)\n",
    "ax[0,0].set_xlabel(\"$X$\", fontsize=16)\n",
    "ax[0,0].set_ylabel(\"$y$\", fontsize=16)\n",
    "\n",
    "## plot h\n",
    "ax[0,1].scatter(X, y, c='lightblue', alpha=.8, label='Training Data')\n",
    "ax[0,1].plot(X, h1, 'k', label=\"$h(X)=h_1(X)$\")\n",
    "ax[0,1].legend(fontsize=12, loc=2)\n",
    "ax[0,1].set_xlabel(\"$X$\", fontsize=16)\n",
    "ax[0,1].set_ylabel(\"$y$\", fontsize=16)\n",
    "\n",
    "### WEAK LEARNER 2 ###\n",
    "## Train tree 2\n",
    "tree2 = DecisionTreeRegressor(max_depth=1)\n",
    "tree2.fit(X.reshape(-1,1),r1)\n",
    "h2 = tree2.predict(X.reshape(-1,1))\n",
    "r2 = r1-h2\n",
    "\n",
    "## plot h2\n",
    "ax[1,0].scatter(X, r1, c='lightblue', alpha=.8, label='$Training Data$')\n",
    "ax[1,0].plot(X, h2, 'k', label=\"$h_2(X)$\")\n",
    "ax[1,0].legend(fontsize=12, loc=2)\n",
    "ax[1,0].set_xlabel(\"$X$\", fontsize=16)\n",
    "ax[1,0].set_ylabel(\"$r_1$\", fontsize=16)\n",
    "\n",
    "## plot h\n",
    "ax[1,1].scatter(X, y, c='lightblue', alpha=.8, label='$Training Data$')\n",
    "ax[1,1].plot(X, h1+h2, 'k', label=\"$h(X)=h_1(X)+h_2(X)$\")\n",
    "ax[1,1].legend(fontsize=12, loc=2)\n",
    "ax[1,1].set_xlabel(\"$X$\", fontsize=16)\n",
    "ax[1,1].set_ylabel(\"$y$\", fontsize=16)\n",
    "\n",
    "\n",
    "### WEAK LEARNER 3 ###\n",
    "## Train tree 3\n",
    "tree3 = DecisionTreeRegressor(max_depth=1)\n",
    "tree3.fit(X.reshape(-1,1),r2)\n",
    "h3 = tree3.predict(X.reshape(-1,1))\n",
    "r3 = r2-h3\n",
    "\n",
    "## plot h3\n",
    "ax[2,0].scatter(X, r2, c='lightblue', alpha=.8, label='$Training Data')\n",
    "ax[2,0].plot(X, h3, 'k', label=\"$h_3(X)$\")\n",
    "ax[2,0].legend(fontsize=12, loc=2)\n",
    "ax[2,0].set_xlabel(\"$X$\", fontsize=16)\n",
    "ax[2,0].set_ylabel(\"$r_2$\", fontsize=16)\n",
    "\n",
    "## plot h\n",
    "ax[2,1].scatter(X, y, c='lightblue', alpha=.8, label='Training Data')\n",
    "ax[2,1].plot(X, h1+h2+h3, 'k', label=\"$h(X)=h_1(X)+h_2(X)+h_3(X)$\")\n",
    "ax[2,1].legend(fontsize=12, loc=2)\n",
    "ax[2,1].set_xlabel(\"$X$\", fontsize=16)\n",
    "ax[2,1].set_ylabel(\"$y$\", fontsize=16)\n",
    "\n",
    "\n",
    "### WEAK LEARNER 4 ###\n",
    "## Train tree 4\n",
    "tree4 = DecisionTreeRegressor(max_depth=1)\n",
    "tree4.fit(X.reshape(-1,1),r3)\n",
    "h4 = tree4.predict(X.reshape(-1,1))\n",
    "r4 = r3-h4\n",
    "\n",
    "## plot h3\n",
    "ax[3,0].scatter(X, r3, c='lightblue', alpha=.8, label='$Training Data$')\n",
    "ax[3,0].plot(X, h4, 'k', label=\"$h_4(X)$\")\n",
    "ax[3,0].legend(fontsize=12, loc=2)\n",
    "ax[3,0].set_xlabel(\"$X$\", fontsize=16)\n",
    "ax[3,0].set_ylabel(\"$r_3$\", fontsize=16)\n",
    "\n",
    "## plot h\n",
    "ax[3,1].scatter(X, y, c='lightblue', alpha=.8, label='$Training Data$')\n",
    "ax[3,1].plot(X, h1+h2+h3+h4, 'k', label=\"$h(X)=h_1(X)+h_2(X)+h_3(X)+h_4(X)$\")\n",
    "ax[3,1].legend(fontsize=12, loc=2)\n",
    "ax[3,1].set_xlabel(\"$X$\", fontsize=16)\n",
    "ax[3,1].set_ylabel(\"$y$\", fontsize=16)\n",
    "\n",
    "ax[0,0].set_title(\"$h_i$ and its\\ntraining data\", fontsize=16)\n",
    "ax[0,1].set_title(\"$h$ and the\\noriginal training data\", fontsize=16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89f3d8",
   "metadata": {},
   "source": [
    "## Gradient boosting in `sklearn`\n",
    "\n",
    "Now that we have an idea of how gradient boosting works, let's see how to implement it in `sklearn`.\n",
    "\n",
    "For regression we use `sklearn`'s `GradientBoostingRegressor`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html</a>. `GradientBoostingRegressor` does precisely what we did above, it trains a series of decision tree regressors (of default `max_depth=3`), where the number of trees trained is determined by `n_estimators` (default value of `100`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import GradientBoosting here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121fc56",
   "metadata": {},
   "source": [
    "Two ways control overfitting with a gradient boosting model is to change the number of trees we train and to change the `learning_rate`. We first demonstrate how the `learning_rate` impacts the model, then touch on the `n_estimators`.\n",
    "\n",
    "#### `learning_rate`\n",
    "\n",
    "The learning rate of the gradient boosting algorithm determines how much weight each weak learner recieves in the final prediction. `sklearn`'s default value is `0.1`. Let's demonstrate the difference between two `learning_rate`s now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8327d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the first booster, rate of .1\n",
    "## n_estimators = 10\n",
    "## with a max_depth of 1\n",
    "small_rate = \n",
    "\n",
    "## make the second, rate of 1\n",
    "large_rate = \n",
    "\n",
    "\n",
    "## fit both\n",
    "small_rate.fit(X.reshape(-1,1), y)\n",
    "large_rate.fit(X.reshape(-1,1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0446543",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(20,8))\n",
    "\n",
    "ax[0].scatter(X, y, alpha=.6, label='Training Points')\n",
    "ax[0].plot(X, small_rate.predict(X.reshape(-1,1)), 'k',label=\"Prediction\")\n",
    "ax[0].set_title(\"learning_rate=0.1\", fontsize=18)\n",
    "ax[0].legend(fontsize=14)\n",
    "ax[0].set_xlabel(\"$X$\", fontsize=16)\n",
    "ax[0].set_ylabel(\"$y$\", fontsize=16)\n",
    "\n",
    "ax[1].scatter(X, y, alpha=.6, label='Training Points')\n",
    "ax[1].plot(X, large_rate.predict(X.reshape(-1,1)), 'k',label=\"Prediction\")\n",
    "ax[1].set_title(\"learning_rate=1\", fontsize=18)\n",
    "ax[1].legend(fontsize=14)\n",
    "ax[1].set_xlabel(\"$X$\", fontsize=16)\n",
    "ax[1].set_ylabel(\"$y$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b4fdd",
   "metadata": {},
   "source": [
    "It is typically preferred to use a small learning rate and train more trees than to use a large learning rate.\n",
    "\n",
    "#### `n_estimators` and early stopping\n",
    "\n",
    "A second way to control over/underfitting a gradient booster is by controlling the number of weak learners you train. \n",
    "\n",
    "One way to find this that may be preferable to cross-validation is to use a validation set. You track the error on the validation set as you train each additional weak learner. Then you pick the number of weak learners that had the lowest validation set error.\n",
    "\n",
    "Let's demonstrate the method `staged_predict` which retuns an iterator over the predictions made by the booster at each level (with one tree, with two trees and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485fe9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here I will generate a validation set because the data are randomly generated\n",
    "## in practice you would need to split the data\n",
    "X_val = np.linspace(-2,2,200)\n",
    "y_val = X_val**2 + np.random.randn(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53905090",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import mse\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c986137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a new booster\n",
    "n_trees = 200\n",
    "gb = GradientBoostingRegressor(max_depth=1,\n",
    "                               n_estimators=n_trees)\n",
    "\n",
    "## fit the booster\n",
    "gb.fit(X.reshape(-1,1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use a list comprehension and staged_predict to get validation errors\n",
    "mses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c488f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(range(1,n_trees+1), mses)\n",
    "plt.scatter([range(1,n_trees+1)[np.argmin(mses)]], [np.min(mses)], c='k')\n",
    "plt.text(range(1,n_trees+1)[np.argmin(mses)], np.min(mses)-.05, \"Min.\", fontsize=14)\n",
    "\n",
    "plt.title(\"Validation Error\", fontsize=20)\n",
    "plt.xlabel(\"Number of Weak Learners\", fontsize=16)\n",
    "plt.ylabel(\"MSE\", fontsize=16)\n",
    "\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "range(1,n_trees+1)[np.argmin(mses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8cdcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num = range(1,n_trees+1)[np.argmin(mses)]\n",
    "gb = GradientBoostingRegressor(max_depth=1, n_estimators=best_num)\n",
    "gb.fit(X.reshape(-1,1),y)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(X,y,label='Training Points')\n",
    "plt.plot(X, gb.predict(X.reshape(-1,1)), 'k',label=\"Prediction\")\n",
    "plt.title(\"n_estimators=\"+str(best_num), fontsize=18)\n",
    "plt.xlabel(\"$X$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe531948",
   "metadata": {},
   "source": [
    "#### Early stopping\n",
    "\n",
    "We can also implement what is known as <i>early stopping</i>, where we stop training additional layers when it appears that we have reached a minimum. This saves time compared to training many trees and then looking retrospectively to find the best one.\n",
    "\n",
    "To do so in `sklearn` we use the `warm_start` argument, which forces `sklearn` to keep older layers when the fit method is called. Let's see what we mean now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef810c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the model object\n",
    "gb = GradientBoostingRegressor(max_depth=1, warm_start=True)\n",
    "\n",
    "## an initial value\n",
    "min_validation_error = float(\"inf\")\n",
    "\n",
    "val_errors = []\n",
    "\n",
    "## to keep track of if our error went up\n",
    "times_error_went_up_in_a_row = 0\n",
    "\n",
    "for n_estimators in range(1,500):\n",
    "    print(\"Training weak learner\",n_estimators)\n",
    "    # set the number of estimators\n",
    "    gb.n_estimators = n_estimators\n",
    "    \n",
    "    # fit/refit the model\n",
    "    gb.fit(X.reshape(-1,1), y)\n",
    "    \n",
    "    # record the validation error\n",
    "    val_errors.append(mean_squared_error(y_val, gb.predict(X_val.reshape(-1,1))))\n",
    "    \n",
    "    ## check if the error went up or down\n",
    "    # if it went down\n",
    "    if val_errors[-1] < min_validation_error:\n",
    "        # record the new minimum val error\n",
    "        min_validation_error = val_errors[-1]\n",
    "        \n",
    "        # reset our up counter\n",
    "        times_error_went_up_in_a_row = 0\n",
    "    # if it went up\n",
    "    else:\n",
    "        # add to the counter\n",
    "        times_error_went_up_in_a_row = times_error_went_up_in_a_row + 1\n",
    "        # if this is the 5th time in a row it has gone up\n",
    "        if times_error_went_up_in_a_row == 10:\n",
    "            # stop early\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(range(1,n_estimators+1), val_errors)\n",
    "plt.scatter([range(1,n_estimators+1)[np.argmin(val_errors)]], [np.min(val_errors)], c='k')\n",
    "plt.text(range(1,n_estimators+1)[np.argmin(val_errors)], np.min(val_errors)-.05, \"Min.\", fontsize=14)\n",
    "\n",
    "plt.title(\"Validation Error\", fontsize=20)\n",
    "plt.xlabel(\"Number of Weak Learners\", fontsize=16)\n",
    "plt.ylabel(\"MSE\", fontsize=16)\n",
    "\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c705c0",
   "metadata": {},
   "source": [
    "### Why \"gradient\" boosting?\n",
    "\n",
    "Before ending this notebook, let's discuss why this is known as \"gradient\" boosting.\n",
    "\n",
    "At step $j$ denote our prediction for $y$ as $\\hat{y} = H_j(X) = \\sum_{k=1}^j h_k(X)$. Then for step $j+1$ we have the following estimate of $y$:\n",
    "\n",
    "$$\n",
    "y \\approx  H_{j+1}(X) = H_j(X) + h_{j+1}(X),\n",
    "$$\n",
    "\n",
    "which we can alternatively think of as:\n",
    "\n",
    "$$\n",
    "h_{j+1}(X) \\approx y - H_j(X).\n",
    "$$\n",
    "\n",
    "Recall that for a regression problem we have typically attempted to minimize the MSE of the estimate, which, for simplicity, we can denote as:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\left(y - H_j(X)\\right)^2, \\text{ at  the } j+1 \\text{ step.}\n",
    "$$\n",
    "\n",
    "Taking the negative gradient of this with respect to the estimate $H_j$ gives:\n",
    "\n",
    "$$\n",
    "\\frac{2}{n} \\left( y - H_j(X) \\right) \\approx \\frac{2}{n}h_{j+1}(X).\n",
    "$$\n",
    "\n",
    "And so gradient boosting is roughly speaking a gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68999813",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df3cf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
