{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9641f464",
   "metadata": {},
   "source": [
    "# Voter Models\n",
    "\n",
    "Rock the vote!\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "- Discuss voter models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b204f",
   "metadata": {},
   "source": [
    "We will cast this in the light of classification, but will mention how it works for regression later in the notebook.\n",
    "\n",
    "Let's say that you have a few different classifiers that you think are pretty good, for instance a logistic regression model, a knn model, a support vector machine, and a random forest model.\n",
    "\n",
    "A voting classifier is one that looks at how each of your classifiers decides to classify a point and goes with the decision that receives the most \"votes\".\n",
    "\n",
    "Let's see how to implement this model type. We will be using the `VotingClassifier` object from `sklearn`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781eb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4930)\n",
    "\n",
    "X = np.zeros((200,2))\n",
    "X[:,0] = np.random.random(200)\n",
    "X[:,1] = np.random.random(200)\n",
    "\n",
    "y = np.zeros(200)\n",
    "y[X[:,0]-X[:,1]>=0] = 1\n",
    "\n",
    "## for predictions\n",
    "xx1, xx2 = np.meshgrid(np.arange(-.01, 1.01, .01),\n",
    "                          np.arange(-.01, 1.01, .01))\n",
    "\n",
    "X_pred = np.zeros((len(xx1.reshape(-1,1)), 2))\n",
    "X_pred[:,0] = xx1.flatten()\n",
    "X_pred[:,1] = xx2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.scatter(X[y == 0,0], \n",
    "            X[y == 0,1],\n",
    "            c='blue',\n",
    "            label=\"0\")\n",
    "plt.scatter(X[y == 1,0], \n",
    "            X[y == 1,1],\n",
    "            c='orange',\n",
    "            marker='v',\n",
    "            label=\"1\")\n",
    "\n",
    "\n",
    "plt.plot([0,1],\n",
    "         [0,1],\n",
    "         'k--',\n",
    "         label=\"Actual Decision Boundary\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 16)\n",
    "plt.legend(fontsize='14', loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f32de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import base classifiers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The VotingClassifier\n",
    "\n",
    "\n",
    "## import accuracy metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb12e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make Base Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d716cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the voting classifier\n",
    "## it is very similar to a pipeline's syntax\n",
    "\n",
    "voting = VotingClassifier(,\n",
    "                         voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4897f97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print each classifier's accuracy\n",
    "\n",
    "for name,clf in ([\"log_clf\",log],[\"rf_clf\",rf],\n",
    "                 [\"svm_clf\",svm],[\"knn_clf\",knn],\n",
    "                 [\"voting_clf\",voting]):\n",
    "    # fit the model\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    # predict\n",
    "    y_pred = clf.predict(X)\n",
    "    \n",
    "    # get acc\n",
    "    acc = sum(y == y_pred)/len(y_pred)\n",
    "    \n",
    "    print(name,\"training set accuracy\",np.round(acc,5))\n",
    "    preds = clf.predict(X_pred)\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    \n",
    "    \n",
    "    plt.scatter(X_pred[preds==1,0],\n",
    "                X_pred[preds==1,1],\n",
    "                alpha=.1,\n",
    "                c='orange',\n",
    "                s=10)\n",
    "    plt.scatter(X_pred[preds==0,0],\n",
    "                X_pred[preds==0,1],\n",
    "                alpha=.1,\n",
    "                c='lightblue',\n",
    "                s=10)\n",
    "\n",
    "    plt.scatter(X[y==1,0], \n",
    "                X[y==1,1],\n",
    "                label='Training 0',\n",
    "                c = 'darkorange',\n",
    "                marker='v',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "    plt.scatter(X[y==0,0], \n",
    "                X[y==0,1],\n",
    "                label='Training 1',\n",
    "                c = 'blue',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "    plt.plot([0,1],\n",
    "             [0,1],\n",
    "             'k--')\n",
    "    \n",
    "    plt.show()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfacbb4",
   "metadata": {},
   "source": [
    "#### Hard voting vs soft voting\n",
    "\n",
    "You can notice that we included the argument `voting = \"hard\"` when we defined our voting classifier. When this is your voting method, the prediction is decided according to the majority vote of the individual classifiers. For example, if 3 out of 4 possible classifiers classify the observation a `1`, then the voter model classifies it as a `1`.\n",
    "\n",
    "The other option is to set `voting = \"soft\"`. For this type of voting classifier predictions are chosen according to the probabilities assigned by each of the constituent classifiers. For each observation the soft voter assigns the class, $c$, for which:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^V P_j(y_i = c | X_i)\n",
    "$$\n",
    "\n",
    "is largest, where $P_j$ denotes the probability according to voting classifier $j$ of $V$ possible classifiers.\n",
    "\n",
    "#### Weights\n",
    "\n",
    "Note that you can also perform weighted voting of the classifiers with the `weights` argument. This would assign some classifiers more pull in deciding the predicted class than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291b95d",
   "metadata": {},
   "source": [
    "### Voter models for regression\n",
    "\n",
    "Voter models can also be made using an ensemble of independent regression models. Crucially, this does not mean that you should build several linear regression models with slightly different features and then feed them into a voter model. Instead you should build a handful of <i>unique</i> regression models for example a:\n",
    "- Linear regression model,\n",
    "- $k$NN regression model,\n",
    "- Support vector regressor and\n",
    "- Random forest regressor.\n",
    "\n",
    "The voter regression model takes the average (or weighted average) of all of the regression models fed into it to make a prediction. It can be implemented with `VotingRegressor` from `sklearn.ensemble`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113aa76",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049ac15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
