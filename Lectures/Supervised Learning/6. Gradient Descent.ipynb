{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a095da25",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient descent is a multi-purpose algorithm that is often used to find coefficient estimates for other more specific algorithms.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Remind ourselves of some calculus,\n",
    "- Discuss some extensions of simple gradient descent and\n",
    "- Show how to implement gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232bcf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b565157b",
   "metadata": {},
   "source": [
    "## The gradient\n",
    "\n",
    "As a quick refresher from calculus III let's remind ourselves of the <i>gradient</i>.\n",
    "\n",
    "The gradient of a differentiable function, $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is given by:\n",
    "\n",
    "$$\n",
    "\\nabla f = \\left( f_1, f_2, \\dots, f_n \\right),\n",
    "$$\n",
    "\n",
    "where $f_i$ denotes the partial derivative of $f$ with respect $x_i$, the independent variable denoting the $i^\\text{th}$ dimension.\n",
    "\n",
    "### Direction of quickest increase/decrease\n",
    "\n",
    "It is a fact from calculus that for any point, $x^*$, the direction of the vector $\\nabla f(x^*)$ is the direction in which $f$ most rapidly increases (for a proof see <a href=\"https://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx#Gradient_Defn\">https://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx#Gradient_Defn</a>). Conversely, the direction of the vector $-\\nabla f(x^*)$ is the direction in which $f$ most rapidly decreases.\n",
    "\n",
    "We will exploit this latter fact.\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "Many of our supervised learning algorithms are fit by minimizing some loss function, $\\ell$. For linear regression this loss function was the mean squared error:\n",
    "\n",
    "$$\n",
    "\\ell_{LR}(\\beta) = \\frac{1}{n} \\sum_{i=1}^n \\left( y^{(i)} - X^{(i)} \\beta  \\right)^2.\n",
    "$$\n",
    "\n",
    "While we can at times find explicit formulae for parameters that minimize $\\ell$, this is not always possible or may be too time consuming (computational time). \n",
    "\n",
    "The <i>gradient descent</i> algorithm offers an alternative approach in which we look to minimize the cost function using its gradient.\n",
    "\n",
    "Suppose $\\ell$ is a function of some parameters stored in a vector $\\beta$. Then gradient descent follows these steps:\n",
    "\n",
    "1. Make an initial guess for $\\hat{\\beta}$ call it $\\beta^*$ (this can be random),\n",
    "2. Calculate $\\nabla \\ell(\\beta^*)$,\n",
    "3. Update your guess for $\\hat{\\beta}$ by:\n",
    "    $$\n",
    "        \\beta^*_{\\text{updated}} = \\beta^* - \\alpha \\nabla f(\\beta^*),\n",
    "    $$\n",
    "4. Repeat steps 2. and 3. until you have reached a certain number of steps or until $||\\beta^*_{\\text{updated}} - \\beta^* ||< \\text{tol}$, for some tolerance level.\n",
    "\n",
    "The $\\alpha$ above is known as the <i>learning rate</i> and is set before running gradient descent (more on this hyperparameter later).\n",
    "\n",
    "#### Example: Implementing for regression\n",
    "\n",
    "For linear regression we have:\n",
    "\n",
    "$$\n",
    "\\nabla \\ell_{LR}(\\beta) = \\frac{2}{n} X^T \\left( X \\beta - y \\right).\n",
    "$$\n",
    "\n",
    "We can use this to program a gradient descent approach to the following regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416391b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating some data\n",
    "np.random.seed(431)\n",
    "\n",
    "X = np.ones((200,2))\n",
    "X[:,1] = 3*np.random.randn(200)\n",
    "\n",
    "## y = 2 + 7x + epsilon\n",
    "y = 2*X[:,0] + 7*X[:,1] + 4*np.random.randn(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.scatter(X[:,1], y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## programming gradient descent\n",
    "\n",
    "## learning rate\n",
    "alpha = .05\n",
    "\n",
    "## initial guess\n",
    "beta_star = np.random.random(size=(2,1))\n",
    "\n",
    "## setting a tolerance\n",
    "tol = 0.000000001\n",
    "\n",
    "## limit to 10000 iterations\n",
    "max_iter = 50000\n",
    "\n",
    "for i in range(max_iter):\n",
    "    ## update = current - alpha*gradient\n",
    "    beta_star_new = \n",
    "    \n",
    "    ## check to see if the change in estimate is beneath the tolerance\n",
    "    if np.sum(np.power(beta_star_new - beta_star,2))/2 < tol:\n",
    "        ## if it is we have beta_hat\n",
    "        beta_hat = beta_star_new\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        beta_star = beta_star_new\n",
    "        \n",
    "    if i + 1 == max_iter:\n",
    "        print(\"Maximum iterations reached before convergence.\")\n",
    "        \n",
    "print(\"Went through\", i, \"iterations\")\n",
    "print(\"beta_hat =\")\n",
    "print(beta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301da50",
   "metadata": {},
   "source": [
    "We can compare this to what we would get using the normal equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7939b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f65c71",
   "metadata": {},
   "source": [
    "The gradient descent approach may be preferable if you are working with a data set with a large number of features because unlike the normal equation it does not require the computation of an inverse, which can be computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de95b158",
   "metadata": {},
   "source": [
    "### Standard adjustments to gradient descent\n",
    "\n",
    "There are a couple of adjustments that get made to gradient descent.\n",
    "\n",
    "#### Mini-batch gradient descent\n",
    "\n",
    "<i>Mini-batch gradient descent</i> involves randomly splitting the training set into smaller subsets known as \"mini-batches\". When performing gradient descent you then calculate the gradient by cycling through these random subsets instead of using the entire training set. For example, if your mini-batches were one tenth of the entire training set the first time through gradient descent loop you would use the first mini-batch to calculate the gradient, the second time through you would use the second mini-batch and so on. When you have used all of the mini-batches (known as completing an <i>epoch</i>), you start the cycle again with mini-batch 1.\n",
    "\n",
    "Mini-batch descent is used to cut down on computation time.\n",
    "\n",
    "#### Stochastic gradient descent\n",
    "\n",
    "<i>Stochastic gradient descent</i> involves randomly generating a learning rate ($\\alpha$) each time through the update step.\n",
    "\n",
    "This is done to avoid having your estimate get stuck in a sub-optimal local minimum, which can occur when your cost function has many local minima. The hope is that a random learning rate will at times be large enough so that the estimate randomly leaves the valley of a local minimum and moves closer to the global minimum.\n",
    "\n",
    "Note stochastic gradient descent algorithms can be used to solve regression and classification problems in `sklearn`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html</a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab24f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the model object\n",
    "sgd = SGDRegressor()\n",
    "\n",
    "## fit the model\n",
    "sgd.fit(X, y)\n",
    "\n",
    "## look at the coef\n",
    "sgd.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc5961",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d1a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
