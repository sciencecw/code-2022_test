{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ba10a5",
   "metadata": {},
   "source": [
    "# Data Splits and Overfitting\n",
    "\n",
    "Now that we have a better idea about overfitting from our bias-variance trade-off notebook, let's discuss how data splits can help prevent us from using models that are overfit on data.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss what role data splits play in combating overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8835a",
   "metadata": {},
   "source": [
    "## The point of data splits\n",
    "\n",
    "Recall that in an earlier notebook we defined three types of data splits used for predictive modelling:\n",
    "- Train test splits,\n",
    "- Validation sets and\n",
    "- Cross-validation.\n",
    "\n",
    "When we defined these approahces we said that we wanted to use them in order to get some kind of estimate of our generalization error and then use that estimate to compare various models. This is particularly true in the case of cross-validation and validation sets.\n",
    "\n",
    "## Data splits \"combating\" overfitting\n",
    "\n",
    "We also mentioned that these splits could help us combat overfitting. However, it is important to note that simply assessing validation set or cross-validation performance when selecting a model does not entirely eliminate the risk of overfitting, this is a common misconception.\n",
    "\n",
    "\n",
    "While using such criteria allows us to choose the model among our candidate models that overfits the least, we may still be overfitting on the training data. Importantly, we can try to assess overfitting with these splits by comparing the performance (say MSE) on the training set and the validation/hold out or test sets. We should expect to see worse performance on the data we <i>did not</i> train the model on, but measuring how much worse can help us identify scenarios in which we are greatly overfitting to the training data.\n",
    "\n",
    "We will continue to learn methods we can reduce how much our model overfits as we continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee8908",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070d622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
